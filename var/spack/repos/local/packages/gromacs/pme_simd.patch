diff --git a/src/gromacs/simd/impl_arm_sve/impl_arm_sve_definitions.h b/src/gromacs/simd/impl_arm_sve/impl_arm_sve_definitions.h
index 012d256030..6708cf3421 100644
--- a/src/gromacs/simd/impl_arm_sve/impl_arm_sve_definitions.h
+++ b/src/gromacs/simd/impl_arm_sve/impl_arm_sve_definitions.h
@@ -102,7 +102,7 @@
 
 
 #if GMX_SIMD_FLOAT_WIDTH > 4
-#    define GMX_SIMD_HAVE_4NSIMD_UTIL_FLOAT 0
+#    define GMX_SIMD_HAVE_4NSIMD_UTIL_FLOAT 1
 #endif
 
 #if GMX_SIMD_DOUBLE_WIDTH > 4
diff --git a/src/gromacs/simd/impl_arm_sve/impl_arm_sve_util_float.h b/src/gromacs/simd/impl_arm_sve/impl_arm_sve_util_float.h
index 7ecfb6ddc8..e9d9837d25 100644
--- a/src/gromacs/simd/impl_arm_sve/impl_arm_sve_util_float.h
+++ b/src/gromacs/simd/impl_arm_sve/impl_arm_sve_util_float.h
@@ -664,6 +664,40 @@ static inline void gmx_simdcall gatherLoadTranspose2Hsimd(const float*       bas
 }
 #endif
 
+#if GMX_SIMD_FLOAT_WIDTH > 8
+static inline SimdFloat gmx_simdcall loadUNDuplicate4(const float* m)
+{
+    svbool_t pg = svwhilelt_b32(0, GMX_SIMD_FLOAT_WIDTH / 4);
+    svfloat32_t res = svld1_f32(pg, m);
+    res = svzip1_f32(res, res);
+    res = svzip1_f32(res, res);
+    return res;
+}
+
+static inline SimdFloat gmx_simdcall load4DuplicateN(const float* m)
+{
+    svbool_t pg = SVE_FLOAT4_MASK;
+#if 0
+    svfloat32_t res = svld1_f32(pg, m);
+    res = svdupq_lane_f32(res, 0);
+    return res;
+#else
+    return svld1rq_f32(pg, m);
+#endif
+}
+#endif
+
+#if GMX_SIMD_FLOAT_WIDTH >= 8
+static inline SimdFloat gmx_simdcall loadU4NOffset(const float* m, int offset)
+{
+    svint32_t offsets = svindex_s32(0, offset);
+    offsets = svzip1_s32(offsets, offsets);
+    offsets = svzip1_s32(offsets, offsets);
+    svbool_t pg = svptrue_b32();
+    offsets = svadd_s32_x(pg, offsets, svdupq_lane_s32(svindex_s32(0, 1), 0));
+    return svld1_gather_s32index_f32(pg, m, offsets);
+}
+#endif
 } // namespace gmx
 
 #endif // GMX_SIMD_IMPL_ARM_SVE_UTIL_FLOAT_H
