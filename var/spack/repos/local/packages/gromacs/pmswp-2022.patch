diff -ruN orig/gromacs-2022/cmake/gmxManageSimdKernelLibraries.cmake gromacs-2022/cmake/gmxManageSimdKernelLibraries.cmake
--- orig/gromacs-2022/cmake/gmxManageSimdKernelLibraries.cmake	2022-03-08 10:49:18.000000000 +0900
+++ gromacs-2022/cmake/gmxManageSimdKernelLibraries.cmake	2022-03-08 11:18:55.000000000 +0900
@@ -97,6 +97,14 @@
     else()
         # Triggering the compilation of the internal version of the library is handled elsewhere.
     endif()
+
+    gmx_option_multichoice(
+        GMX_${name}_UNROLL_INNER
+        "Unrolling factor for the ${name} nonbonded kernels"
+        "0"
+        0 1 2 3)
+    mark_as_advanced(GMX_${name}_UNROLL_INNER)
+      
 endmacro()
 
 # Inputs:
diff -ruN orig/gromacs-2022/src/config.h.cmakein gromacs-2022/src/config.h.cmakein
--- orig/gromacs-2022/src/config.h.cmakein	2022-02-23 01:05:08.000000000 +0900
+++ gromacs-2022/src/config.h.cmakein	2022-03-08 11:21:20.000000000 +0900
@@ -130,6 +130,12 @@
 /* Whether NBNXM and other SIMD kernels should be compiled */
 #cmakedefine01 GMX_USE_SIMD_KERNELS
 
+/* Unrolling factor for the 2XMM nonbonded kernels */
+#define GMX_2XMM_UNROLL_INNER @GMX_2XMM_UNROLL_INNER@
+
+/* Unrolling factor for the 4XM nonbonded kernels */
+#define GMX_4XM_UNROLL_INNER @GMX_2XMM_UNROLL_INNER@
+
 /* Integer byte order is big endian. */
 #cmakedefine01 GMX_INTEGER_BIG_ENDIAN
 
diff -ruN orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_0.h gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_0.h
--- orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_0.h	1970-01-01 09:00:00.000000000 +0900
+++ gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_0.h	2022-03-08 10:54:48.000000000 +0900
@@ -0,0 +1,898 @@
+/*
+ * This file is part of the GROMACS molecular simulation package.
+ *
+ * Copyright 2012- The GROMACS Authors
+ * and the project initiators Erik Lindahl, Berk Hess and David van der Spoel.
+ * Consult the AUTHORS/COPYING files and https://www.gromacs.org for details.
+ *
+ * GROMACS is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public License
+ * as published by the Free Software Foundation; either version 2.1
+ * of the License, or (at your option) any later version.
+ *
+ * GROMACS is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with GROMACS; if not, see
+ * https://www.gnu.org/licenses, or write to the Free Software Foundation,
+ * Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA.
+ *
+ * If you want to redistribute modifications to GROMACS, please
+ * consider that scientific software is very special. Version
+ * control is crucial - bugs must be traceable. We will be happy to
+ * consider code for inclusion in the official distribution, but
+ * derived work must not be called official GROMACS. Details are found
+ * in the README & COPYING files - if they are missing, get the
+ * official version at https://www.gromacs.org.
+ *
+ * To help us fund GROMACS development, we humbly ask that you cite
+ * the research papers on the package. Check out https://www.gromacs.org.
+ */
+
+/* This is the innermost loop contents for the 4 x N atom simd kernel.
+ * This flavor of the kernel duplicates the data for N j-particles in
+ * 2xN wide simd registers to do operate on 2 i-particles at once.
+ * This leads to 4/2=2 sets of most instructions. Therefore we call
+ * this kernel 2x(N+N) = 2xnn
+ *
+ * This 2xnn kernel is basically the 4xn equivalent with half the registers
+ * and instructions removed.
+ *
+ * An alternative would be to load to different cluster of N j-particles
+ * into simd registers, giving a 4x(N+N) kernel. This doubles the amount
+ * of instructions, which could lead to better scheduling. But we actually
+ * observed worse scheduling for the AVX-256 4x8 normal analytical PME
+ * kernel, which has a lower pair throughput than 2x(4+4) with gcc 4.7.
+ * It could be worth trying this option, but it takes some more effort.
+ * This 2xnn kernel is basically the 4xn equivalent with
+ */
+
+
+/* When calculating RF or Ewald interactions we calculate the electrostatic/LJ
+ * forces on excluded atom pairs here in the non-bonded loops.
+ * But when energies and/or virial is required we calculate them
+ * separately to as then it is easier to separate the energy and virial
+ * contributions.
+ */
+#if defined CHECK_EXCLS && (defined CALC_COULOMB || defined LJ_EWALD_GEOM)
+#    define EXCL_FORCES
+#endif
+#if defined CALC_COULOMB || defined CALC_COUL_TAB || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH \
+        || defined LJ_COULOMB_LB || defined HALF_LJ || defined EXCL_FORCES || defined LJ_COMB_LB
+#    define SKIP_INVSQRT 0
+#else
+#    define SKIP_INVSQRT 1
+#endif
+
+#ifdef ENERGY_GROUPS
+    /* Energy group indices for two atoms packed into one int */
+    int SP0egp_jj[UNROLLJ / 2];
+#endif
+
+#ifdef CHECK_EXCLS
+    /* Interaction (non-exclusion) mask of all 1's or 0's */
+    SimdBool SP0interact_S0;
+    SimdBool SP0interact_S2;
+#endif
+
+    SimdReal SP0jx_S, SP0jy_S, SP0jz_S;
+    SimdReal SP0dx_S0, SP0dy_S0, SP0dz_S0;
+    SimdReal SP0dx_S2, SP0dy_S2, SP0dz_S2;
+    SimdReal SP0tx_S0, SP0ty_S0, SP0tz_S0;
+    SimdReal SP0tx_S2, SP0ty_S2, SP0tz_S2;
+    SimdReal SP0rsq_S0, SP0rinvsq_S0;
+    SimdReal SP0rsq_S2, SP0rinvsq_S2;
+#if !SKIP_INVSQRT
+    SimdReal SP0rinv_S0, SP0rinv_S2;
+#endif
+    /* wco: within cut-off, mask of all 1's or 0's */
+    SimdBool SP0wco_S0;
+    SimdBool SP0wco_S2;
+#ifdef VDW_CUTOFF_CHECK
+    SimdBool SP0wco_vdw_S0;
+#    ifndef HALF_LJ
+    SimdBool SP0wco_vdw_S2;
+#    endif
+#endif
+
+#if (defined CALC_COULOMB && defined CALC_COUL_TAB) || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    SimdReal                                                        SP0r_S0;
+#    if (defined CALC_COULOMB && defined CALC_COUL_TAB) || !defined HALF_LJ
+    SimdReal                                                        SP0r_S2;
+#    endif
+#endif
+
+#if defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    SimdReal SP0rsw_S0, SP0rsw2_S0;
+#    ifndef HALF_LJ
+    SimdReal SP0rsw_S2, SP0rsw2_S2;
+#    endif
+#endif
+
+#ifdef CALC_COULOMB
+#    ifdef CHECK_EXCLS
+    /* 1/r masked with the interaction mask */
+    SimdReal SP0rinv_ex_S0;
+    SimdReal SP0rinv_ex_S2;
+#    endif
+    SimdReal SP0jq_S;
+    SimdReal SP0qq_S0;
+    SimdReal SP0qq_S2;
+#    ifdef CALC_COUL_TAB
+    /* The force (PME mesh force) we need to subtract from 1/r^2 */
+    SimdReal SP0fsub_S0;
+    SimdReal SP0fsub_S2;
+#    endif
+#    ifdef CALC_COUL_EWALD
+    SimdReal SP0brsq_S0, SP0brsq_S2;
+    SimdReal SP0ewcorr_S0, SP0ewcorr_S2;
+#    endif
+
+    /* frcoul = (1/r - fsub)*r */
+    SimdReal SP0frcoul_S0;
+    SimdReal SP0frcoul_S2;
+#    ifdef CALC_COUL_TAB
+    /* For tables: r, rs=r/sp, rf=floor(rs), frac=rs-rf */
+    SimdReal SP0rs_S0, SP0rf_S0, SP0frac_S0;
+    SimdReal SP0rs_S2, SP0rf_S2, SP0frac_S2;
+    /* Table index: rs truncated to an int */
+    SimdInt32 SP0ti_S0, SP0ti_S2;
+    /* Linear force table values */
+    SimdReal SP0ctab0_S0, SP0ctab1_S0;
+    SimdReal SP0ctab0_S2, SP0ctab1_S2;
+#        ifdef CALC_ENERGIES
+    /* Quadratic energy table value */
+    SimdReal SP0ctabv_S0, SP0dum_S0;
+    SimdReal SP0ctabv_S2, SP0dum_S2;
+#        endif
+#    endif
+#    if defined CALC_ENERGIES && (defined CALC_COUL_EWALD || defined CALC_COUL_TAB)
+    /* The potential (PME mesh) we need to subtract from 1/r */
+    SimdReal SP0vc_sub_S0;
+    SimdReal SP0vc_sub_S2;
+#    endif
+#    ifdef CALC_ENERGIES
+    /* Electrostatic potential */
+    SimdReal SP0vcoul_S0;
+    SimdReal SP0vcoul_S2;
+#    endif
+#endif
+    /* The force times 1/r */
+    SimdReal SP0fscal_S0;
+    SimdReal SP0fscal_S2;
+
+#ifdef CALC_LJ
+#    ifdef LJ_COMB_LB
+    /* LJ sigma_j/2 and sqrt(epsilon_j) */
+    SimdReal SP0hsig_j_S, SP0seps_j_S;
+    /* LJ sigma_ij and epsilon_ij */
+    SimdReal SP0sig_S0, SP0eps_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0sig_S2, SP0eps_S2;
+#        endif
+#        ifdef CALC_ENERGIES
+    SimdReal SP0sig2_S0, SP0sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0sig2_S2, SP0sig6_S2;
+#            endif
+#        endif /* LJ_COMB_LB */
+#    endif     /* CALC_LJ */
+
+#    ifdef LJ_COMB_GEOM
+    SimdReal SP0c6s_j_S, SP0c12s_j_S;
+#    endif
+
+    /* Intermediate variables for LJ calculation */
+#    ifndef LJ_COMB_LB
+    SimdReal SP0rinvsix_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0rinvsix_S2;
+#        endif
+#    endif
+#    ifdef LJ_COMB_LB
+    SimdReal SP0sir_S0, SP0sir2_S0, SP0sir6_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0sir_S2, SP0sir2_S2, SP0sir6_S2;
+#        endif
+#    endif
+
+    SimdReal SP0FrLJ6_S0, SP0FrLJ12_S0, SP0frLJ_S0;
+#    ifndef HALF_LJ
+    SimdReal SP0FrLJ6_S2, SP0FrLJ12_S2, SP0frLJ_S2;
+#    endif
+#endif /* CALC_LJ */
+
+    /* j-cluster index */
+    const int SP0cj = l_cj[cjind+0].cj;
+
+    /* Atom indices (of the first atom in the cluster) */
+    const int SP0aj = SP0cj * UNROLLJ;
+#if defined CALC_LJ && (defined LJ_COMB_GEOM || defined LJ_COMB_LB || defined LJ_EWALD_GEOM)
+    /* Index for loading LJ parameters, complicated when interleaving */
+    const int SP0aj2 = SP0aj * 2;
+#endif
+
+#ifdef CHECK_EXCLS
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+0].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+#endif /* CHECK_EXCLS */
+
+    /* load j atom coordinates */
+    loadDuplicate3Hsimd<STRIDE>(x + SP0aj * DIM, &SP0jx_S, &SP0jy_S, &SP0jz_S);
+
+    /* Calculate distance */
+    SP0dx_S0 = ix_S0 - SP0jx_S;
+    SP0dy_S0 = iy_S0 - SP0jy_S;
+    SP0dz_S0 = iz_S0 - SP0jz_S;
+    SP0dx_S2 = ix_S2 - SP0jx_S;
+    SP0dy_S2 = iy_S2 - SP0jy_S;
+    SP0dz_S2 = iz_S2 - SP0jz_S;
+
+    /* rsq = dx*dx+dy*dy+dz*dz */
+    SP0rsq_S0 = norm2(SP0dx_S0, SP0dy_S0, SP0dz_S0);
+    SP0rsq_S2 = norm2(SP0dx_S2, SP0dy_S2, SP0dz_S2);
+
+    /* Do the cut-off check */
+    SP0wco_S0 = (SP0rsq_S0 < rc2_S);
+    SP0wco_S2 = (SP0rsq_S2 < rc2_S);
+
+#ifdef CHECK_EXCLS
+#    ifdef EXCL_FORCES
+    /* Only remove the (sub-)diagonal to avoid double counting */
+#        if UNROLLJ == UNROLLI
+    if (cj == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask_S0;
+        wco_S2 = wco_S2 && diagonal_mask_S2;
+    }
+#        else
+#            if UNROLLJ == 2 * UNROLLI
+    if (cj * 2 == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask0_S0;
+        wco_S2 = wco_S2 && diagonal_mask0_S2;
+    }
+    else if (cj * 2 + 1 == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask1_S0;
+        wco_S2 = wco_S2 && diagonal_mask1_S2;
+    }
+#            else
+#                error "only UNROLLJ == UNROLLI*(1 or 2) currently supported in 2xnn kernels"
+#            endif
+#        endif
+#    else /* EXCL_FORCES */
+    /* No exclusion forces: remove all excluded atom pairs from the list */
+    wco_S0 = wco_S0 && interact_S0;
+    wco_S2 = wco_S2 && interact_S2;
+#    endif
+#endif
+
+#ifdef COUNT_PAIRS
+    {
+        int                              i, j;
+        alignas(GMX_SIMD_ALIGNMENT) real tmp[GMX_SIMD_REAL_WIDTH];
+
+        for (i = 0; i < UNROLLI; i += 2)
+        {
+            store(tmp, rc2_S - (i == 0 ? rsq_S0 : rsq_S2));
+            for (j = 0; j < 2 * UNROLLJ; j++)
+            {
+                if (tmp[j] >= 0)
+                {
+                    npair++;
+                }
+            }
+        }
+    }
+#endif
+
+    // Ensure the distances do not fall below the limit where r^-12 overflows.
+    // This should never happen for normal interactions.
+    SP0rsq_S0 = max(SP0rsq_S0, minRsq_S);
+    SP0rsq_S2 = max(SP0rsq_S2, minRsq_S);
+
+    /* Calculate 1/r */
+#if SKIP_INVSQRT
+    SP0rinvsq_S0 = invMask(SP0rsq_S0, SP0wco_S0);
+    SP0rinvsq_S2 = invMask(SP0rsq_S2, SP0wco_S2);
+#else
+    /* and set rinv to zero for r beyond the cut-off */
+    SP0rinv_S0 = invsqrtMask(SP0rsq_S0, SP0wco_S0);
+    SP0rinv_S2 = invsqrtMask(SP0rsq_S2, SP0wco_S2);
+#endif
+
+#ifdef CALC_COULOMB
+    /* Load parameters for j atom */
+    SP0jq_S = loadDuplicateHsimd(q + SP0aj);
+    SP0qq_S0 = iq_S0 * SP0jq_S;
+    SP0qq_S2 = iq_S2 * SP0jq_S;
+#endif
+
+#ifdef CALC_LJ
+#    if !defined LJ_COMB_GEOM && !defined LJ_COMB_LB && !defined FIX_LJ_C
+    SimdReal                                                     SP0c6_S0, SP0c12_S0;
+#        ifdef HALF_LJ
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP0aj, &SP0c6_S0, &SP0c12_S0);
+#        else
+    SimdReal SP0c6_S2, SP0c12_S2;
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP0aj, &SP0c6_S0, &SP0c12_S0, &SP0c6_S2, &SP0c12_S2);
+#        endif
+#    endif /* not defined any LJ rule */
+
+#    ifdef LJ_COMB_GEOM
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP0aj2, &SP0c6s_j_S, &SP0c12s_j_S);
+    SimdReal SP0c6_S0 = c6s_S0 * SP0c6s_j_S;
+#        ifndef HALF_LJ
+    SimdReal SP0c6_S2 = c6s_S2 * SP0c6s_j_S;
+#        endif
+    SimdReal SP0c12_S0 = c12s_S0 * SP0c12s_j_S;
+#        ifndef HALF_LJ
+    SimdReal SP0c12_S2 = c12s_S2 * SP0c12s_j_S;
+#        endif
+#    endif /* LJ_COMB_GEOM */
+
+#    ifdef LJ_COMB_LB
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP0aj2, &SP0hsig_j_S, &SP0seps_j_S);
+
+    SP0sig_S0 = hsig_i_S0 + SP0hsig_j_S;
+    SP0eps_S0 = seps_i_S0 * SP0seps_j_S;
+#        ifndef HALF_LJ
+    SP0sig_S2 = hsig_i_S2 + SP0hsig_j_S;
+    SP0eps_S2 = seps_i_S2 * SP0seps_j_S;
+#        endif
+#    endif /* LJ_COMB_LB */
+
+#endif /* CALC_LJ */
+
+#if !SKIP_INVSQRT
+    SP0rinvsq_S0 = SP0rinv_S0 * SP0rinv_S0;
+    SP0rinvsq_S2 = SP0rinv_S2 * SP0rinv_S2;
+#endif
+
+#ifdef CALC_COULOMB
+    /* Note that here we calculate force*r, not the usual force/r.
+     * This allows avoiding masking the reaction-field contribution,
+     * as frcoul is later multiplied by rinvsq which has been
+     * masked with the cut-off check.
+     */
+
+#    ifdef EXCL_FORCES
+    /* Only add 1/r for non-excluded atom pairs */
+    rinv_ex_S0 = selectByMask(rinv_S0, interact_S0);
+    rinv_ex_S2 = selectByMask(rinv_S2, interact_S2);
+#    else
+    /* No exclusion forces, we always need 1/r */
+#        define SP0rinv_ex_S0 SP0rinv_S0
+#        define SP0rinv_ex_S2 SP0rinv_S2
+#    endif
+
+#    ifdef CALC_COUL_RF
+    /* Electrostatic interactions */
+    SP0frcoul_S0 = SP0qq_S0 * fma(SP0rsq_S0, mrc_3_S, SP0rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fma(SP0rsq_S2, mrc_3_S, SP0rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    /* and (merge) mask energy for cut-off and diagonal */
+    SP0vcoul_S0 = maskzMul(SP0qq_S0, SP0rinv_ex_S0 + fma(SP0rsq_S0, hrc_3_S, moh_rc_S), SP0wco_S0);
+    SP0vcoul_S2 = maskzMul(SP0qq_S2, SP0rinv_ex_S2 + fma(SP0rsq_S2, hrc_3_S, moh_rc_S), SP0wco_S2);
+#        endif
+#    endif
+
+#    ifdef CALC_COUL_EWALD
+    /* We need to mask (or limit) rsq for the cut-off,
+     * as large distances can cause an overflow in gmx_pmecorrF/V.
+     */
+    SP0brsq_S0   = maskzMul(beta2_S, SP0rsq_S0, SP0wco_S0);
+    SP0brsq_S2   = maskzMul(beta2_S, SP0rsq_S2, SP0wco_S2);
+    SP0ewcorr_S0 = beta_S * pmeForceCorrection(SP0brsq_S0);
+    SP0ewcorr_S2 = beta_S * pmeForceCorrection(SP0brsq_S2);
+    SP0frcoul_S0 = SP0qq_S0 * fma(SP0ewcorr_S0, SP0brsq_S0, SP0rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fma(SP0ewcorr_S2, SP0brsq_S2, SP0rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    SP0vc_sub_S0 = beta_S * pmePotentialCorrection(SP0brsq_S0);
+    SP0vc_sub_S2 = beta_S * pmePotentialCorrection(SP0brsq_S2);
+#        endif
+
+#    endif /* CALC_COUL_EWALD */
+
+#    ifdef CALC_COUL_TAB
+    /* Electrostatic interactions */
+    SP0r_S0 = SP0rsq_S0 * SP0rinv_S0;
+    SP0r_S2 = SP0rsq_S2 * SP0rinv_S2;
+    /* Convert r to scaled table units */
+    SP0rs_S0 = SP0r_S0 * invtsp_S;
+    SP0rs_S2 = SP0r_S2 * invtsp_S;
+    /* Truncate scaled r to an int */
+    SP0ti_S0 = cvttR2I(SP0rs_S0);
+    SP0ti_S2 = cvttR2I(SP0rs_S2);
+
+    SP0rf_S0 = trunc(SP0rs_S0);
+    SP0rf_S2 = trunc(SP0rs_S2);
+
+    SP0frac_S0 = SP0rs_S0 - SP0rf_S0;
+    SP0frac_S2 = SP0rs_S2 - SP0rf_S2;
+
+    /* Load and interpolate table forces and possibly energies.
+     * Force and energy can be combined in one table, stride 4: FDV0
+     * or in two separate tables with stride 1: F and V
+     * Currently single precision uses FDV0, double F and V.
+     */
+#        ifndef CALC_ENERGIES
+#            ifdef TAB_FDV0
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S0, &ctab0_S0, &ctab1_S0);
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S2, &ctab0_S2, &ctab1_S2);
+#            else
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S0, &SP0ctab0_S0, &SP0ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S2, &SP0ctab0_S2, &SP0ctab1_S2);
+    SP0ctab1_S0   = SP0ctab1_S0 - SP0ctab0_S0;
+    SP0ctab1_S2   = SP0ctab1_S2 - SP0ctab0_S2;
+#            endif
+#        else
+#            ifdef TAB_FDV0
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S0, &ctab0_S0, &ctab1_S0, &ctabv_S0, &dum_S0);
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S2, &ctab0_S2, &ctab1_S2, &ctabv_S2, &dum_S2);
+#            else
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S0, &SP0ctab0_S0, &SP0ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP0ti_S0, &SP0ctabv_S0, &SP0dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S2, &SP0ctab0_S2, &SP0ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP0ti_S2, &SP0ctabv_S2, &SP0dum_S2);
+    SP0ctab1_S0 = SP0ctab1_S0 - SP0ctab0_S0;
+    SP0ctab1_S2 = SP0ctab1_S2 - SP0ctab0_S2;
+#            endif
+#        endif
+    SP0fsub_S0   = fma(SP0frac_S0, SP0ctab1_S0, SP0ctab0_S0);
+    SP0fsub_S2   = fma(SP0frac_S2, SP0ctab1_S2, SP0ctab0_S2);
+    SP0frcoul_S0 = SP0qq_S0 * fnma(SP0fsub_S0, SP0r_S0, SP0rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fnma(SP0fsub_S2, SP0r_S2, SP0rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    SP0vc_sub_S0 = fma((mhalfsp_S * SP0frac_S0), (SP0ctab0_S0 + SP0fsub_S0), SP0ctabv_S0);
+    SP0vc_sub_S2 = fma((mhalfsp_S * SP0frac_S2), (SP0ctab0_S2 + SP0fsub_S2), SP0ctabv_S2);
+#        endif
+#    endif /* CALC_COUL_TAB */
+
+#    if defined CALC_ENERGIES && (defined CALC_COUL_EWALD || defined CALC_COUL_TAB)
+#        ifndef NO_SHIFT_EWALD
+    /* Add Ewald potential shift to vc_sub for convenience */
+#            ifdef CHECK_EXCLS
+    vc_sub_S0 = maskAdd(vc_sub_S0, sh_ewald_S, interact_S0);
+    vc_sub_S2 = maskAdd(vc_sub_S2, sh_ewald_S, interact_S2);
+#            else
+    SP0vc_sub_S0  = SP0vc_sub_S0 + sh_ewald_S;
+    SP0vc_sub_S2  = SP0vc_sub_S2 + sh_ewald_S;
+#            endif
+#        endif
+
+    /* and (merge) mask energy for cut-off and diagonal */
+    SP0vcoul_S0 = maskzMul(SP0qq_S0, SP0rinv_ex_S0 - SP0vc_sub_S0, SP0wco_S0);
+    SP0vcoul_S2 = maskzMul(SP0qq_S2, SP0rinv_ex_S2 - SP0vc_sub_S2, SP0wco_S2);
+
+#    endif
+
+#endif /* CALC_COULOMB */
+
+#ifdef CALC_LJ
+    /* Lennard-Jones interaction */
+
+#    ifdef VDW_CUTOFF_CHECK
+    SP0wco_vdw_S0 = (SP0rsq_S0 < rcvdw2_S);
+#        ifndef HALF_LJ
+    SP0wco_vdw_S2 = (SP0rsq_S2 < rcvdw2_S);
+#        endif
+#    else
+    /* Same cut-off for Coulomb and VdW, reuse the registers */
+#        define SP0wco_vdw_S0 SP0wco_S0
+#        define SP0wco_vdw_S2 SP0wco_S2
+#    endif
+
+#    ifndef LJ_COMB_LB
+    SP0rinvsix_S0 = SP0rinvsq_S0 * SP0rinvsq_S0;
+#        ifdef EXCL_FORCES
+    rinvsix_S0 = maskzMul(rinvsix_S0, rinvsq_S0, interact_S0);
+#        else
+    SP0rinvsix_S0 = SP0rinvsix_S0 * SP0rinvsq_S0;
+#        endif
+#        ifndef HALF_LJ
+    SP0rinvsix_S2 = SP0rinvsq_S2 * SP0rinvsq_S2;
+#            ifdef EXCL_FORCES
+    rinvsix_S2 = maskzMul(rinvsix_S2, rinvsq_S2, interact_S2);
+#            else
+    SP0rinvsix_S2 = SP0rinvsix_S2 * SP0rinvsq_S2;
+#            endif
+#        endif
+
+#        if defined LJ_CUT || defined LJ_POT_SWITCH
+    /* We have plain LJ or LJ-PME with simple C6/6 C12/12 coefficients */
+    SP0FrLJ6_S0 = SP0c6_S0 * SP0rinvsix_S0;
+#            ifndef HALF_LJ
+    SP0FrLJ6_S2 = SP0c6_S2 * SP0rinvsix_S2;
+#            endif
+    SP0FrLJ12_S0 = SP0c12_S0 * SP0rinvsix_S0 * SP0rinvsix_S0;
+#            ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0c12_S2 * SP0rinvsix_S2 * SP0rinvsix_S2;
+#            endif
+#        endif
+
+#        if defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    /* We switch the LJ force */
+    SP0r_S0    = SP0rsq_S0 * SP0rinv_S0;
+    SP0rsw_S0  = max(SP0r_S0 - rswitch_S, zero_S);
+    SP0rsw2_S0 = SP0rsw_S0 * SP0rsw_S0;
+#            ifndef HALF_LJ
+    SP0r_S2    = SP0rsq_S2 * SP0rinv_S2;
+    SP0rsw_S2  = max(SP0r_S2 - rswitch_S, zero_S);
+    SP0rsw2_S2 = SP0rsw_S2 * SP0rsw_S2;
+#            endif
+#        endif
+
+#        ifdef LJ_FORCE_SWITCH
+
+#            define add_fr_switch(fr, rsw, rsw2_r, c2, c3) fma(fma(c3, rsw, c2), rsw2_r, fr)
+    SimdReal SP0rsw2_r_S0 = SP0rsw2_S0 * SP0r_S0;
+    SP0FrLJ6_S0           = SP0c6_S0 * add_fr_switch(SP0rinvsix_S0, SP0rsw_S0, SP0rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+#            ifndef HALF_LJ
+    SimdReal SP0rsw2_r_S2 = SP0rsw2_S2 * SP0r_S2;
+    SP0FrLJ6_S2           = SP0c6_S2 * add_fr_switch(SP0rinvsix_S2, SP0rsw_S2, SP0rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+#            endif
+    SP0FrLJ12_S0 = SP0c12_S0 * add_fr_switch(SP0rinvsix_S0 * SP0rinvsix_S0, SP0rsw_S0, SP0rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+#            ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0c12_S2 * add_fr_switch(SP0rinvsix_S2 * SP0rinvsix_S2, SP0rsw_S2, SP0rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+#            endif
+#            undef add_fr_switch
+#        endif /* LJ_FORCE_SWITCH */
+
+#    endif /* not LJ_COMB_LB */
+
+#    ifdef LJ_COMB_LB
+    SP0sir_S0 = SP0sig_S0 * SP0rinv_S0;
+#        ifndef HALF_LJ
+    SP0sir_S2 = SP0sig_S2 * SP0rinv_S2;
+#        endif
+    SP0sir2_S0 = SP0sir_S0 * SP0sir_S0;
+#        ifndef HALF_LJ
+    SP0sir2_S2 = SP0sir_S2 * SP0sir_S2;
+#        endif
+#        ifdef VDW_CUTOFF_CHECK
+    SP0sir6_S0 = maskzMul(SP0sir2_S0, SP0sir2_S0, SP0wco_vdw_S0);
+#        else
+    SP0sir6_S0    = SP0sir2_S0 * SP0sir2_S0;
+#        endif
+#        ifdef EXCL_FORCES
+    sir6_S0 = maskzMul(sir6_S0, sir2_S0, interact_S0);
+#        else
+    SP0sir6_S0    = SP0sir6_S0 * SP0sir2_S0;
+#        endif
+#        ifndef HALF_LJ
+#            ifdef VDW_CUTOFF_CHECK
+    SP0sir6_S2 = maskzMul(SP0sir2_S2, SP0sir2_S2, SP0wco_vdw_S2);
+#            else
+    SP0sir6_S2    = SP0sir2_S2 * SP0sir2_S2;
+#            endif
+#            ifdef EXCL_FORCES
+    sir6_S2 = maskzMul(sir6_S2, sir2_S2, interact_S2);
+#            else
+    SP0sir6_S2    = SP0sir6_S2 * SP0sir2_S2;
+#            endif
+#        endif
+    SP0FrLJ6_S0 = SP0eps_S0 * SP0sir6_S0;
+#        ifndef HALF_LJ
+    SP0FrLJ6_S2 = SP0eps_S2 * SP0sir6_S2;
+#        endif
+    SP0FrLJ12_S0 = SP0FrLJ6_S0 * SP0sir6_S0;
+#        ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0FrLJ6_S2 * SP0sir6_S2;
+#        endif
+#        if defined CALC_ENERGIES
+    /* We need C6 and C12 to calculate the LJ potential shift */
+    SP0sig2_S0 = SP0sig_S0 * SP0sig_S0;
+#            ifndef HALF_LJ
+    SP0sig2_S2 = SP0sig_S2 * SP0sig_S2;
+#            endif
+    SP0sig6_S0 = SP0sig2_S0 * SP0sig2_S0 * SP0sig2_S0;
+#            ifndef HALF_LJ
+    SP0sig6_S2 = SP0sig2_S2 * SP0sig2_S2 * SP0sig2_S2;
+#            endif
+    SimdReal SP0c6_S0 = SP0eps_S0 * SP0sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0c6_S2 = SP0eps_S2 * SP0sig6_S2;
+#            endif
+    SimdReal SP0c12_S0 = SP0c6_S0 * SP0sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0c12_S2 = SP0c6_S2 * SP0sig6_S2;
+#            endif
+#        endif
+#    endif /* LJ_COMB_LB */
+
+    /* Determine the total scalar LJ force*r */
+    SP0frLJ_S0 = SP0FrLJ12_S0 - SP0FrLJ6_S0;
+#    ifndef HALF_LJ
+    SP0frLJ_S2 = SP0FrLJ12_S2 - SP0FrLJ6_S2;
+#    endif
+
+#    if (defined LJ_CUT || defined LJ_FORCE_SWITCH) && defined CALC_ENERGIES
+
+#        ifdef LJ_CUT
+    /* Calculate the LJ energies, with constant potential shift */
+    SimdReal SP0VLJ6_S0 = sixth_S * fma(SP0c6_S0, p6_cpot_S, SP0FrLJ6_S0);
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ6_S2 = sixth_S * fma(SP0c6_S2, p6_cpot_S, SP0FrLJ6_S2);
+#            endif
+    SimdReal SP0VLJ12_S0 = twelveth_S * fma(SP0c12_S0, p12_cpot_S, SP0FrLJ12_S0);
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ12_S2 = twelveth_S * fma(SP0c12_S2, p12_cpot_S, SP0FrLJ12_S2);
+#            endif
+#        endif /* LJ_CUT */
+
+#        ifdef LJ_FORCE_SWITCH
+#            define v_fswitch_pr(rsw, rsw2, c0, c3, c4) fma(fma(c4, rsw, c3), (rsw2) * (rsw), c0)
+
+    SimdReal SP0VLJ6_S0 = SP0c6_S0 * fma(sixth_S, SP0rinvsix_S0, v_fswitch_pr(SP0rsw_S0, SP0rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ6_S2 = SP0c6_S2 * fma(sixth_S, SP0rinvsix_S2, v_fswitch_pr(SP0rsw_S2, SP0rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+#            endif
+    SimdReal SP0VLJ12_S0 = SP0c12_S0 * fma(twelveth_S, SP0rinvsix_S0 * SP0rinvsix_S0, v_fswitch_pr(SP0rsw_S0, SP0rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ12_S2 = SP0c12_S2 * fma(twelveth_S, SP0rinvsix_S2 * SP0rinvsix_S2, v_fswitch_pr(SP0rsw_S2, SP0rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+#            endif
+#            undef v_fswitch_pr
+#        endif /* LJ_FORCE_SWITCH */
+
+    /* Add up the repulsion and dispersion */
+    SimdReal SP0VLJ_S0 = SP0VLJ12_S0 - SP0VLJ6_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0VLJ_S2 = SP0VLJ12_S2 - SP0VLJ6_S2;
+#        endif
+
+#    endif /* (LJ_CUT || LJ_FORCE_SWITCH) && CALC_ENERGIES */
+
+#    ifdef LJ_POT_SWITCH
+    /* We always need the potential, since it is needed for the force */
+    SimdReal SP0VLJ_S0 = fnma(sixth_S, SP0FrLJ6_S0, twelveth_S * SP0FrLJ12_S0);
+#        ifndef HALF_LJ
+    SimdReal SP0VLJ_S2 = fnma(sixth_S, SP0FrLJ6_S2, twelveth_S * SP0FrLJ12_S2);
+#        endif
+
+    {
+        SimdReal SP0sw_S0, SP0dsw_S0;
+#        ifndef HALF_LJ
+        SimdReal SP0sw_S2, SP0dsw_S2;
+#        endif
+
+#        define switch_pr(rsw, rsw2, c3, c4, c5) \
+            fma(fma(fma(c5, rsw, c4), rsw, c3), (rsw2) * (rsw), one_S)
+#        define dswitch_pr(rsw, rsw2, c2, c3, c4) fma(fma(c4, rsw, c3), rsw, c2) * (rsw2)
+
+        SP0sw_S0  = switch_pr(SP0rsw_S0, SP0rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP0dsw_S0 = dswitch_pr(SP0rsw_S0, SP0rsw2_S0, swF2_S, swF3_S, swF4_S);
+#        ifndef HALF_LJ
+        SP0sw_S2  = switch_pr(SP0rsw_S2, SP0rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP0dsw_S2 = dswitch_pr(SP0rsw_S2, SP0rsw2_S2, swF2_S, swF3_S, swF4_S);
+#        endif
+        SP0frLJ_S0 = fnma(SP0dsw_S0 * SP0VLJ_S0, SP0r_S0, SP0sw_S0 * SP0frLJ_S0);
+#        ifndef HALF_LJ
+        SP0frLJ_S2 = fnma(SP0dsw_S2 * SP0VLJ_S2, SP0r_S2, SP0sw_S2 * SP0frLJ_S2);
+#        endif
+#        ifdef CALC_ENERGIES
+        SP0VLJ_S0 = SP0sw_S0 * SP0VLJ_S0;
+#            ifndef HALF_LJ
+        SP0VLJ_S2 = SP0sw_S2 * SP0VLJ_S2;
+#            endif
+#        endif
+
+#        undef switch_pr
+#        undef dswitch_pr
+    }
+#    endif /* LJ_POT_SWITCH */
+
+#    if defined CALC_ENERGIES && defined CHECK_EXCLS
+    /* The potential shift should be removed for excluded pairs */
+    VLJ_S0 = selectByMask(VLJ_S0, interact_S0);
+#        ifndef HALF_LJ
+    VLJ_S2 = selectByMask(VLJ_S2, interact_S2);
+#        endif
+#    endif
+
+#    ifdef LJ_EWALD_GEOM
+    {
+        SimdReal SP0c6s_j_S;
+        SimdReal SP0c6grid_S0, SP0rinvsix_nm_S0, SP0cr2_S0, SP0expmcr2_S0, SP0poly_S0;
+#        ifndef HALF_LJ
+        SimdReal SP0c6grid_S2, SP0rinvsix_nm_S2, SP0cr2_S2, SP0expmcr2_S2, SP0poly_S2;
+#        endif
+#        ifdef CALC_ENERGIES
+        SimdReal SP0sh_mask_S0;
+#            ifndef HALF_LJ
+        SimdReal SP0sh_mask_S2;
+#            endif
+#        endif
+
+        /* Determine C6 for the grid using the geometric combination rule */
+        SP0c6s_j_S   = loadDuplicateHsimd(ljc + SP0aj2);
+        SP0c6grid_S0 = c6s_S0 * SP0c6s_j_S;
+#        ifndef HALF_LJ
+        SP0c6grid_S2 = c6s_S2 * SP0c6s_j_S;
+#        endif
+
+#        ifdef CHECK_EXCLS
+        /* Recalculate rinvsix without exclusion mask (compiler might optimize) */
+        rinvsix_nm_S0 = rinvsq_S0 * rinvsq_S0 * rinvsq_S0;
+#            ifndef HALF_LJ
+        rinvsix_nm_S2 = rinvsq_S2 * rinvsq_S2 * rinvsq_S2;
+#            endif
+#        else
+        /* We didn't use a mask, so we can copy */
+        SP0rinvsix_nm_S0 = SP0rinvsix_S0;
+#            ifndef HALF_LJ
+        SP0rinvsix_nm_S2 = SP0rinvsix_S2;
+#            endif
+#        endif
+
+        /* Mask for the cut-off to avoid overflow of cr2^2 */
+        SP0cr2_S0 = maskzMul(lje_c2_S, SP0rsq_S0, SP0wco_vdw_S0);
+#        ifndef HALF_LJ
+        SP0cr2_S2 = maskzMul(lje_c2_S, SP0rsq_S2, SP0wco_vdw_S2);
+#        endif
+        // Unsafe version of our exp() should be fine, since these arguments should never
+        // be smaller than -127 for any reasonable choice of cutoff or ewald coefficients.
+        SP0expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP0cr2_S0);
+#        ifndef HALF_LJ
+        SP0expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP0cr2_S2);
+#        endif
+
+        /* 1 + cr2 + 1/2*cr2^2 */
+        SP0poly_S0 = fma(fma(half_S, SP0cr2_S0, one_S), SP0cr2_S0, one_S);
+#        ifndef HALF_LJ
+        SP0poly_S2 = fma(fma(half_S, SP0cr2_S2, one_S), SP0cr2_S2, one_S);
+#        endif
+
+        /* We calculate LJ F*r = (6*C6)*(r^-6 - F_mesh/6), we use:
+         * r^-6*cexp*(1 + cr2 + cr2^2/2 + cr2^3/6) = cexp*(r^-6*poly + c^6/6)
+         */
+        SP0frLJ_S0 = fma(SP0c6grid_S0, fnma(SP0expmcr2_S0, fma(SP0rinvsix_nm_S0, SP0poly_S0, lje_c6_6_S), SP0rinvsix_nm_S0), SP0frLJ_S0);
+#        ifndef HALF_LJ
+        SP0frLJ_S2 = fma(SP0c6grid_S2, fnma(SP0expmcr2_S2, fma(SP0rinvsix_nm_S2, SP0poly_S2, lje_c6_6_S), SP0rinvsix_nm_S2), SP0frLJ_S2);
+#        endif
+
+#        ifdef CALC_ENERGIES
+#            ifdef CHECK_EXCLS
+        sh_mask_S0 = selectByMask(lje_vc_S, interact_S0);
+#                ifndef HALF_LJ
+        sh_mask_S2 = selectByMask(lje_vc_S, interact_S2);
+#                endif
+#            else
+        SP0sh_mask_S0 = lje_vc_S;
+#                ifndef HALF_LJ
+        SP0sh_mask_S2 = lje_vc_S;
+#                endif
+#            endif
+
+        SP0VLJ_S0 = fma(sixth_S * SP0c6grid_S0, fma(SP0rinvsix_nm_S0, fnma(SP0expmcr2_S0, SP0poly_S0, one_S), SP0sh_mask_S0), SP0VLJ_S0);
+#            ifndef HALF_LJ
+        SP0VLJ_S2 = fma(sixth_S * SP0c6grid_S2, fma(SP0rinvsix_nm_S2, fnma(SP0expmcr2_S2, SP0poly_S2, one_S), SP0sh_mask_S2), SP0VLJ_S2);
+#            endif
+#        endif /* CALC_ENERGIES */
+    }
+#    endif /* LJ_EWALD_GEOM */
+
+#    if defined VDW_CUTOFF_CHECK
+    /* frLJ is multiplied later by rinvsq, which is masked for the Coulomb
+     * cut-off, but if the VdW cut-off is shorter, we need to mask with that.
+     */
+    SP0frLJ_S0 = selectByMask(SP0frLJ_S0, SP0wco_vdw_S0);
+#        ifndef HALF_LJ
+    SP0frLJ_S2 = selectByMask(SP0frLJ_S2, SP0wco_vdw_S2);
+#        endif
+#    endif
+
+#    ifdef CALC_ENERGIES
+    /* The potential shift should be removed for pairs beyond cut-off */
+    SP0VLJ_S0 = selectByMask(SP0VLJ_S0, SP0wco_vdw_S0);
+#        ifndef HALF_LJ
+    SP0VLJ_S2 = selectByMask(SP0VLJ_S2, SP0wco_vdw_S2);
+#        endif
+#    endif
+
+#endif /* CALC_LJ */
+
+#ifdef CALC_ENERGIES
+#    ifdef ENERGY_GROUPS
+    /* Extract the group pair index per j pair.
+     * Energy groups are stored per i-cluster, so things get
+     * complicated when the i- and j-cluster size don't match.
+     */
+    {
+#        if UNROLLJ == 2
+#error
+        const int egps_j    = nbatParams.energrp[cj >> 1];
+        egp_jj[0]       = ((egps_j >> ((cj & 1) * egps_jshift)) & egps_jmask) * egps_jstride;
+#        else
+        /* We assume UNROLLI <= UNROLLJ */
+        for (int jdi = 0; jdi < UNROLLJ / UNROLLI; jdi++)
+        {
+            const int SP0egps_j = nbatParams.energrp[SP0cj * (UNROLLJ / UNROLLI) + jdi];
+            for (int jj = 0; jj < (UNROLLI / 2); jj++)
+            {
+                SP0egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP0egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+            }
+        }
+#        endif
+    }
+#    endif
+
+#    ifdef CALC_COULOMB
+#        ifndef ENERGY_GROUPS
+    vctot_S = vctot_S + SP0vcoul_S0 + SP0vcoul_S2;
+#        else
+    add_ener_grp_halves(SP0vcoul_S0, vctp[0], vctp[1], SP0egp_jj);
+    add_ener_grp_halves(SP0vcoul_S2, vctp[2], vctp[3], SP0egp_jj);
+#        endif
+#    endif
+
+#    ifdef CALC_LJ
+#        ifndef ENERGY_GROUPS
+    Vvdwtot_S = Vvdwtot_S
+                + SP0VLJ_S0
+#            ifndef HALF_LJ
+                + SP0VLJ_S2
+#            endif
+            ;
+#        else
+    add_ener_grp_halves(SP0VLJ_S0, vvdwtp[0], vvdwtp[1], SP0egp_jj);
+#            ifndef HALF_LJ
+    add_ener_grp_halves(SP0VLJ_S2, vvdwtp[2], vvdwtp[3], SP0egp_jj);
+#            endif
+#        endif
+#    endif /* CALC_LJ */
+#endif     /* CALC_ENERGIES */
+
+#ifdef CALC_LJ
+#    ifdef CALC_COULOMB
+    SP0fscal_S0 = SP0rinvsq_S0 * (SP0frcoul_S0 + SP0frLJ_S0);
+#    else
+    SP0fscal_S0 = SP0rinvsq_S0 * SP0frLJ_S0;
+#    endif
+#else
+    fscal_S0 = rinvsq_S0 * frcoul_S0;
+#endif /* CALC_LJ */
+#if defined CALC_LJ && !defined HALF_LJ
+#    ifdef CALC_COULOMB
+    SP0fscal_S2 = SP0rinvsq_S2 * (SP0frcoul_S2 + SP0frLJ_S2);
+#    else
+    SP0fscal_S2 = SP0rinvsq_S2 * SP0frLJ_S2;
+#    endif
+#else
+    /* Atom 2 and 3 don't have LJ, so only add Coulomb forces */
+    SP0fscal_S2 = SP0rinvsq_S2 * SP0frcoul_S2;
+#endif
+
+    /* Calculate temporary vectorial force */
+    SP0tx_S0 = SP0fscal_S0 * SP0dx_S0;
+    SP0tx_S2 = SP0fscal_S2 * SP0dx_S2;
+    SP0ty_S0 = SP0fscal_S0 * SP0dy_S0;
+    SP0ty_S2 = SP0fscal_S2 * SP0dy_S2;
+    SP0tz_S0 = SP0fscal_S0 * SP0dz_S0;
+    SP0tz_S2 = SP0fscal_S2 * SP0dz_S2;
+
+    /* Increment i atom force */
+    fix_S0 = fix_S0 + SP0tx_S0;
+    fix_S2 = fix_S2 + SP0tx_S2;
+    fiy_S0 = fiy_S0 + SP0ty_S0;
+    fiy_S2 = fiy_S2 + SP0ty_S2;
+    fiz_S0 = fiz_S0 + SP0tz_S0;
+    fiz_S2 = fiz_S2 + SP0tz_S2;
+
+    /* Decrement j atom force */
+    decr3Hsimd(f + SP0aj * DIM, SP0tx_S0 + SP0tx_S2, SP0ty_S0 + SP0ty_S2, SP0tz_S0 + SP0tz_S2);
+
+#undef SP0rinv_ex_S0
+#undef SP0rinv_ex_S2
+
+#undef SP0wco_vdw_S0
+#undef SP0wco_vdw_S2
+
+#undef EXCL_FORCES
+#undef SKIP_INVSQRT
diff -ruN orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_1.h gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_1.h
--- orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_1.h	1970-01-01 09:00:00.000000000 +0900
+++ gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_1.h	2022-03-08 10:54:48.000000000 +0900
@@ -0,0 +1,1164 @@
+/*
+ * This file is part of the GROMACS molecular simulation package.
+ *
+ * Copyright 2012- The GROMACS Authors
+ * and the project initiators Erik Lindahl, Berk Hess and David van der Spoel.
+ * Consult the AUTHORS/COPYING files and https://www.gromacs.org for details.
+ *
+ * GROMACS is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public License
+ * as published by the Free Software Foundation; either version 2.1
+ * of the License, or (at your option) any later version.
+ *
+ * GROMACS is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with GROMACS; if not, see
+ * https://www.gnu.org/licenses, or write to the Free Software Foundation,
+ * Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA.
+ *
+ * If you want to redistribute modifications to GROMACS, please
+ * consider that scientific software is very special. Version
+ * control is crucial - bugs must be traceable. We will be happy to
+ * consider code for inclusion in the official distribution, but
+ * derived work must not be called official GROMACS. Details are found
+ * in the README & COPYING files - if they are missing, get the
+ * official version at https://www.gromacs.org.
+ *
+ * To help us fund GROMACS development, we humbly ask that you cite
+ * the research papers on the package. Check out https://www.gromacs.org.
+ */
+
+/* This is the innermost loop contents for the 4 x N atom simd kernel.
+ * This flavor of the kernel duplicates the data for N j-particles in
+ * 2xN wide simd registers to do operate on 2 i-particles at once.
+ * This leads to 4/2=2 sets of most instructions. Therefore we call
+ * this kernel 2x(N+N) = 2xnn
+ *
+ * This 2xnn kernel is basically the 4xn equivalent with half the registers
+ * and instructions removed.
+ *
+ * An alternative would be to load to different cluster of N j-particles
+ * into simd registers, giving a 4x(N+N) kernel. This doubles the amount
+ * of instructions, which could lead to better scheduling. But we actually
+ * observed worse scheduling for the AVX-256 4x8 normal analytical PME
+ * kernel, which has a lower pair throughput than 2x(4+4) with gcc 4.7.
+ * It could be worth trying this option, but it takes some more effort.
+ * This 2xnn kernel is basically the 4xn equivalent with
+ */
+
+
+/* When calculating RF or Ewald interactions we calculate the electrostatic/LJ
+ * forces on excluded atom pairs here in the non-bonded loops.
+ * But when energies and/or virial is required we calculate them
+ * separately to as then it is easier to separate the energy and virial
+ * contributions.
+ */
+#if defined CHECK_EXCLS && (defined CALC_COULOMB || defined LJ_EWALD_GEOM)
+#    define EXCL_FORCES
+#endif
+#if defined CALC_COULOMB || defined CALC_COUL_TAB || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH \
+        || defined LJ_COULOMB_LB || defined HALF_LJ || defined EXCL_FORCES || defined LJ_COMB_LB
+#    define SKIP_INVSQRT 0
+#else
+#    define SKIP_INVSQRT 1
+#endif
+
+#ifdef ENERGY_GROUPS
+    /* Energy group indices for two atoms packed into one int */
+    int SP0egp_jj[UNROLLJ / 2];
+    int SP1egp_jj[UNROLLJ / 2];
+#endif
+
+#ifdef CHECK_EXCLS
+    /* Interaction (non-exclusion) mask of all 1's or 0's */
+    SimdBool SP0interact_S0;
+    SimdBool SP1interact_S0;
+    SimdBool SP0interact_S2;
+    SimdBool SP1interact_S2;
+#endif
+
+    SimdReal SP0jx_S, SP0jy_S, SP0jz_S;
+    SimdReal SP1jx_S, SP1jy_S, SP1jz_S;
+    SimdReal SP0dx_S0, SP0dy_S0, SP0dz_S0;
+    SimdReal SP1dx_S0, SP1dy_S0, SP1dz_S0;
+    SimdReal SP0dx_S2, SP0dy_S2, SP0dz_S2;
+    SimdReal SP1dx_S2, SP1dy_S2, SP1dz_S2;
+    SimdReal SP0tx_S0, SP0ty_S0, SP0tz_S0;
+    SimdReal SP1tx_S0, SP1ty_S0, SP1tz_S0;
+    SimdReal SP0tx_S2, SP0ty_S2, SP0tz_S2;
+    SimdReal SP1tx_S2, SP1ty_S2, SP1tz_S2;
+    SimdReal SP0rsq_S0, SP0rinvsq_S0;
+    SimdReal SP1rsq_S0, SP1rinvsq_S0;
+    SimdReal SP0rsq_S2, SP0rinvsq_S2;
+    SimdReal SP1rsq_S2, SP1rinvsq_S2;
+#if !SKIP_INVSQRT
+    SimdReal SP0rinv_S0, SP0rinv_S2;
+    SimdReal SP1rinv_S0, SP1rinv_S2;
+#endif
+    /* wco: within cut-off, mask of all 1's or 0's */
+    SimdBool SP0wco_S0;
+    SimdBool SP1wco_S0;
+    SimdBool SP0wco_S2;
+    SimdBool SP1wco_S2;
+#ifdef VDW_CUTOFF_CHECK
+    SimdBool SP0wco_vdw_S0;
+    SimdBool SP1wco_vdw_S0;
+#    ifndef HALF_LJ
+    SimdBool SP0wco_vdw_S2;
+    SimdBool SP1wco_vdw_S2;
+#    endif
+#endif
+
+#if (defined CALC_COULOMB && defined CALC_COUL_TAB) || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    SimdReal                                                        SP0r_S0;
+    SimdReal                                                        SP1r_S0;
+#    if (defined CALC_COULOMB && defined CALC_COUL_TAB) || !defined HALF_LJ
+    SimdReal                                                        SP0r_S2;
+    SimdReal                                                        SP1r_S2;
+#    endif
+#endif
+
+#if defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    SimdReal SP0rsw_S0, SP0rsw2_S0;
+    SimdReal SP1rsw_S0, SP1rsw2_S0;
+#    ifndef HALF_LJ
+    SimdReal SP0rsw_S2, SP0rsw2_S2;
+    SimdReal SP1rsw_S2, SP1rsw2_S2;
+#    endif
+#endif
+
+#ifdef CALC_COULOMB
+#    ifdef CHECK_EXCLS
+    /* 1/r masked with the interaction mask */
+    SimdReal SP0rinv_ex_S0;
+    SimdReal SP1rinv_ex_S0;
+    SimdReal SP0rinv_ex_S2;
+    SimdReal SP1rinv_ex_S2;
+#    endif
+    SimdReal SP0jq_S;
+    SimdReal SP1jq_S;
+    SimdReal SP0qq_S0;
+    SimdReal SP1qq_S0;
+    SimdReal SP0qq_S2;
+    SimdReal SP1qq_S2;
+#    ifdef CALC_COUL_TAB
+    /* The force (PME mesh force) we need to subtract from 1/r^2 */
+    SimdReal SP0fsub_S0;
+    SimdReal SP1fsub_S0;
+    SimdReal SP0fsub_S2;
+    SimdReal SP1fsub_S2;
+#    endif
+#    ifdef CALC_COUL_EWALD
+    SimdReal SP0brsq_S0, SP0brsq_S2;
+    SimdReal SP1brsq_S0, SP1brsq_S2;
+    SimdReal SP0ewcorr_S0, SP0ewcorr_S2;
+    SimdReal SP1ewcorr_S0, SP1ewcorr_S2;
+#    endif
+
+    /* frcoul = (1/r - fsub)*r */
+    SimdReal SP0frcoul_S0;
+    SimdReal SP1frcoul_S0;
+    SimdReal SP0frcoul_S2;
+    SimdReal SP1frcoul_S2;
+#    ifdef CALC_COUL_TAB
+    /* For tables: r, rs=r/sp, rf=floor(rs), frac=rs-rf */
+    SimdReal SP0rs_S0, SP0rf_S0, SP0frac_S0;
+    SimdReal SP1rs_S0, SP1rf_S0, SP1frac_S0;
+    SimdReal SP0rs_S2, SP0rf_S2, SP0frac_S2;
+    SimdReal SP1rs_S2, SP1rf_S2, SP1frac_S2;
+    /* Table index: rs truncated to an int */
+    SimdInt32 SP0ti_S0, SP0ti_S2;
+    SimdInt32 SP1ti_S0, SP1ti_S2;
+    /* Linear force table values */
+    SimdReal SP0ctab0_S0, SP0ctab1_S0;
+    SimdReal SP1ctab0_S0, SP1ctab1_S0;
+    SimdReal SP0ctab0_S2, SP0ctab1_S2;
+    SimdReal SP1ctab0_S2, SP1ctab1_S2;
+#        ifdef CALC_ENERGIES
+    /* Quadratic energy table value */
+    SimdReal SP0ctabv_S0, SP0dum_S0;
+    SimdReal SP1ctabv_S0, SP1dum_S0;
+    SimdReal SP0ctabv_S2, SP0dum_S2;
+    SimdReal SP1ctabv_S2, SP1dum_S2;
+#        endif
+#    endif
+#    if defined CALC_ENERGIES && (defined CALC_COUL_EWALD || defined CALC_COUL_TAB)
+    /* The potential (PME mesh) we need to subtract from 1/r */
+    SimdReal SP0vc_sub_S0;
+    SimdReal SP1vc_sub_S0;
+    SimdReal SP0vc_sub_S2;
+    SimdReal SP1vc_sub_S2;
+#    endif
+#    ifdef CALC_ENERGIES
+    /* Electrostatic potential */
+    SimdReal SP0vcoul_S0;
+    SimdReal SP1vcoul_S0;
+    SimdReal SP0vcoul_S2;
+    SimdReal SP1vcoul_S2;
+#    endif
+#endif
+    /* The force times 1/r */
+    SimdReal SP0fscal_S0;
+    SimdReal SP1fscal_S0;
+    SimdReal SP0fscal_S2;
+    SimdReal SP1fscal_S2;
+
+#ifdef CALC_LJ
+#    ifdef LJ_COMB_LB
+    /* LJ sigma_j/2 and sqrt(epsilon_j) */
+    SimdReal SP0hsig_j_S, SP0seps_j_S;
+    SimdReal SP1hsig_j_S, SP1seps_j_S;
+    /* LJ sigma_ij and epsilon_ij */
+    SimdReal SP0sig_S0, SP0eps_S0;
+    SimdReal SP1sig_S0, SP1eps_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0sig_S2, SP0eps_S2;
+    SimdReal SP1sig_S2, SP1eps_S2;
+#        endif
+#        ifdef CALC_ENERGIES
+    SimdReal SP0sig2_S0, SP0sig6_S0;
+    SimdReal SP1sig2_S0, SP1sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0sig2_S2, SP0sig6_S2;
+    SimdReal SP1sig2_S2, SP1sig6_S2;
+#            endif
+#        endif /* LJ_COMB_LB */
+#    endif     /* CALC_LJ */
+
+#    ifdef LJ_COMB_GEOM
+    SimdReal SP0c6s_j_S, SP0c12s_j_S;
+    SimdReal SP1c6s_j_S, SP1c12s_j_S;
+#    endif
+
+    /* Intermediate variables for LJ calculation */
+#    ifndef LJ_COMB_LB
+    SimdReal SP0rinvsix_S0;
+    SimdReal SP1rinvsix_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0rinvsix_S2;
+    SimdReal SP1rinvsix_S2;
+#        endif
+#    endif
+#    ifdef LJ_COMB_LB
+    SimdReal SP0sir_S0, SP0sir2_S0, SP0sir6_S0;
+    SimdReal SP1sir_S0, SP1sir2_S0, SP1sir6_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0sir_S2, SP0sir2_S2, SP0sir6_S2;
+    SimdReal SP1sir_S2, SP1sir2_S2, SP1sir6_S2;
+#        endif
+#    endif
+
+    SimdReal SP0FrLJ6_S0, SP0FrLJ12_S0, SP0frLJ_S0;
+    SimdReal SP1FrLJ6_S0, SP1FrLJ12_S0, SP1frLJ_S0;
+#    ifndef HALF_LJ
+    SimdReal SP0FrLJ6_S2, SP0FrLJ12_S2, SP0frLJ_S2;
+    SimdReal SP1FrLJ6_S2, SP1FrLJ12_S2, SP1frLJ_S2;
+#    endif
+#endif /* CALC_LJ */
+
+    /* j-cluster index */
+    const int SP0cj = l_cj[cjind+0].cj;
+    const int SP1cj = l_cj[cjind+1].cj;
+
+    /* Atom indices (of the first atom in the cluster) */
+    const int SP0aj = SP0cj * UNROLLJ;
+    const int SP1aj = SP1cj * UNROLLJ;
+#if defined CALC_LJ && (defined LJ_COMB_GEOM || defined LJ_COMB_LB || defined LJ_EWALD_GEOM)
+    /* Index for loading LJ parameters, complicated when interleaving */
+    const int SP0aj2 = SP0aj * 2;
+    const int SP1aj2 = SP1aj * 2;
+#endif
+
+#ifdef CHECK_EXCLS
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+0].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+1].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+#endif /* CHECK_EXCLS */
+
+    /* load j atom coordinates */
+    loadDuplicate3Hsimd<STRIDE>(x + SP0aj * DIM, &SP0jx_S, &SP0jy_S, &SP0jz_S);
+    loadDuplicate3Hsimd<STRIDE>(x + SP1aj * DIM, &SP1jx_S, &SP1jy_S, &SP1jz_S);
+
+    /* Calculate distance */
+    SP0dx_S0 = ix_S0 - SP0jx_S;
+    SP1dx_S0 = ix_S0 - SP1jx_S;
+    SP0dy_S0 = iy_S0 - SP0jy_S;
+    SP1dy_S0 = iy_S0 - SP1jy_S;
+    SP0dz_S0 = iz_S0 - SP0jz_S;
+    SP1dz_S0 = iz_S0 - SP1jz_S;
+    SP0dx_S2 = ix_S2 - SP0jx_S;
+    SP1dx_S2 = ix_S2 - SP1jx_S;
+    SP0dy_S2 = iy_S2 - SP0jy_S;
+    SP1dy_S2 = iy_S2 - SP1jy_S;
+    SP0dz_S2 = iz_S2 - SP0jz_S;
+    SP1dz_S2 = iz_S2 - SP1jz_S;
+
+    /* rsq = dx*dx+dy*dy+dz*dz */
+    SP0rsq_S0 = norm2(SP0dx_S0, SP0dy_S0, SP0dz_S0);
+    SP1rsq_S0 = norm2(SP1dx_S0, SP1dy_S0, SP1dz_S0);
+    SP0rsq_S2 = norm2(SP0dx_S2, SP0dy_S2, SP0dz_S2);
+    SP1rsq_S2 = norm2(SP1dx_S2, SP1dy_S2, SP1dz_S2);
+
+    /* Do the cut-off check */
+    SP0wco_S0 = (SP0rsq_S0 < rc2_S);
+    SP1wco_S0 = (SP1rsq_S0 < rc2_S);
+    SP0wco_S2 = (SP0rsq_S2 < rc2_S);
+    SP1wco_S2 = (SP1rsq_S2 < rc2_S);
+
+#ifdef CHECK_EXCLS
+#    ifdef EXCL_FORCES
+    /* Only remove the (sub-)diagonal to avoid double counting */
+#        if UNROLLJ == UNROLLI
+    if (cj == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask_S0;
+        wco_S2 = wco_S2 && diagonal_mask_S2;
+    }
+#        else
+#            if UNROLLJ == 2 * UNROLLI
+    if (cj * 2 == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask0_S0;
+        wco_S2 = wco_S2 && diagonal_mask0_S2;
+    }
+    else if (cj * 2 + 1 == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask1_S0;
+        wco_S2 = wco_S2 && diagonal_mask1_S2;
+    }
+#            else
+#                error "only UNROLLJ == UNROLLI*(1 or 2) currently supported in 2xnn kernels"
+#            endif
+#        endif
+#    else /* EXCL_FORCES */
+    /* No exclusion forces: remove all excluded atom pairs from the list */
+    wco_S0 = wco_S0 && interact_S0;
+    wco_S2 = wco_S2 && interact_S2;
+#    endif
+#endif
+
+#ifdef COUNT_PAIRS
+    {
+        int                              i, j;
+        alignas(GMX_SIMD_ALIGNMENT) real tmp[GMX_SIMD_REAL_WIDTH];
+
+        for (i = 0; i < UNROLLI; i += 2)
+        {
+            store(tmp, rc2_S - (i == 0 ? rsq_S0 : rsq_S2));
+            for (j = 0; j < 2 * UNROLLJ; j++)
+            {
+                if (tmp[j] >= 0)
+                {
+                    npair++;
+                }
+            }
+        }
+    }
+#endif
+
+    // Ensure the distances do not fall below the limit where r^-12 overflows.
+    // This should never happen for normal interactions.
+    SP0rsq_S0 = max(SP0rsq_S0, minRsq_S);
+    SP1rsq_S0 = max(SP1rsq_S0, minRsq_S);
+    SP0rsq_S2 = max(SP0rsq_S2, minRsq_S);
+    SP1rsq_S2 = max(SP1rsq_S2, minRsq_S);
+
+    /* Calculate 1/r */
+#if SKIP_INVSQRT
+    SP0rinvsq_S0 = invMask(SP0rsq_S0, SP0wco_S0);
+    SP1rinvsq_S0 = invMask(SP1rsq_S0, SP1wco_S0);
+    SP0rinvsq_S2 = invMask(SP0rsq_S2, SP0wco_S2);
+    SP1rinvsq_S2 = invMask(SP1rsq_S2, SP1wco_S2);
+#else
+    /* and set rinv to zero for r beyond the cut-off */
+    SP0rinv_S0 = invsqrtMask(SP0rsq_S0, SP0wco_S0);
+    SP1rinv_S0 = invsqrtMask(SP1rsq_S0, SP1wco_S0);
+    SP0rinv_S2 = invsqrtMask(SP0rsq_S2, SP0wco_S2);
+    SP1rinv_S2 = invsqrtMask(SP1rsq_S2, SP1wco_S2);
+#endif
+
+#ifdef CALC_COULOMB
+    /* Load parameters for j atom */
+    SP0jq_S = loadDuplicateHsimd(q + SP0aj);
+    SP1jq_S = loadDuplicateHsimd(q + SP1aj);
+    SP0qq_S0 = iq_S0 * SP0jq_S;
+    SP1qq_S0 = iq_S0 * SP1jq_S;
+    SP0qq_S2 = iq_S2 * SP0jq_S;
+    SP1qq_S2 = iq_S2 * SP1jq_S;
+#endif
+
+#ifdef CALC_LJ
+#    if !defined LJ_COMB_GEOM && !defined LJ_COMB_LB && !defined FIX_LJ_C
+    SimdReal                                                     SP0c6_S0, SP0c12_S0;
+    SimdReal                                                     SP1c6_S0, SP1c12_S0;
+#        ifdef HALF_LJ
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP0aj, &SP0c6_S0, &SP0c12_S0);
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP1aj, &SP1c6_S0, &SP1c12_S0);
+#        else
+    SimdReal SP0c6_S2, SP0c12_S2;
+    SimdReal SP1c6_S2, SP1c12_S2;
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP0aj, &SP0c6_S0, &SP0c12_S0, &SP0c6_S2, &SP0c12_S2);
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP1aj, &SP1c6_S0, &SP1c12_S0, &SP1c6_S2, &SP1c12_S2);
+#        endif
+#    endif /* not defined any LJ rule */
+
+#    ifdef LJ_COMB_GEOM
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP0aj2, &SP0c6s_j_S, &SP0c12s_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP1aj2, &SP1c6s_j_S, &SP1c12s_j_S);
+    SimdReal SP0c6_S0 = c6s_S0 * SP0c6s_j_S;
+    SimdReal SP1c6_S0 = c6s_S0 * SP1c6s_j_S;
+#        ifndef HALF_LJ
+    SimdReal SP0c6_S2 = c6s_S2 * SP0c6s_j_S;
+    SimdReal SP1c6_S2 = c6s_S2 * SP1c6s_j_S;
+#        endif
+    SimdReal SP0c12_S0 = c12s_S0 * SP0c12s_j_S;
+    SimdReal SP1c12_S0 = c12s_S0 * SP1c12s_j_S;
+#        ifndef HALF_LJ
+    SimdReal SP0c12_S2 = c12s_S2 * SP0c12s_j_S;
+    SimdReal SP1c12_S2 = c12s_S2 * SP1c12s_j_S;
+#        endif
+#    endif /* LJ_COMB_GEOM */
+
+#    ifdef LJ_COMB_LB
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP0aj2, &SP0hsig_j_S, &SP0seps_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP1aj2, &SP1hsig_j_S, &SP1seps_j_S);
+
+    SP0sig_S0 = hsig_i_S0 + SP0hsig_j_S;
+    SP1sig_S0 = hsig_i_S0 + SP1hsig_j_S;
+    SP0eps_S0 = seps_i_S0 * SP0seps_j_S;
+    SP1eps_S0 = seps_i_S0 * SP1seps_j_S;
+#        ifndef HALF_LJ
+    SP0sig_S2 = hsig_i_S2 + SP0hsig_j_S;
+    SP1sig_S2 = hsig_i_S2 + SP1hsig_j_S;
+    SP0eps_S2 = seps_i_S2 * SP0seps_j_S;
+    SP1eps_S2 = seps_i_S2 * SP1seps_j_S;
+#        endif
+#    endif /* LJ_COMB_LB */
+
+#endif /* CALC_LJ */
+
+#if !SKIP_INVSQRT
+    SP0rinvsq_S0 = SP0rinv_S0 * SP0rinv_S0;
+    SP1rinvsq_S0 = SP1rinv_S0 * SP1rinv_S0;
+    SP0rinvsq_S2 = SP0rinv_S2 * SP0rinv_S2;
+    SP1rinvsq_S2 = SP1rinv_S2 * SP1rinv_S2;
+#endif
+
+#ifdef CALC_COULOMB
+    /* Note that here we calculate force*r, not the usual force/r.
+     * This allows avoiding masking the reaction-field contribution,
+     * as frcoul is later multiplied by rinvsq which has been
+     * masked with the cut-off check.
+     */
+
+#    ifdef EXCL_FORCES
+    /* Only add 1/r for non-excluded atom pairs */
+    rinv_ex_S0 = selectByMask(rinv_S0, interact_S0);
+    rinv_ex_S2 = selectByMask(rinv_S2, interact_S2);
+#    else
+    /* No exclusion forces, we always need 1/r */
+#        define SP0rinv_ex_S0 SP0rinv_S0
+#        define SP1rinv_ex_S0 SP1rinv_S0
+#        define SP0rinv_ex_S2 SP0rinv_S2
+#        define SP1rinv_ex_S2 SP1rinv_S2
+#    endif
+
+#    ifdef CALC_COUL_RF
+    /* Electrostatic interactions */
+    SP0frcoul_S0 = SP0qq_S0 * fma(SP0rsq_S0, mrc_3_S, SP0rinv_ex_S0);
+    SP1frcoul_S0 = SP1qq_S0 * fma(SP1rsq_S0, mrc_3_S, SP1rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fma(SP0rsq_S2, mrc_3_S, SP0rinv_ex_S2);
+    SP1frcoul_S2 = SP1qq_S2 * fma(SP1rsq_S2, mrc_3_S, SP1rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    /* and (merge) mask energy for cut-off and diagonal */
+    SP0vcoul_S0 = maskzMul(SP0qq_S0, SP0rinv_ex_S0 + fma(SP0rsq_S0, hrc_3_S, moh_rc_S), SP0wco_S0);
+    SP1vcoul_S0 = maskzMul(SP1qq_S0, SP1rinv_ex_S0 + fma(SP1rsq_S0, hrc_3_S, moh_rc_S), SP1wco_S0);
+    SP0vcoul_S2 = maskzMul(SP0qq_S2, SP0rinv_ex_S2 + fma(SP0rsq_S2, hrc_3_S, moh_rc_S), SP0wco_S2);
+    SP1vcoul_S2 = maskzMul(SP1qq_S2, SP1rinv_ex_S2 + fma(SP1rsq_S2, hrc_3_S, moh_rc_S), SP1wco_S2);
+#        endif
+#    endif
+
+#    ifdef CALC_COUL_EWALD
+    /* We need to mask (or limit) rsq for the cut-off,
+     * as large distances can cause an overflow in gmx_pmecorrF/V.
+     */
+    SP0brsq_S0   = maskzMul(beta2_S, SP0rsq_S0, SP0wco_S0);
+    SP1brsq_S0   = maskzMul(beta2_S, SP1rsq_S0, SP1wco_S0);
+    SP0brsq_S2   = maskzMul(beta2_S, SP0rsq_S2, SP0wco_S2);
+    SP1brsq_S2   = maskzMul(beta2_S, SP1rsq_S2, SP1wco_S2);
+    SP0ewcorr_S0 = beta_S * pmeForceCorrection(SP0brsq_S0);
+    SP1ewcorr_S0 = beta_S * pmeForceCorrection(SP1brsq_S0);
+    SP0ewcorr_S2 = beta_S * pmeForceCorrection(SP0brsq_S2);
+    SP1ewcorr_S2 = beta_S * pmeForceCorrection(SP1brsq_S2);
+    SP0frcoul_S0 = SP0qq_S0 * fma(SP0ewcorr_S0, SP0brsq_S0, SP0rinv_ex_S0);
+    SP1frcoul_S0 = SP1qq_S0 * fma(SP1ewcorr_S0, SP1brsq_S0, SP1rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fma(SP0ewcorr_S2, SP0brsq_S2, SP0rinv_ex_S2);
+    SP1frcoul_S2 = SP1qq_S2 * fma(SP1ewcorr_S2, SP1brsq_S2, SP1rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    SP0vc_sub_S0 = beta_S * pmePotentialCorrection(SP0brsq_S0);
+    SP1vc_sub_S0 = beta_S * pmePotentialCorrection(SP1brsq_S0);
+    SP0vc_sub_S2 = beta_S * pmePotentialCorrection(SP0brsq_S2);
+    SP1vc_sub_S2 = beta_S * pmePotentialCorrection(SP1brsq_S2);
+#        endif
+
+#    endif /* CALC_COUL_EWALD */
+
+#    ifdef CALC_COUL_TAB
+    /* Electrostatic interactions */
+    SP0r_S0 = SP0rsq_S0 * SP0rinv_S0;
+    SP1r_S0 = SP1rsq_S0 * SP1rinv_S0;
+    SP0r_S2 = SP0rsq_S2 * SP0rinv_S2;
+    SP1r_S2 = SP1rsq_S2 * SP1rinv_S2;
+    /* Convert r to scaled table units */
+    SP0rs_S0 = SP0r_S0 * invtsp_S;
+    SP1rs_S0 = SP1r_S0 * invtsp_S;
+    SP0rs_S2 = SP0r_S2 * invtsp_S;
+    SP1rs_S2 = SP1r_S2 * invtsp_S;
+    /* Truncate scaled r to an int */
+    SP0ti_S0 = cvttR2I(SP0rs_S0);
+    SP1ti_S0 = cvttR2I(SP1rs_S0);
+    SP0ti_S2 = cvttR2I(SP0rs_S2);
+    SP1ti_S2 = cvttR2I(SP1rs_S2);
+
+    SP0rf_S0 = trunc(SP0rs_S0);
+    SP1rf_S0 = trunc(SP1rs_S0);
+    SP0rf_S2 = trunc(SP0rs_S2);
+    SP1rf_S2 = trunc(SP1rs_S2);
+
+    SP0frac_S0 = SP0rs_S0 - SP0rf_S0;
+    SP1frac_S0 = SP1rs_S0 - SP1rf_S0;
+    SP0frac_S2 = SP0rs_S2 - SP0rf_S2;
+    SP1frac_S2 = SP1rs_S2 - SP1rf_S2;
+
+    /* Load and interpolate table forces and possibly energies.
+     * Force and energy can be combined in one table, stride 4: FDV0
+     * or in two separate tables with stride 1: F and V
+     * Currently single precision uses FDV0, double F and V.
+     */
+#        ifndef CALC_ENERGIES
+#            ifdef TAB_FDV0
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S0, &ctab0_S0, &ctab1_S0);
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S2, &ctab0_S2, &ctab1_S2);
+#            else
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S0, &SP0ctab0_S0, &SP0ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S0, &SP1ctab0_S0, &SP1ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S2, &SP0ctab0_S2, &SP0ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S2, &SP1ctab0_S2, &SP1ctab1_S2);
+    SP0ctab1_S0   = SP0ctab1_S0 - SP0ctab0_S0;
+    SP1ctab1_S0   = SP1ctab1_S0 - SP1ctab0_S0;
+    SP0ctab1_S2   = SP0ctab1_S2 - SP0ctab0_S2;
+    SP1ctab1_S2   = SP1ctab1_S2 - SP1ctab0_S2;
+#            endif
+#        else
+#            ifdef TAB_FDV0
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S0, &ctab0_S0, &ctab1_S0, &ctabv_S0, &dum_S0);
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S2, &ctab0_S2, &ctab1_S2, &ctabv_S2, &dum_S2);
+#            else
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S0, &SP0ctab0_S0, &SP0ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S0, &SP1ctab0_S0, &SP1ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP0ti_S0, &SP0ctabv_S0, &SP0dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP1ti_S0, &SP1ctabv_S0, &SP1dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S2, &SP0ctab0_S2, &SP0ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S2, &SP1ctab0_S2, &SP1ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP0ti_S2, &SP0ctabv_S2, &SP0dum_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP1ti_S2, &SP1ctabv_S2, &SP1dum_S2);
+    SP0ctab1_S0 = SP0ctab1_S0 - SP0ctab0_S0;
+    SP1ctab1_S0 = SP1ctab1_S0 - SP1ctab0_S0;
+    SP0ctab1_S2 = SP0ctab1_S2 - SP0ctab0_S2;
+    SP1ctab1_S2 = SP1ctab1_S2 - SP1ctab0_S2;
+#            endif
+#        endif
+    SP0fsub_S0   = fma(SP0frac_S0, SP0ctab1_S0, SP0ctab0_S0);
+    SP1fsub_S0   = fma(SP1frac_S0, SP1ctab1_S0, SP1ctab0_S0);
+    SP0fsub_S2   = fma(SP0frac_S2, SP0ctab1_S2, SP0ctab0_S2);
+    SP1fsub_S2   = fma(SP1frac_S2, SP1ctab1_S2, SP1ctab0_S2);
+    SP0frcoul_S0 = SP0qq_S0 * fnma(SP0fsub_S0, SP0r_S0, SP0rinv_ex_S0);
+    SP1frcoul_S0 = SP1qq_S0 * fnma(SP1fsub_S0, SP1r_S0, SP1rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fnma(SP0fsub_S2, SP0r_S2, SP0rinv_ex_S2);
+    SP1frcoul_S2 = SP1qq_S2 * fnma(SP1fsub_S2, SP1r_S2, SP1rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    SP0vc_sub_S0 = fma((mhalfsp_S * SP0frac_S0), (SP0ctab0_S0 + SP0fsub_S0), SP0ctabv_S0);
+    SP1vc_sub_S0 = fma((mhalfsp_S * SP1frac_S0), (SP1ctab0_S0 + SP1fsub_S0), SP1ctabv_S0);
+    SP0vc_sub_S2 = fma((mhalfsp_S * SP0frac_S2), (SP0ctab0_S2 + SP0fsub_S2), SP0ctabv_S2);
+    SP1vc_sub_S2 = fma((mhalfsp_S * SP1frac_S2), (SP1ctab0_S2 + SP1fsub_S2), SP1ctabv_S2);
+#        endif
+#    endif /* CALC_COUL_TAB */
+
+#    if defined CALC_ENERGIES && (defined CALC_COUL_EWALD || defined CALC_COUL_TAB)
+#        ifndef NO_SHIFT_EWALD
+    /* Add Ewald potential shift to vc_sub for convenience */
+#            ifdef CHECK_EXCLS
+    vc_sub_S0 = maskAdd(vc_sub_S0, sh_ewald_S, interact_S0);
+    vc_sub_S2 = maskAdd(vc_sub_S2, sh_ewald_S, interact_S2);
+#            else
+    SP0vc_sub_S0  = SP0vc_sub_S0 + sh_ewald_S;
+    SP1vc_sub_S0  = SP1vc_sub_S0 + sh_ewald_S;
+    SP0vc_sub_S2  = SP0vc_sub_S2 + sh_ewald_S;
+    SP1vc_sub_S2  = SP1vc_sub_S2 + sh_ewald_S;
+#            endif
+#        endif
+
+    /* and (merge) mask energy for cut-off and diagonal */
+    SP0vcoul_S0 = maskzMul(SP0qq_S0, SP0rinv_ex_S0 - SP0vc_sub_S0, SP0wco_S0);
+    SP1vcoul_S0 = maskzMul(SP1qq_S0, SP1rinv_ex_S0 - SP1vc_sub_S0, SP1wco_S0);
+    SP0vcoul_S2 = maskzMul(SP0qq_S2, SP0rinv_ex_S2 - SP0vc_sub_S2, SP0wco_S2);
+    SP1vcoul_S2 = maskzMul(SP1qq_S2, SP1rinv_ex_S2 - SP1vc_sub_S2, SP1wco_S2);
+
+#    endif
+
+#endif /* CALC_COULOMB */
+
+#ifdef CALC_LJ
+    /* Lennard-Jones interaction */
+
+#    ifdef VDW_CUTOFF_CHECK
+    SP0wco_vdw_S0 = (SP0rsq_S0 < rcvdw2_S);
+    SP1wco_vdw_S0 = (SP1rsq_S0 < rcvdw2_S);
+#        ifndef HALF_LJ
+    SP0wco_vdw_S2 = (SP0rsq_S2 < rcvdw2_S);
+    SP1wco_vdw_S2 = (SP1rsq_S2 < rcvdw2_S);
+#        endif
+#    else
+    /* Same cut-off for Coulomb and VdW, reuse the registers */
+#        define SP0wco_vdw_S0 SP0wco_S0
+#        define SP1wco_vdw_S0 SP1wco_S0
+#        define SP0wco_vdw_S2 SP0wco_S2
+#        define SP1wco_vdw_S2 SP1wco_S2
+#    endif
+
+#    ifndef LJ_COMB_LB
+    SP0rinvsix_S0 = SP0rinvsq_S0 * SP0rinvsq_S0;
+    SP1rinvsix_S0 = SP1rinvsq_S0 * SP1rinvsq_S0;
+#        ifdef EXCL_FORCES
+    rinvsix_S0 = maskzMul(rinvsix_S0, rinvsq_S0, interact_S0);
+#        else
+    SP0rinvsix_S0 = SP0rinvsix_S0 * SP0rinvsq_S0;
+    SP1rinvsix_S0 = SP1rinvsix_S0 * SP1rinvsq_S0;
+#        endif
+#        ifndef HALF_LJ
+    SP0rinvsix_S2 = SP0rinvsq_S2 * SP0rinvsq_S2;
+    SP1rinvsix_S2 = SP1rinvsq_S2 * SP1rinvsq_S2;
+#            ifdef EXCL_FORCES
+    rinvsix_S2 = maskzMul(rinvsix_S2, rinvsq_S2, interact_S2);
+#            else
+    SP0rinvsix_S2 = SP0rinvsix_S2 * SP0rinvsq_S2;
+    SP1rinvsix_S2 = SP1rinvsix_S2 * SP1rinvsq_S2;
+#            endif
+#        endif
+
+#        if defined LJ_CUT || defined LJ_POT_SWITCH
+    /* We have plain LJ or LJ-PME with simple C6/6 C12/12 coefficients */
+    SP0FrLJ6_S0 = SP0c6_S0 * SP0rinvsix_S0;
+    SP1FrLJ6_S0 = SP1c6_S0 * SP1rinvsix_S0;
+#            ifndef HALF_LJ
+    SP0FrLJ6_S2 = SP0c6_S2 * SP0rinvsix_S2;
+    SP1FrLJ6_S2 = SP1c6_S2 * SP1rinvsix_S2;
+#            endif
+    SP0FrLJ12_S0 = SP0c12_S0 * SP0rinvsix_S0 * SP0rinvsix_S0;
+    SP1FrLJ12_S0 = SP1c12_S0 * SP1rinvsix_S0 * SP1rinvsix_S0;
+#            ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0c12_S2 * SP0rinvsix_S2 * SP0rinvsix_S2;
+    SP1FrLJ12_S2 = SP1c12_S2 * SP1rinvsix_S2 * SP1rinvsix_S2;
+#            endif
+#        endif
+
+#        if defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    /* We switch the LJ force */
+    SP0r_S0    = SP0rsq_S0 * SP0rinv_S0;
+    SP1r_S0    = SP1rsq_S0 * SP1rinv_S0;
+    SP0rsw_S0  = max(SP0r_S0 - rswitch_S, zero_S);
+    SP1rsw_S0  = max(SP1r_S0 - rswitch_S, zero_S);
+    SP0rsw2_S0 = SP0rsw_S0 * SP0rsw_S0;
+    SP1rsw2_S0 = SP1rsw_S0 * SP1rsw_S0;
+#            ifndef HALF_LJ
+    SP0r_S2    = SP0rsq_S2 * SP0rinv_S2;
+    SP1r_S2    = SP1rsq_S2 * SP1rinv_S2;
+    SP0rsw_S2  = max(SP0r_S2 - rswitch_S, zero_S);
+    SP1rsw_S2  = max(SP1r_S2 - rswitch_S, zero_S);
+    SP0rsw2_S2 = SP0rsw_S2 * SP0rsw_S2;
+    SP1rsw2_S2 = SP1rsw_S2 * SP1rsw_S2;
+#            endif
+#        endif
+
+#        ifdef LJ_FORCE_SWITCH
+
+#            define add_fr_switch(fr, rsw, rsw2_r, c2, c3) fma(fma(c3, rsw, c2), rsw2_r, fr)
+    SimdReal SP0rsw2_r_S0 = SP0rsw2_S0 * SP0r_S0;
+    SimdReal SP1rsw2_r_S0 = SP1rsw2_S0 * SP1r_S0;
+    SP0FrLJ6_S0           = SP0c6_S0 * add_fr_switch(SP0rinvsix_S0, SP0rsw_S0, SP0rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+    SP1FrLJ6_S0           = SP1c6_S0 * add_fr_switch(SP1rinvsix_S0, SP1rsw_S0, SP1rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+#            ifndef HALF_LJ
+    SimdReal SP0rsw2_r_S2 = SP0rsw2_S2 * SP0r_S2;
+    SimdReal SP1rsw2_r_S2 = SP1rsw2_S2 * SP1r_S2;
+    SP0FrLJ6_S2           = SP0c6_S2 * add_fr_switch(SP0rinvsix_S2, SP0rsw_S2, SP0rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+    SP1FrLJ6_S2           = SP1c6_S2 * add_fr_switch(SP1rinvsix_S2, SP1rsw_S2, SP1rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+#            endif
+    SP0FrLJ12_S0 = SP0c12_S0 * add_fr_switch(SP0rinvsix_S0 * SP0rinvsix_S0, SP0rsw_S0, SP0rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+    SP1FrLJ12_S0 = SP1c12_S0 * add_fr_switch(SP1rinvsix_S0 * SP1rinvsix_S0, SP1rsw_S0, SP1rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+#            ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0c12_S2 * add_fr_switch(SP0rinvsix_S2 * SP0rinvsix_S2, SP0rsw_S2, SP0rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+    SP1FrLJ12_S2 = SP1c12_S2 * add_fr_switch(SP1rinvsix_S2 * SP1rinvsix_S2, SP1rsw_S2, SP1rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+#            endif
+#            undef add_fr_switch
+#        endif /* LJ_FORCE_SWITCH */
+
+#    endif /* not LJ_COMB_LB */
+
+#    ifdef LJ_COMB_LB
+    SP0sir_S0 = SP0sig_S0 * SP0rinv_S0;
+    SP1sir_S0 = SP1sig_S0 * SP1rinv_S0;
+#        ifndef HALF_LJ
+    SP0sir_S2 = SP0sig_S2 * SP0rinv_S2;
+    SP1sir_S2 = SP1sig_S2 * SP1rinv_S2;
+#        endif
+    SP0sir2_S0 = SP0sir_S0 * SP0sir_S0;
+    SP1sir2_S0 = SP1sir_S0 * SP1sir_S0;
+#        ifndef HALF_LJ
+    SP0sir2_S2 = SP0sir_S2 * SP0sir_S2;
+    SP1sir2_S2 = SP1sir_S2 * SP1sir_S2;
+#        endif
+#        ifdef VDW_CUTOFF_CHECK
+    SP0sir6_S0 = maskzMul(SP0sir2_S0, SP0sir2_S0, SP0wco_vdw_S0);
+    SP1sir6_S0 = maskzMul(SP1sir2_S0, SP1sir2_S0, SP1wco_vdw_S0);
+#        else
+    SP0sir6_S0    = SP0sir2_S0 * SP0sir2_S0;
+    SP1sir6_S0    = SP1sir2_S0 * SP1sir2_S0;
+#        endif
+#        ifdef EXCL_FORCES
+    sir6_S0 = maskzMul(sir6_S0, sir2_S0, interact_S0);
+#        else
+    SP0sir6_S0    = SP0sir6_S0 * SP0sir2_S0;
+    SP1sir6_S0    = SP1sir6_S0 * SP1sir2_S0;
+#        endif
+#        ifndef HALF_LJ
+#            ifdef VDW_CUTOFF_CHECK
+    SP0sir6_S2 = maskzMul(SP0sir2_S2, SP0sir2_S2, SP0wco_vdw_S2);
+    SP1sir6_S2 = maskzMul(SP1sir2_S2, SP1sir2_S2, SP1wco_vdw_S2);
+#            else
+    SP0sir6_S2    = SP0sir2_S2 * SP0sir2_S2;
+    SP1sir6_S2    = SP1sir2_S2 * SP1sir2_S2;
+#            endif
+#            ifdef EXCL_FORCES
+    sir6_S2 = maskzMul(sir6_S2, sir2_S2, interact_S2);
+#            else
+    SP0sir6_S2    = SP0sir6_S2 * SP0sir2_S2;
+    SP1sir6_S2    = SP1sir6_S2 * SP1sir2_S2;
+#            endif
+#        endif
+    SP0FrLJ6_S0 = SP0eps_S0 * SP0sir6_S0;
+    SP1FrLJ6_S0 = SP1eps_S0 * SP1sir6_S0;
+#        ifndef HALF_LJ
+    SP0FrLJ6_S2 = SP0eps_S2 * SP0sir6_S2;
+    SP1FrLJ6_S2 = SP1eps_S2 * SP1sir6_S2;
+#        endif
+    SP0FrLJ12_S0 = SP0FrLJ6_S0 * SP0sir6_S0;
+    SP1FrLJ12_S0 = SP1FrLJ6_S0 * SP1sir6_S0;
+#        ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0FrLJ6_S2 * SP0sir6_S2;
+    SP1FrLJ12_S2 = SP1FrLJ6_S2 * SP1sir6_S2;
+#        endif
+#        if defined CALC_ENERGIES
+    /* We need C6 and C12 to calculate the LJ potential shift */
+    SP0sig2_S0 = SP0sig_S0 * SP0sig_S0;
+    SP1sig2_S0 = SP1sig_S0 * SP1sig_S0;
+#            ifndef HALF_LJ
+    SP0sig2_S2 = SP0sig_S2 * SP0sig_S2;
+    SP1sig2_S2 = SP1sig_S2 * SP1sig_S2;
+#            endif
+    SP0sig6_S0 = SP0sig2_S0 * SP0sig2_S0 * SP0sig2_S0;
+    SP1sig6_S0 = SP1sig2_S0 * SP1sig2_S0 * SP1sig2_S0;
+#            ifndef HALF_LJ
+    SP0sig6_S2 = SP0sig2_S2 * SP0sig2_S2 * SP0sig2_S2;
+    SP1sig6_S2 = SP1sig2_S2 * SP1sig2_S2 * SP1sig2_S2;
+#            endif
+    SimdReal SP0c6_S0 = SP0eps_S0 * SP0sig6_S0;
+    SimdReal SP1c6_S0 = SP1eps_S0 * SP1sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0c6_S2 = SP0eps_S2 * SP0sig6_S2;
+    SimdReal SP1c6_S2 = SP1eps_S2 * SP1sig6_S2;
+#            endif
+    SimdReal SP0c12_S0 = SP0c6_S0 * SP0sig6_S0;
+    SimdReal SP1c12_S0 = SP1c6_S0 * SP1sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0c12_S2 = SP0c6_S2 * SP0sig6_S2;
+    SimdReal SP1c12_S2 = SP1c6_S2 * SP1sig6_S2;
+#            endif
+#        endif
+#    endif /* LJ_COMB_LB */
+
+    /* Determine the total scalar LJ force*r */
+    SP0frLJ_S0 = SP0FrLJ12_S0 - SP0FrLJ6_S0;
+    SP1frLJ_S0 = SP1FrLJ12_S0 - SP1FrLJ6_S0;
+#    ifndef HALF_LJ
+    SP0frLJ_S2 = SP0FrLJ12_S2 - SP0FrLJ6_S2;
+    SP1frLJ_S2 = SP1FrLJ12_S2 - SP1FrLJ6_S2;
+#    endif
+
+#    if (defined LJ_CUT || defined LJ_FORCE_SWITCH) && defined CALC_ENERGIES
+
+#        ifdef LJ_CUT
+    /* Calculate the LJ energies, with constant potential shift */
+    SimdReal SP0VLJ6_S0 = sixth_S * fma(SP0c6_S0, p6_cpot_S, SP0FrLJ6_S0);
+    SimdReal SP1VLJ6_S0 = sixth_S * fma(SP1c6_S0, p6_cpot_S, SP1FrLJ6_S0);
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ6_S2 = sixth_S * fma(SP0c6_S2, p6_cpot_S, SP0FrLJ6_S2);
+    SimdReal SP1VLJ6_S2 = sixth_S * fma(SP1c6_S2, p6_cpot_S, SP1FrLJ6_S2);
+#            endif
+    SimdReal SP0VLJ12_S0 = twelveth_S * fma(SP0c12_S0, p12_cpot_S, SP0FrLJ12_S0);
+    SimdReal SP1VLJ12_S0 = twelveth_S * fma(SP1c12_S0, p12_cpot_S, SP1FrLJ12_S0);
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ12_S2 = twelveth_S * fma(SP0c12_S2, p12_cpot_S, SP0FrLJ12_S2);
+    SimdReal SP1VLJ12_S2 = twelveth_S * fma(SP1c12_S2, p12_cpot_S, SP1FrLJ12_S2);
+#            endif
+#        endif /* LJ_CUT */
+
+#        ifdef LJ_FORCE_SWITCH
+#            define v_fswitch_pr(rsw, rsw2, c0, c3, c4) fma(fma(c4, rsw, c3), (rsw2) * (rsw), c0)
+
+    SimdReal SP0VLJ6_S0 = SP0c6_S0 * fma(sixth_S, SP0rinvsix_S0, v_fswitch_pr(SP0rsw_S0, SP0rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP1VLJ6_S0 = SP1c6_S0 * fma(sixth_S, SP1rinvsix_S0, v_fswitch_pr(SP1rsw_S0, SP1rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ6_S2 = SP0c6_S2 * fma(sixth_S, SP0rinvsix_S2, v_fswitch_pr(SP0rsw_S2, SP0rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP1VLJ6_S2 = SP1c6_S2 * fma(sixth_S, SP1rinvsix_S2, v_fswitch_pr(SP1rsw_S2, SP1rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+#            endif
+    SimdReal SP0VLJ12_S0 = SP0c12_S0 * fma(twelveth_S, SP0rinvsix_S0 * SP0rinvsix_S0, v_fswitch_pr(SP0rsw_S0, SP0rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP1VLJ12_S0 = SP1c12_S0 * fma(twelveth_S, SP1rinvsix_S0 * SP1rinvsix_S0, v_fswitch_pr(SP1rsw_S0, SP1rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ12_S2 = SP0c12_S2 * fma(twelveth_S, SP0rinvsix_S2 * SP0rinvsix_S2, v_fswitch_pr(SP0rsw_S2, SP0rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP1VLJ12_S2 = SP1c12_S2 * fma(twelveth_S, SP1rinvsix_S2 * SP1rinvsix_S2, v_fswitch_pr(SP1rsw_S2, SP1rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+#            endif
+#            undef v_fswitch_pr
+#        endif /* LJ_FORCE_SWITCH */
+
+    /* Add up the repulsion and dispersion */
+    SimdReal SP0VLJ_S0 = SP0VLJ12_S0 - SP0VLJ6_S0;
+    SimdReal SP1VLJ_S0 = SP1VLJ12_S0 - SP1VLJ6_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0VLJ_S2 = SP0VLJ12_S2 - SP0VLJ6_S2;
+    SimdReal SP1VLJ_S2 = SP1VLJ12_S2 - SP1VLJ6_S2;
+#        endif
+
+#    endif /* (LJ_CUT || LJ_FORCE_SWITCH) && CALC_ENERGIES */
+
+#    ifdef LJ_POT_SWITCH
+    /* We always need the potential, since it is needed for the force */
+    SimdReal SP0VLJ_S0 = fnma(sixth_S, SP0FrLJ6_S0, twelveth_S * SP0FrLJ12_S0);
+    SimdReal SP1VLJ_S0 = fnma(sixth_S, SP1FrLJ6_S0, twelveth_S * SP1FrLJ12_S0);
+#        ifndef HALF_LJ
+    SimdReal SP0VLJ_S2 = fnma(sixth_S, SP0FrLJ6_S2, twelveth_S * SP0FrLJ12_S2);
+    SimdReal SP1VLJ_S2 = fnma(sixth_S, SP1FrLJ6_S2, twelveth_S * SP1FrLJ12_S2);
+#        endif
+
+    {
+        SimdReal SP0sw_S0, SP0dsw_S0;
+        SimdReal SP1sw_S0, SP1dsw_S0;
+#        ifndef HALF_LJ
+        SimdReal SP0sw_S2, SP0dsw_S2;
+        SimdReal SP1sw_S2, SP1dsw_S2;
+#        endif
+
+#        define switch_pr(rsw, rsw2, c3, c4, c5) \
+            fma(fma(fma(c5, rsw, c4), rsw, c3), (rsw2) * (rsw), one_S)
+#        define dswitch_pr(rsw, rsw2, c2, c3, c4) fma(fma(c4, rsw, c3), rsw, c2) * (rsw2)
+
+        SP0sw_S0  = switch_pr(SP0rsw_S0, SP0rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP1sw_S0  = switch_pr(SP1rsw_S0, SP1rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP0dsw_S0 = dswitch_pr(SP0rsw_S0, SP0rsw2_S0, swF2_S, swF3_S, swF4_S);
+        SP1dsw_S0 = dswitch_pr(SP1rsw_S0, SP1rsw2_S0, swF2_S, swF3_S, swF4_S);
+#        ifndef HALF_LJ
+        SP0sw_S2  = switch_pr(SP0rsw_S2, SP0rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP1sw_S2  = switch_pr(SP1rsw_S2, SP1rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP0dsw_S2 = dswitch_pr(SP0rsw_S2, SP0rsw2_S2, swF2_S, swF3_S, swF4_S);
+        SP1dsw_S2 = dswitch_pr(SP1rsw_S2, SP1rsw2_S2, swF2_S, swF3_S, swF4_S);
+#        endif
+        SP0frLJ_S0 = fnma(SP0dsw_S0 * SP0VLJ_S0, SP0r_S0, SP0sw_S0 * SP0frLJ_S0);
+        SP1frLJ_S0 = fnma(SP1dsw_S0 * SP1VLJ_S0, SP1r_S0, SP1sw_S0 * SP1frLJ_S0);
+#        ifndef HALF_LJ
+        SP0frLJ_S2 = fnma(SP0dsw_S2 * SP0VLJ_S2, SP0r_S2, SP0sw_S2 * SP0frLJ_S2);
+        SP1frLJ_S2 = fnma(SP1dsw_S2 * SP1VLJ_S2, SP1r_S2, SP1sw_S2 * SP1frLJ_S2);
+#        endif
+#        ifdef CALC_ENERGIES
+        SP0VLJ_S0 = SP0sw_S0 * SP0VLJ_S0;
+        SP1VLJ_S0 = SP1sw_S0 * SP1VLJ_S0;
+#            ifndef HALF_LJ
+        SP0VLJ_S2 = SP0sw_S2 * SP0VLJ_S2;
+        SP1VLJ_S2 = SP1sw_S2 * SP1VLJ_S2;
+#            endif
+#        endif
+
+#        undef switch_pr
+#        undef dswitch_pr
+    }
+#    endif /* LJ_POT_SWITCH */
+
+#    if defined CALC_ENERGIES && defined CHECK_EXCLS
+    /* The potential shift should be removed for excluded pairs */
+    VLJ_S0 = selectByMask(VLJ_S0, interact_S0);
+#        ifndef HALF_LJ
+    VLJ_S2 = selectByMask(VLJ_S2, interact_S2);
+#        endif
+#    endif
+
+#    ifdef LJ_EWALD_GEOM
+    {
+        SimdReal SP0c6s_j_S;
+        SimdReal SP1c6s_j_S;
+        SimdReal SP0c6grid_S0, SP0rinvsix_nm_S0, SP0cr2_S0, SP0expmcr2_S0, SP0poly_S0;
+        SimdReal SP1c6grid_S0, SP1rinvsix_nm_S0, SP1cr2_S0, SP1expmcr2_S0, SP1poly_S0;
+#        ifndef HALF_LJ
+        SimdReal SP0c6grid_S2, SP0rinvsix_nm_S2, SP0cr2_S2, SP0expmcr2_S2, SP0poly_S2;
+        SimdReal SP1c6grid_S2, SP1rinvsix_nm_S2, SP1cr2_S2, SP1expmcr2_S2, SP1poly_S2;
+#        endif
+#        ifdef CALC_ENERGIES
+        SimdReal SP0sh_mask_S0;
+        SimdReal SP1sh_mask_S0;
+#            ifndef HALF_LJ
+        SimdReal SP0sh_mask_S2;
+        SimdReal SP1sh_mask_S2;
+#            endif
+#        endif
+
+        /* Determine C6 for the grid using the geometric combination rule */
+        SP0c6s_j_S   = loadDuplicateHsimd(ljc + SP0aj2);
+        SP1c6s_j_S   = loadDuplicateHsimd(ljc + SP1aj2);
+        SP0c6grid_S0 = c6s_S0 * SP0c6s_j_S;
+        SP1c6grid_S0 = c6s_S0 * SP1c6s_j_S;
+#        ifndef HALF_LJ
+        SP0c6grid_S2 = c6s_S2 * SP0c6s_j_S;
+        SP1c6grid_S2 = c6s_S2 * SP1c6s_j_S;
+#        endif
+
+#        ifdef CHECK_EXCLS
+        /* Recalculate rinvsix without exclusion mask (compiler might optimize) */
+        rinvsix_nm_S0 = rinvsq_S0 * rinvsq_S0 * rinvsq_S0;
+#            ifndef HALF_LJ
+        rinvsix_nm_S2 = rinvsq_S2 * rinvsq_S2 * rinvsq_S2;
+#            endif
+#        else
+        /* We didn't use a mask, so we can copy */
+        SP0rinvsix_nm_S0 = SP0rinvsix_S0;
+        SP1rinvsix_nm_S0 = SP1rinvsix_S0;
+#            ifndef HALF_LJ
+        SP0rinvsix_nm_S2 = SP0rinvsix_S2;
+        SP1rinvsix_nm_S2 = SP1rinvsix_S2;
+#            endif
+#        endif
+
+        /* Mask for the cut-off to avoid overflow of cr2^2 */
+        SP0cr2_S0 = maskzMul(lje_c2_S, SP0rsq_S0, SP0wco_vdw_S0);
+        SP1cr2_S0 = maskzMul(lje_c2_S, SP1rsq_S0, SP1wco_vdw_S0);
+#        ifndef HALF_LJ
+        SP0cr2_S2 = maskzMul(lje_c2_S, SP0rsq_S2, SP0wco_vdw_S2);
+        SP1cr2_S2 = maskzMul(lje_c2_S, SP1rsq_S2, SP1wco_vdw_S2);
+#        endif
+        // Unsafe version of our exp() should be fine, since these arguments should never
+        // be smaller than -127 for any reasonable choice of cutoff or ewald coefficients.
+        SP0expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP0cr2_S0);
+        SP1expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP1cr2_S0);
+#        ifndef HALF_LJ
+        SP0expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP0cr2_S2);
+        SP1expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP1cr2_S2);
+#        endif
+
+        /* 1 + cr2 + 1/2*cr2^2 */
+        SP0poly_S0 = fma(fma(half_S, SP0cr2_S0, one_S), SP0cr2_S0, one_S);
+        SP1poly_S0 = fma(fma(half_S, SP1cr2_S0, one_S), SP1cr2_S0, one_S);
+#        ifndef HALF_LJ
+        SP0poly_S2 = fma(fma(half_S, SP0cr2_S2, one_S), SP0cr2_S2, one_S);
+        SP1poly_S2 = fma(fma(half_S, SP1cr2_S2, one_S), SP1cr2_S2, one_S);
+#        endif
+
+        /* We calculate LJ F*r = (6*C6)*(r^-6 - F_mesh/6), we use:
+         * r^-6*cexp*(1 + cr2 + cr2^2/2 + cr2^3/6) = cexp*(r^-6*poly + c^6/6)
+         */
+        SP0frLJ_S0 = fma(SP0c6grid_S0, fnma(SP0expmcr2_S0, fma(SP0rinvsix_nm_S0, SP0poly_S0, lje_c6_6_S), SP0rinvsix_nm_S0), SP0frLJ_S0);
+        SP1frLJ_S0 = fma(SP1c6grid_S0, fnma(SP1expmcr2_S0, fma(SP1rinvsix_nm_S0, SP1poly_S0, lje_c6_6_S), SP1rinvsix_nm_S0), SP1frLJ_S0);
+#        ifndef HALF_LJ
+        SP0frLJ_S2 = fma(SP0c6grid_S2, fnma(SP0expmcr2_S2, fma(SP0rinvsix_nm_S2, SP0poly_S2, lje_c6_6_S), SP0rinvsix_nm_S2), SP0frLJ_S2);
+        SP1frLJ_S2 = fma(SP1c6grid_S2, fnma(SP1expmcr2_S2, fma(SP1rinvsix_nm_S2, SP1poly_S2, lje_c6_6_S), SP1rinvsix_nm_S2), SP1frLJ_S2);
+#        endif
+
+#        ifdef CALC_ENERGIES
+#            ifdef CHECK_EXCLS
+        sh_mask_S0 = selectByMask(lje_vc_S, interact_S0);
+#                ifndef HALF_LJ
+        sh_mask_S2 = selectByMask(lje_vc_S, interact_S2);
+#                endif
+#            else
+        SP0sh_mask_S0 = lje_vc_S;
+        SP1sh_mask_S0 = lje_vc_S;
+#                ifndef HALF_LJ
+        SP0sh_mask_S2 = lje_vc_S;
+        SP1sh_mask_S2 = lje_vc_S;
+#                endif
+#            endif
+
+        SP0VLJ_S0 = fma(sixth_S * SP0c6grid_S0, fma(SP0rinvsix_nm_S0, fnma(SP0expmcr2_S0, SP0poly_S0, one_S), SP0sh_mask_S0), SP0VLJ_S0);
+        SP1VLJ_S0 = fma(sixth_S * SP1c6grid_S0, fma(SP1rinvsix_nm_S0, fnma(SP1expmcr2_S0, SP1poly_S0, one_S), SP1sh_mask_S0), SP1VLJ_S0);
+#            ifndef HALF_LJ
+        SP0VLJ_S2 = fma(sixth_S * SP0c6grid_S2, fma(SP0rinvsix_nm_S2, fnma(SP0expmcr2_S2, SP0poly_S2, one_S), SP0sh_mask_S2), SP0VLJ_S2);
+        SP1VLJ_S2 = fma(sixth_S * SP1c6grid_S2, fma(SP1rinvsix_nm_S2, fnma(SP1expmcr2_S2, SP1poly_S2, one_S), SP1sh_mask_S2), SP1VLJ_S2);
+#            endif
+#        endif /* CALC_ENERGIES */
+    }
+#    endif /* LJ_EWALD_GEOM */
+
+#    if defined VDW_CUTOFF_CHECK
+    /* frLJ is multiplied later by rinvsq, which is masked for the Coulomb
+     * cut-off, but if the VdW cut-off is shorter, we need to mask with that.
+     */
+    SP0frLJ_S0 = selectByMask(SP0frLJ_S0, SP0wco_vdw_S0);
+    SP1frLJ_S0 = selectByMask(SP1frLJ_S0, SP1wco_vdw_S0);
+#        ifndef HALF_LJ
+    SP0frLJ_S2 = selectByMask(SP0frLJ_S2, SP0wco_vdw_S2);
+    SP1frLJ_S2 = selectByMask(SP1frLJ_S2, SP1wco_vdw_S2);
+#        endif
+#    endif
+
+#    ifdef CALC_ENERGIES
+    /* The potential shift should be removed for pairs beyond cut-off */
+    SP0VLJ_S0 = selectByMask(SP0VLJ_S0, SP0wco_vdw_S0);
+    SP1VLJ_S0 = selectByMask(SP1VLJ_S0, SP1wco_vdw_S0);
+#        ifndef HALF_LJ
+    SP0VLJ_S2 = selectByMask(SP0VLJ_S2, SP0wco_vdw_S2);
+    SP1VLJ_S2 = selectByMask(SP1VLJ_S2, SP1wco_vdw_S2);
+#        endif
+#    endif
+
+#endif /* CALC_LJ */
+
+#ifdef CALC_ENERGIES
+#    ifdef ENERGY_GROUPS
+    /* Extract the group pair index per j pair.
+     * Energy groups are stored per i-cluster, so things get
+     * complicated when the i- and j-cluster size don't match.
+     */
+    {
+#        if UNROLLJ == 2
+#error
+        const int egps_j    = nbatParams.energrp[cj >> 1];
+        egp_jj[0]       = ((egps_j >> ((cj & 1) * egps_jshift)) & egps_jmask) * egps_jstride;
+#        else
+        /* We assume UNROLLI <= UNROLLJ */
+        for (int jdi = 0; jdi < UNROLLJ / UNROLLI; jdi++)
+        {
+            const int SP0egps_j = nbatParams.energrp[SP0cj * (UNROLLJ / UNROLLI) + jdi];
+            const int SP1egps_j = nbatParams.energrp[SP1cj * (UNROLLJ / UNROLLI) + jdi];
+            for (int jj = 0; jj < (UNROLLI / 2); jj++)
+            {
+                SP0egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP0egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+                SP1egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP1egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+            }
+        }
+#        endif
+    }
+#    endif
+
+#    ifdef CALC_COULOMB
+#        ifndef ENERGY_GROUPS
+    vctot_S = vctot_S + SP0vcoul_S0 + SP0vcoul_S2;
+    vctot_S = vctot_S + SP1vcoul_S0 + SP1vcoul_S2;
+#        else
+    add_ener_grp_halves(SP0vcoul_S0, vctp[0], vctp[1], SP0egp_jj);
+    add_ener_grp_halves(SP1vcoul_S0, vctp[0], vctp[1], SP1egp_jj);
+    add_ener_grp_halves(SP0vcoul_S2, vctp[2], vctp[3], SP0egp_jj);
+    add_ener_grp_halves(SP1vcoul_S2, vctp[2], vctp[3], SP1egp_jj);
+#        endif
+#    endif
+
+#    ifdef CALC_LJ
+#        ifndef ENERGY_GROUPS
+    Vvdwtot_S = Vvdwtot_S
+                + SP0VLJ_S0
+                + SP1VLJ_S0
+#            ifndef HALF_LJ
+                + SP0VLJ_S2
+                + SP1VLJ_S2
+#            endif
+            ;
+#        else
+    add_ener_grp_halves(SP0VLJ_S0, vvdwtp[0], vvdwtp[1], SP0egp_jj);
+    add_ener_grp_halves(SP1VLJ_S0, vvdwtp[0], vvdwtp[1], SP1egp_jj);
+#            ifndef HALF_LJ
+    add_ener_grp_halves(SP0VLJ_S2, vvdwtp[2], vvdwtp[3], SP0egp_jj);
+    add_ener_grp_halves(SP1VLJ_S2, vvdwtp[2], vvdwtp[3], SP1egp_jj);
+#            endif
+#        endif
+#    endif /* CALC_LJ */
+#endif     /* CALC_ENERGIES */
+
+#ifdef CALC_LJ
+#    ifdef CALC_COULOMB
+    SP0fscal_S0 = SP0rinvsq_S0 * (SP0frcoul_S0 + SP0frLJ_S0);
+    SP1fscal_S0 = SP1rinvsq_S0 * (SP1frcoul_S0 + SP1frLJ_S0);
+#    else
+    SP0fscal_S0 = SP0rinvsq_S0 * SP0frLJ_S0;
+    SP1fscal_S0 = SP1rinvsq_S0 * SP1frLJ_S0;
+#    endif
+#else
+    fscal_S0 = rinvsq_S0 * frcoul_S0;
+#endif /* CALC_LJ */
+#if defined CALC_LJ && !defined HALF_LJ
+#    ifdef CALC_COULOMB
+    SP0fscal_S2 = SP0rinvsq_S2 * (SP0frcoul_S2 + SP0frLJ_S2);
+    SP1fscal_S2 = SP1rinvsq_S2 * (SP1frcoul_S2 + SP1frLJ_S2);
+#    else
+    SP0fscal_S2 = SP0rinvsq_S2 * SP0frLJ_S2;
+    SP1fscal_S2 = SP1rinvsq_S2 * SP1frLJ_S2;
+#    endif
+#else
+    /* Atom 2 and 3 don't have LJ, so only add Coulomb forces */
+    SP0fscal_S2 = SP0rinvsq_S2 * SP0frcoul_S2;
+    SP1fscal_S2 = SP1rinvsq_S2 * SP1frcoul_S2;
+#endif
+
+    /* Calculate temporary vectorial force */
+    SP0tx_S0 = SP0fscal_S0 * SP0dx_S0;
+    SP1tx_S0 = SP1fscal_S0 * SP1dx_S0;
+    SP0tx_S2 = SP0fscal_S2 * SP0dx_S2;
+    SP1tx_S2 = SP1fscal_S2 * SP1dx_S2;
+    SP0ty_S0 = SP0fscal_S0 * SP0dy_S0;
+    SP1ty_S0 = SP1fscal_S0 * SP1dy_S0;
+    SP0ty_S2 = SP0fscal_S2 * SP0dy_S2;
+    SP1ty_S2 = SP1fscal_S2 * SP1dy_S2;
+    SP0tz_S0 = SP0fscal_S0 * SP0dz_S0;
+    SP1tz_S0 = SP1fscal_S0 * SP1dz_S0;
+    SP0tz_S2 = SP0fscal_S2 * SP0dz_S2;
+    SP1tz_S2 = SP1fscal_S2 * SP1dz_S2;
+
+    /* Increment i atom force */
+    fix_S0 = fix_S0 + SP0tx_S0;
+    fix_S0 = fix_S0 + SP1tx_S0;
+    fix_S2 = fix_S2 + SP0tx_S2;
+    fix_S2 = fix_S2 + SP1tx_S2;
+    fiy_S0 = fiy_S0 + SP0ty_S0;
+    fiy_S0 = fiy_S0 + SP1ty_S0;
+    fiy_S2 = fiy_S2 + SP0ty_S2;
+    fiy_S2 = fiy_S2 + SP1ty_S2;
+    fiz_S0 = fiz_S0 + SP0tz_S0;
+    fiz_S0 = fiz_S0 + SP1tz_S0;
+    fiz_S2 = fiz_S2 + SP0tz_S2;
+    fiz_S2 = fiz_S2 + SP1tz_S2;
+
+    /* Decrement j atom force */
+    decr3Hsimd(f + SP0aj * DIM, SP0tx_S0 + SP0tx_S2, SP0ty_S0 + SP0ty_S2, SP0tz_S0 + SP0tz_S2);
+    decr3Hsimd(f + SP1aj * DIM, SP1tx_S0 + SP1tx_S2, SP1ty_S0 + SP1ty_S2, SP1tz_S0 + SP1tz_S2);
+
+#undef SP0rinv_ex_S0
+#undef SP1rinv_ex_S0
+#undef SP0rinv_ex_S2
+#undef SP1rinv_ex_S2
+
+#undef SP0wco_vdw_S0
+#undef SP1wco_vdw_S0
+#undef SP0wco_vdw_S2
+#undef SP1wco_vdw_S2
+
+#undef EXCL_FORCES
+#undef SKIP_INVSQRT
diff -ruN orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_2.h gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_2.h
--- orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_2.h	1970-01-01 09:00:00.000000000 +0900
+++ gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_2.h	2022-03-08 10:54:48.000000000 +0900
@@ -0,0 +1,1430 @@
+/*
+ * This file is part of the GROMACS molecular simulation package.
+ *
+ * Copyright 2012- The GROMACS Authors
+ * and the project initiators Erik Lindahl, Berk Hess and David van der Spoel.
+ * Consult the AUTHORS/COPYING files and https://www.gromacs.org for details.
+ *
+ * GROMACS is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public License
+ * as published by the Free Software Foundation; either version 2.1
+ * of the License, or (at your option) any later version.
+ *
+ * GROMACS is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with GROMACS; if not, see
+ * https://www.gnu.org/licenses, or write to the Free Software Foundation,
+ * Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA.
+ *
+ * If you want to redistribute modifications to GROMACS, please
+ * consider that scientific software is very special. Version
+ * control is crucial - bugs must be traceable. We will be happy to
+ * consider code for inclusion in the official distribution, but
+ * derived work must not be called official GROMACS. Details are found
+ * in the README & COPYING files - if they are missing, get the
+ * official version at https://www.gromacs.org.
+ *
+ * To help us fund GROMACS development, we humbly ask that you cite
+ * the research papers on the package. Check out https://www.gromacs.org.
+ */
+
+/* This is the innermost loop contents for the 4 x N atom simd kernel.
+ * This flavor of the kernel duplicates the data for N j-particles in
+ * 2xN wide simd registers to do operate on 2 i-particles at once.
+ * This leads to 4/2=2 sets of most instructions. Therefore we call
+ * this kernel 2x(N+N) = 2xnn
+ *
+ * This 2xnn kernel is basically the 4xn equivalent with half the registers
+ * and instructions removed.
+ *
+ * An alternative would be to load to different cluster of N j-particles
+ * into simd registers, giving a 4x(N+N) kernel. This doubles the amount
+ * of instructions, which could lead to better scheduling. But we actually
+ * observed worse scheduling for the AVX-256 4x8 normal analytical PME
+ * kernel, which has a lower pair throughput than 2x(4+4) with gcc 4.7.
+ * It could be worth trying this option, but it takes some more effort.
+ * This 2xnn kernel is basically the 4xn equivalent with
+ */
+
+
+/* When calculating RF or Ewald interactions we calculate the electrostatic/LJ
+ * forces on excluded atom pairs here in the non-bonded loops.
+ * But when energies and/or virial is required we calculate them
+ * separately to as then it is easier to separate the energy and virial
+ * contributions.
+ */
+#if defined CHECK_EXCLS && (defined CALC_COULOMB || defined LJ_EWALD_GEOM)
+#    define EXCL_FORCES
+#endif
+#if defined CALC_COULOMB || defined CALC_COUL_TAB || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH \
+        || defined LJ_COULOMB_LB || defined HALF_LJ || defined EXCL_FORCES || defined LJ_COMB_LB
+#    define SKIP_INVSQRT 0
+#else
+#    define SKIP_INVSQRT 1
+#endif
+
+#ifdef ENERGY_GROUPS
+    /* Energy group indices for two atoms packed into one int */
+    int SP0egp_jj[UNROLLJ / 2];
+    int SP1egp_jj[UNROLLJ / 2];
+    int SP2egp_jj[UNROLLJ / 2];
+#endif
+
+#ifdef CHECK_EXCLS
+    /* Interaction (non-exclusion) mask of all 1's or 0's */
+    SimdBool SP0interact_S0;
+    SimdBool SP1interact_S0;
+    SimdBool SP2interact_S0;
+    SimdBool SP0interact_S2;
+    SimdBool SP1interact_S2;
+    SimdBool SP2interact_S2;
+#endif
+
+    SimdReal SP0jx_S, SP0jy_S, SP0jz_S;
+    SimdReal SP1jx_S, SP1jy_S, SP1jz_S;
+    SimdReal SP2jx_S, SP2jy_S, SP2jz_S;
+    SimdReal SP0dx_S0, SP0dy_S0, SP0dz_S0;
+    SimdReal SP1dx_S0, SP1dy_S0, SP1dz_S0;
+    SimdReal SP2dx_S0, SP2dy_S0, SP2dz_S0;
+    SimdReal SP0dx_S2, SP0dy_S2, SP0dz_S2;
+    SimdReal SP1dx_S2, SP1dy_S2, SP1dz_S2;
+    SimdReal SP2dx_S2, SP2dy_S2, SP2dz_S2;
+    SimdReal SP0tx_S0, SP0ty_S0, SP0tz_S0;
+    SimdReal SP1tx_S0, SP1ty_S0, SP1tz_S0;
+    SimdReal SP2tx_S0, SP2ty_S0, SP2tz_S0;
+    SimdReal SP0tx_S2, SP0ty_S2, SP0tz_S2;
+    SimdReal SP1tx_S2, SP1ty_S2, SP1tz_S2;
+    SimdReal SP2tx_S2, SP2ty_S2, SP2tz_S2;
+    SimdReal SP0rsq_S0, SP0rinvsq_S0;
+    SimdReal SP1rsq_S0, SP1rinvsq_S0;
+    SimdReal SP2rsq_S0, SP2rinvsq_S0;
+    SimdReal SP0rsq_S2, SP0rinvsq_S2;
+    SimdReal SP1rsq_S2, SP1rinvsq_S2;
+    SimdReal SP2rsq_S2, SP2rinvsq_S2;
+#if !SKIP_INVSQRT
+    SimdReal SP0rinv_S0, SP0rinv_S2;
+    SimdReal SP1rinv_S0, SP1rinv_S2;
+    SimdReal SP2rinv_S0, SP2rinv_S2;
+#endif
+    /* wco: within cut-off, mask of all 1's or 0's */
+    SimdBool SP0wco_S0;
+    SimdBool SP1wco_S0;
+    SimdBool SP2wco_S0;
+    SimdBool SP0wco_S2;
+    SimdBool SP1wco_S2;
+    SimdBool SP2wco_S2;
+#ifdef VDW_CUTOFF_CHECK
+    SimdBool SP0wco_vdw_S0;
+    SimdBool SP1wco_vdw_S0;
+    SimdBool SP2wco_vdw_S0;
+#    ifndef HALF_LJ
+    SimdBool SP0wco_vdw_S2;
+    SimdBool SP1wco_vdw_S2;
+    SimdBool SP2wco_vdw_S2;
+#    endif
+#endif
+
+#if (defined CALC_COULOMB && defined CALC_COUL_TAB) || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    SimdReal                                                        SP0r_S0;
+    SimdReal                                                        SP1r_S0;
+    SimdReal                                                        SP2r_S0;
+#    if (defined CALC_COULOMB && defined CALC_COUL_TAB) || !defined HALF_LJ
+    SimdReal                                                        SP0r_S2;
+    SimdReal                                                        SP1r_S2;
+    SimdReal                                                        SP2r_S2;
+#    endif
+#endif
+
+#if defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    SimdReal SP0rsw_S0, SP0rsw2_S0;
+    SimdReal SP1rsw_S0, SP1rsw2_S0;
+    SimdReal SP2rsw_S0, SP2rsw2_S0;
+#    ifndef HALF_LJ
+    SimdReal SP0rsw_S2, SP0rsw2_S2;
+    SimdReal SP1rsw_S2, SP1rsw2_S2;
+    SimdReal SP2rsw_S2, SP2rsw2_S2;
+#    endif
+#endif
+
+#ifdef CALC_COULOMB
+#    ifdef CHECK_EXCLS
+    /* 1/r masked with the interaction mask */
+    SimdReal SP0rinv_ex_S0;
+    SimdReal SP1rinv_ex_S0;
+    SimdReal SP2rinv_ex_S0;
+    SimdReal SP0rinv_ex_S2;
+    SimdReal SP1rinv_ex_S2;
+    SimdReal SP2rinv_ex_S2;
+#    endif
+    SimdReal SP0jq_S;
+    SimdReal SP1jq_S;
+    SimdReal SP2jq_S;
+    SimdReal SP0qq_S0;
+    SimdReal SP1qq_S0;
+    SimdReal SP2qq_S0;
+    SimdReal SP0qq_S2;
+    SimdReal SP1qq_S2;
+    SimdReal SP2qq_S2;
+#    ifdef CALC_COUL_TAB
+    /* The force (PME mesh force) we need to subtract from 1/r^2 */
+    SimdReal SP0fsub_S0;
+    SimdReal SP1fsub_S0;
+    SimdReal SP2fsub_S0;
+    SimdReal SP0fsub_S2;
+    SimdReal SP1fsub_S2;
+    SimdReal SP2fsub_S2;
+#    endif
+#    ifdef CALC_COUL_EWALD
+    SimdReal SP0brsq_S0, SP0brsq_S2;
+    SimdReal SP1brsq_S0, SP1brsq_S2;
+    SimdReal SP2brsq_S0, SP2brsq_S2;
+    SimdReal SP0ewcorr_S0, SP0ewcorr_S2;
+    SimdReal SP1ewcorr_S0, SP1ewcorr_S2;
+    SimdReal SP2ewcorr_S0, SP2ewcorr_S2;
+#    endif
+
+    /* frcoul = (1/r - fsub)*r */
+    SimdReal SP0frcoul_S0;
+    SimdReal SP1frcoul_S0;
+    SimdReal SP2frcoul_S0;
+    SimdReal SP0frcoul_S2;
+    SimdReal SP1frcoul_S2;
+    SimdReal SP2frcoul_S2;
+#    ifdef CALC_COUL_TAB
+    /* For tables: r, rs=r/sp, rf=floor(rs), frac=rs-rf */
+    SimdReal SP0rs_S0, SP0rf_S0, SP0frac_S0;
+    SimdReal SP1rs_S0, SP1rf_S0, SP1frac_S0;
+    SimdReal SP2rs_S0, SP2rf_S0, SP2frac_S0;
+    SimdReal SP0rs_S2, SP0rf_S2, SP0frac_S2;
+    SimdReal SP1rs_S2, SP1rf_S2, SP1frac_S2;
+    SimdReal SP2rs_S2, SP2rf_S2, SP2frac_S2;
+    /* Table index: rs truncated to an int */
+    SimdInt32 SP0ti_S0, SP0ti_S2;
+    SimdInt32 SP1ti_S0, SP1ti_S2;
+    SimdInt32 SP2ti_S0, SP2ti_S2;
+    /* Linear force table values */
+    SimdReal SP0ctab0_S0, SP0ctab1_S0;
+    SimdReal SP1ctab0_S0, SP1ctab1_S0;
+    SimdReal SP2ctab0_S0, SP2ctab1_S0;
+    SimdReal SP0ctab0_S2, SP0ctab1_S2;
+    SimdReal SP1ctab0_S2, SP1ctab1_S2;
+    SimdReal SP2ctab0_S2, SP2ctab1_S2;
+#        ifdef CALC_ENERGIES
+    /* Quadratic energy table value */
+    SimdReal SP0ctabv_S0, SP0dum_S0;
+    SimdReal SP1ctabv_S0, SP1dum_S0;
+    SimdReal SP2ctabv_S0, SP2dum_S0;
+    SimdReal SP0ctabv_S2, SP0dum_S2;
+    SimdReal SP1ctabv_S2, SP1dum_S2;
+    SimdReal SP2ctabv_S2, SP2dum_S2;
+#        endif
+#    endif
+#    if defined CALC_ENERGIES && (defined CALC_COUL_EWALD || defined CALC_COUL_TAB)
+    /* The potential (PME mesh) we need to subtract from 1/r */
+    SimdReal SP0vc_sub_S0;
+    SimdReal SP1vc_sub_S0;
+    SimdReal SP2vc_sub_S0;
+    SimdReal SP0vc_sub_S2;
+    SimdReal SP1vc_sub_S2;
+    SimdReal SP2vc_sub_S2;
+#    endif
+#    ifdef CALC_ENERGIES
+    /* Electrostatic potential */
+    SimdReal SP0vcoul_S0;
+    SimdReal SP1vcoul_S0;
+    SimdReal SP2vcoul_S0;
+    SimdReal SP0vcoul_S2;
+    SimdReal SP1vcoul_S2;
+    SimdReal SP2vcoul_S2;
+#    endif
+#endif
+    /* The force times 1/r */
+    SimdReal SP0fscal_S0;
+    SimdReal SP1fscal_S0;
+    SimdReal SP2fscal_S0;
+    SimdReal SP0fscal_S2;
+    SimdReal SP1fscal_S2;
+    SimdReal SP2fscal_S2;
+
+#ifdef CALC_LJ
+#    ifdef LJ_COMB_LB
+    /* LJ sigma_j/2 and sqrt(epsilon_j) */
+    SimdReal SP0hsig_j_S, SP0seps_j_S;
+    SimdReal SP1hsig_j_S, SP1seps_j_S;
+    SimdReal SP2hsig_j_S, SP2seps_j_S;
+    /* LJ sigma_ij and epsilon_ij */
+    SimdReal SP0sig_S0, SP0eps_S0;
+    SimdReal SP1sig_S0, SP1eps_S0;
+    SimdReal SP2sig_S0, SP2eps_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0sig_S2, SP0eps_S2;
+    SimdReal SP1sig_S2, SP1eps_S2;
+    SimdReal SP2sig_S2, SP2eps_S2;
+#        endif
+#        ifdef CALC_ENERGIES
+    SimdReal SP0sig2_S0, SP0sig6_S0;
+    SimdReal SP1sig2_S0, SP1sig6_S0;
+    SimdReal SP2sig2_S0, SP2sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0sig2_S2, SP0sig6_S2;
+    SimdReal SP1sig2_S2, SP1sig6_S2;
+    SimdReal SP2sig2_S2, SP2sig6_S2;
+#            endif
+#        endif /* LJ_COMB_LB */
+#    endif     /* CALC_LJ */
+
+#    ifdef LJ_COMB_GEOM
+    SimdReal SP0c6s_j_S, SP0c12s_j_S;
+    SimdReal SP1c6s_j_S, SP1c12s_j_S;
+    SimdReal SP2c6s_j_S, SP2c12s_j_S;
+#    endif
+
+    /* Intermediate variables for LJ calculation */
+#    ifndef LJ_COMB_LB
+    SimdReal SP0rinvsix_S0;
+    SimdReal SP1rinvsix_S0;
+    SimdReal SP2rinvsix_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0rinvsix_S2;
+    SimdReal SP1rinvsix_S2;
+    SimdReal SP2rinvsix_S2;
+#        endif
+#    endif
+#    ifdef LJ_COMB_LB
+    SimdReal SP0sir_S0, SP0sir2_S0, SP0sir6_S0;
+    SimdReal SP1sir_S0, SP1sir2_S0, SP1sir6_S0;
+    SimdReal SP2sir_S0, SP2sir2_S0, SP2sir6_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0sir_S2, SP0sir2_S2, SP0sir6_S2;
+    SimdReal SP1sir_S2, SP1sir2_S2, SP1sir6_S2;
+    SimdReal SP2sir_S2, SP2sir2_S2, SP2sir6_S2;
+#        endif
+#    endif
+
+    SimdReal SP0FrLJ6_S0, SP0FrLJ12_S0, SP0frLJ_S0;
+    SimdReal SP1FrLJ6_S0, SP1FrLJ12_S0, SP1frLJ_S0;
+    SimdReal SP2FrLJ6_S0, SP2FrLJ12_S0, SP2frLJ_S0;
+#    ifndef HALF_LJ
+    SimdReal SP0FrLJ6_S2, SP0FrLJ12_S2, SP0frLJ_S2;
+    SimdReal SP1FrLJ6_S2, SP1FrLJ12_S2, SP1frLJ_S2;
+    SimdReal SP2FrLJ6_S2, SP2FrLJ12_S2, SP2frLJ_S2;
+#    endif
+#endif /* CALC_LJ */
+
+    /* j-cluster index */
+    const int SP0cj = l_cj[cjind+0].cj;
+    const int SP1cj = l_cj[cjind+1].cj;
+    const int SP2cj = l_cj[cjind+2].cj;
+
+    /* Atom indices (of the first atom in the cluster) */
+    const int SP0aj = SP0cj * UNROLLJ;
+    const int SP1aj = SP1cj * UNROLLJ;
+    const int SP2aj = SP2cj * UNROLLJ;
+#if defined CALC_LJ && (defined LJ_COMB_GEOM || defined LJ_COMB_LB || defined LJ_EWALD_GEOM)
+    /* Index for loading LJ parameters, complicated when interleaving */
+    const int SP0aj2 = SP0aj * 2;
+    const int SP1aj2 = SP1aj * 2;
+    const int SP2aj2 = SP2aj * 2;
+#endif
+
+#ifdef CHECK_EXCLS
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+0].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+1].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+2].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+#endif /* CHECK_EXCLS */
+
+    /* load j atom coordinates */
+    loadDuplicate3Hsimd<STRIDE>(x + SP0aj * DIM, &SP0jx_S, &SP0jy_S, &SP0jz_S);
+    loadDuplicate3Hsimd<STRIDE>(x + SP1aj * DIM, &SP1jx_S, &SP1jy_S, &SP1jz_S);
+    loadDuplicate3Hsimd<STRIDE>(x + SP2aj * DIM, &SP2jx_S, &SP2jy_S, &SP2jz_S);
+
+    /* Calculate distance */
+    SP0dx_S0 = ix_S0 - SP0jx_S;
+    SP1dx_S0 = ix_S0 - SP1jx_S;
+    SP2dx_S0 = ix_S0 - SP2jx_S;
+    SP0dy_S0 = iy_S0 - SP0jy_S;
+    SP1dy_S0 = iy_S0 - SP1jy_S;
+    SP2dy_S0 = iy_S0 - SP2jy_S;
+    SP0dz_S0 = iz_S0 - SP0jz_S;
+    SP1dz_S0 = iz_S0 - SP1jz_S;
+    SP2dz_S0 = iz_S0 - SP2jz_S;
+    SP0dx_S2 = ix_S2 - SP0jx_S;
+    SP1dx_S2 = ix_S2 - SP1jx_S;
+    SP2dx_S2 = ix_S2 - SP2jx_S;
+    SP0dy_S2 = iy_S2 - SP0jy_S;
+    SP1dy_S2 = iy_S2 - SP1jy_S;
+    SP2dy_S2 = iy_S2 - SP2jy_S;
+    SP0dz_S2 = iz_S2 - SP0jz_S;
+    SP1dz_S2 = iz_S2 - SP1jz_S;
+    SP2dz_S2 = iz_S2 - SP2jz_S;
+
+    /* rsq = dx*dx+dy*dy+dz*dz */
+    SP0rsq_S0 = norm2(SP0dx_S0, SP0dy_S0, SP0dz_S0);
+    SP1rsq_S0 = norm2(SP1dx_S0, SP1dy_S0, SP1dz_S0);
+    SP2rsq_S0 = norm2(SP2dx_S0, SP2dy_S0, SP2dz_S0);
+    SP0rsq_S2 = norm2(SP0dx_S2, SP0dy_S2, SP0dz_S2);
+    SP1rsq_S2 = norm2(SP1dx_S2, SP1dy_S2, SP1dz_S2);
+    SP2rsq_S2 = norm2(SP2dx_S2, SP2dy_S2, SP2dz_S2);
+
+    /* Do the cut-off check */
+    SP0wco_S0 = (SP0rsq_S0 < rc2_S);
+    SP1wco_S0 = (SP1rsq_S0 < rc2_S);
+    SP2wco_S0 = (SP2rsq_S0 < rc2_S);
+    SP0wco_S2 = (SP0rsq_S2 < rc2_S);
+    SP1wco_S2 = (SP1rsq_S2 < rc2_S);
+    SP2wco_S2 = (SP2rsq_S2 < rc2_S);
+
+#ifdef CHECK_EXCLS
+#    ifdef EXCL_FORCES
+    /* Only remove the (sub-)diagonal to avoid double counting */
+#        if UNROLLJ == UNROLLI
+    if (cj == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask_S0;
+        wco_S2 = wco_S2 && diagonal_mask_S2;
+    }
+#        else
+#            if UNROLLJ == 2 * UNROLLI
+    if (cj * 2 == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask0_S0;
+        wco_S2 = wco_S2 && diagonal_mask0_S2;
+    }
+    else if (cj * 2 + 1 == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask1_S0;
+        wco_S2 = wco_S2 && diagonal_mask1_S2;
+    }
+#            else
+#                error "only UNROLLJ == UNROLLI*(1 or 2) currently supported in 2xnn kernels"
+#            endif
+#        endif
+#    else /* EXCL_FORCES */
+    /* No exclusion forces: remove all excluded atom pairs from the list */
+    wco_S0 = wco_S0 && interact_S0;
+    wco_S2 = wco_S2 && interact_S2;
+#    endif
+#endif
+
+#ifdef COUNT_PAIRS
+    {
+        int                              i, j;
+        alignas(GMX_SIMD_ALIGNMENT) real tmp[GMX_SIMD_REAL_WIDTH];
+
+        for (i = 0; i < UNROLLI; i += 2)
+        {
+            store(tmp, rc2_S - (i == 0 ? rsq_S0 : rsq_S2));
+            for (j = 0; j < 2 * UNROLLJ; j++)
+            {
+                if (tmp[j] >= 0)
+                {
+                    npair++;
+                }
+            }
+        }
+    }
+#endif
+
+    // Ensure the distances do not fall below the limit where r^-12 overflows.
+    // This should never happen for normal interactions.
+    SP0rsq_S0 = max(SP0rsq_S0, minRsq_S);
+    SP1rsq_S0 = max(SP1rsq_S0, minRsq_S);
+    SP2rsq_S0 = max(SP2rsq_S0, minRsq_S);
+    SP0rsq_S2 = max(SP0rsq_S2, minRsq_S);
+    SP1rsq_S2 = max(SP1rsq_S2, minRsq_S);
+    SP2rsq_S2 = max(SP2rsq_S2, minRsq_S);
+
+    /* Calculate 1/r */
+#if SKIP_INVSQRT
+    SP0rinvsq_S0 = invMask(SP0rsq_S0, SP0wco_S0);
+    SP1rinvsq_S0 = invMask(SP1rsq_S0, SP1wco_S0);
+    SP2rinvsq_S0 = invMask(SP2rsq_S0, SP2wco_S0);
+    SP0rinvsq_S2 = invMask(SP0rsq_S2, SP0wco_S2);
+    SP1rinvsq_S2 = invMask(SP1rsq_S2, SP1wco_S2);
+    SP2rinvsq_S2 = invMask(SP2rsq_S2, SP2wco_S2);
+#else
+    /* and set rinv to zero for r beyond the cut-off */
+    SP0rinv_S0 = invsqrtMask(SP0rsq_S0, SP0wco_S0);
+    SP1rinv_S0 = invsqrtMask(SP1rsq_S0, SP1wco_S0);
+    SP2rinv_S0 = invsqrtMask(SP2rsq_S0, SP2wco_S0);
+    SP0rinv_S2 = invsqrtMask(SP0rsq_S2, SP0wco_S2);
+    SP1rinv_S2 = invsqrtMask(SP1rsq_S2, SP1wco_S2);
+    SP2rinv_S2 = invsqrtMask(SP2rsq_S2, SP2wco_S2);
+#endif
+
+#ifdef CALC_COULOMB
+    /* Load parameters for j atom */
+    SP0jq_S = loadDuplicateHsimd(q + SP0aj);
+    SP1jq_S = loadDuplicateHsimd(q + SP1aj);
+    SP2jq_S = loadDuplicateHsimd(q + SP2aj);
+    SP0qq_S0 = iq_S0 * SP0jq_S;
+    SP1qq_S0 = iq_S0 * SP1jq_S;
+    SP2qq_S0 = iq_S0 * SP2jq_S;
+    SP0qq_S2 = iq_S2 * SP0jq_S;
+    SP1qq_S2 = iq_S2 * SP1jq_S;
+    SP2qq_S2 = iq_S2 * SP2jq_S;
+#endif
+
+#ifdef CALC_LJ
+#    if !defined LJ_COMB_GEOM && !defined LJ_COMB_LB && !defined FIX_LJ_C
+    SimdReal                                                     SP0c6_S0, SP0c12_S0;
+    SimdReal                                                     SP1c6_S0, SP1c12_S0;
+    SimdReal                                                     SP2c6_S0, SP2c12_S0;
+#        ifdef HALF_LJ
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP0aj, &SP0c6_S0, &SP0c12_S0);
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP1aj, &SP1c6_S0, &SP1c12_S0);
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP2aj, &SP2c6_S0, &SP2c12_S0);
+#        else
+    SimdReal SP0c6_S2, SP0c12_S2;
+    SimdReal SP1c6_S2, SP1c12_S2;
+    SimdReal SP2c6_S2, SP2c12_S2;
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP0aj, &SP0c6_S0, &SP0c12_S0, &SP0c6_S2, &SP0c12_S2);
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP1aj, &SP1c6_S0, &SP1c12_S0, &SP1c6_S2, &SP1c12_S2);
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP2aj, &SP2c6_S0, &SP2c12_S0, &SP2c6_S2, &SP2c12_S2);
+#        endif
+#    endif /* not defined any LJ rule */
+
+#    ifdef LJ_COMB_GEOM
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP0aj2, &SP0c6s_j_S, &SP0c12s_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP1aj2, &SP1c6s_j_S, &SP1c12s_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP2aj2, &SP2c6s_j_S, &SP2c12s_j_S);
+    SimdReal SP0c6_S0 = c6s_S0 * SP0c6s_j_S;
+    SimdReal SP1c6_S0 = c6s_S0 * SP1c6s_j_S;
+    SimdReal SP2c6_S0 = c6s_S0 * SP2c6s_j_S;
+#        ifndef HALF_LJ
+    SimdReal SP0c6_S2 = c6s_S2 * SP0c6s_j_S;
+    SimdReal SP1c6_S2 = c6s_S2 * SP1c6s_j_S;
+    SimdReal SP2c6_S2 = c6s_S2 * SP2c6s_j_S;
+#        endif
+    SimdReal SP0c12_S0 = c12s_S0 * SP0c12s_j_S;
+    SimdReal SP1c12_S0 = c12s_S0 * SP1c12s_j_S;
+    SimdReal SP2c12_S0 = c12s_S0 * SP2c12s_j_S;
+#        ifndef HALF_LJ
+    SimdReal SP0c12_S2 = c12s_S2 * SP0c12s_j_S;
+    SimdReal SP1c12_S2 = c12s_S2 * SP1c12s_j_S;
+    SimdReal SP2c12_S2 = c12s_S2 * SP2c12s_j_S;
+#        endif
+#    endif /* LJ_COMB_GEOM */
+
+#    ifdef LJ_COMB_LB
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP0aj2, &SP0hsig_j_S, &SP0seps_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP1aj2, &SP1hsig_j_S, &SP1seps_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP2aj2, &SP2hsig_j_S, &SP2seps_j_S);
+
+    SP0sig_S0 = hsig_i_S0 + SP0hsig_j_S;
+    SP1sig_S0 = hsig_i_S0 + SP1hsig_j_S;
+    SP2sig_S0 = hsig_i_S0 + SP2hsig_j_S;
+    SP0eps_S0 = seps_i_S0 * SP0seps_j_S;
+    SP1eps_S0 = seps_i_S0 * SP1seps_j_S;
+    SP2eps_S0 = seps_i_S0 * SP2seps_j_S;
+#        ifndef HALF_LJ
+    SP0sig_S2 = hsig_i_S2 + SP0hsig_j_S;
+    SP1sig_S2 = hsig_i_S2 + SP1hsig_j_S;
+    SP2sig_S2 = hsig_i_S2 + SP2hsig_j_S;
+    SP0eps_S2 = seps_i_S2 * SP0seps_j_S;
+    SP1eps_S2 = seps_i_S2 * SP1seps_j_S;
+    SP2eps_S2 = seps_i_S2 * SP2seps_j_S;
+#        endif
+#    endif /* LJ_COMB_LB */
+
+#endif /* CALC_LJ */
+
+#if !SKIP_INVSQRT
+    SP0rinvsq_S0 = SP0rinv_S0 * SP0rinv_S0;
+    SP1rinvsq_S0 = SP1rinv_S0 * SP1rinv_S0;
+    SP2rinvsq_S0 = SP2rinv_S0 * SP2rinv_S0;
+    SP0rinvsq_S2 = SP0rinv_S2 * SP0rinv_S2;
+    SP1rinvsq_S2 = SP1rinv_S2 * SP1rinv_S2;
+    SP2rinvsq_S2 = SP2rinv_S2 * SP2rinv_S2;
+#endif
+
+#ifdef CALC_COULOMB
+    /* Note that here we calculate force*r, not the usual force/r.
+     * This allows avoiding masking the reaction-field contribution,
+     * as frcoul is later multiplied by rinvsq which has been
+     * masked with the cut-off check.
+     */
+
+#    ifdef EXCL_FORCES
+    /* Only add 1/r for non-excluded atom pairs */
+    rinv_ex_S0 = selectByMask(rinv_S0, interact_S0);
+    rinv_ex_S2 = selectByMask(rinv_S2, interact_S2);
+#    else
+    /* No exclusion forces, we always need 1/r */
+#        define SP0rinv_ex_S0 SP0rinv_S0
+#        define SP1rinv_ex_S0 SP1rinv_S0
+#        define SP2rinv_ex_S0 SP2rinv_S0
+#        define SP0rinv_ex_S2 SP0rinv_S2
+#        define SP1rinv_ex_S2 SP1rinv_S2
+#        define SP2rinv_ex_S2 SP2rinv_S2
+#    endif
+
+#    ifdef CALC_COUL_RF
+    /* Electrostatic interactions */
+    SP0frcoul_S0 = SP0qq_S0 * fma(SP0rsq_S0, mrc_3_S, SP0rinv_ex_S0);
+    SP1frcoul_S0 = SP1qq_S0 * fma(SP1rsq_S0, mrc_3_S, SP1rinv_ex_S0);
+    SP2frcoul_S0 = SP2qq_S0 * fma(SP2rsq_S0, mrc_3_S, SP2rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fma(SP0rsq_S2, mrc_3_S, SP0rinv_ex_S2);
+    SP1frcoul_S2 = SP1qq_S2 * fma(SP1rsq_S2, mrc_3_S, SP1rinv_ex_S2);
+    SP2frcoul_S2 = SP2qq_S2 * fma(SP2rsq_S2, mrc_3_S, SP2rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    /* and (merge) mask energy for cut-off and diagonal */
+    SP0vcoul_S0 = maskzMul(SP0qq_S0, SP0rinv_ex_S0 + fma(SP0rsq_S0, hrc_3_S, moh_rc_S), SP0wco_S0);
+    SP1vcoul_S0 = maskzMul(SP1qq_S0, SP1rinv_ex_S0 + fma(SP1rsq_S0, hrc_3_S, moh_rc_S), SP1wco_S0);
+    SP2vcoul_S0 = maskzMul(SP2qq_S0, SP2rinv_ex_S0 + fma(SP2rsq_S0, hrc_3_S, moh_rc_S), SP2wco_S0);
+    SP0vcoul_S2 = maskzMul(SP0qq_S2, SP0rinv_ex_S2 + fma(SP0rsq_S2, hrc_3_S, moh_rc_S), SP0wco_S2);
+    SP1vcoul_S2 = maskzMul(SP1qq_S2, SP1rinv_ex_S2 + fma(SP1rsq_S2, hrc_3_S, moh_rc_S), SP1wco_S2);
+    SP2vcoul_S2 = maskzMul(SP2qq_S2, SP2rinv_ex_S2 + fma(SP2rsq_S2, hrc_3_S, moh_rc_S), SP2wco_S2);
+#        endif
+#    endif
+
+#    ifdef CALC_COUL_EWALD
+    /* We need to mask (or limit) rsq for the cut-off,
+     * as large distances can cause an overflow in gmx_pmecorrF/V.
+     */
+    SP0brsq_S0   = maskzMul(beta2_S, SP0rsq_S0, SP0wco_S0);
+    SP1brsq_S0   = maskzMul(beta2_S, SP1rsq_S0, SP1wco_S0);
+    SP2brsq_S0   = maskzMul(beta2_S, SP2rsq_S0, SP2wco_S0);
+    SP0brsq_S2   = maskzMul(beta2_S, SP0rsq_S2, SP0wco_S2);
+    SP1brsq_S2   = maskzMul(beta2_S, SP1rsq_S2, SP1wco_S2);
+    SP2brsq_S2   = maskzMul(beta2_S, SP2rsq_S2, SP2wco_S2);
+    SP0ewcorr_S0 = beta_S * pmeForceCorrection(SP0brsq_S0);
+    SP1ewcorr_S0 = beta_S * pmeForceCorrection(SP1brsq_S0);
+    SP2ewcorr_S0 = beta_S * pmeForceCorrection(SP2brsq_S0);
+    SP0ewcorr_S2 = beta_S * pmeForceCorrection(SP0brsq_S2);
+    SP1ewcorr_S2 = beta_S * pmeForceCorrection(SP1brsq_S2);
+    SP2ewcorr_S2 = beta_S * pmeForceCorrection(SP2brsq_S2);
+    SP0frcoul_S0 = SP0qq_S0 * fma(SP0ewcorr_S0, SP0brsq_S0, SP0rinv_ex_S0);
+    SP1frcoul_S0 = SP1qq_S0 * fma(SP1ewcorr_S0, SP1brsq_S0, SP1rinv_ex_S0);
+    SP2frcoul_S0 = SP2qq_S0 * fma(SP2ewcorr_S0, SP2brsq_S0, SP2rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fma(SP0ewcorr_S2, SP0brsq_S2, SP0rinv_ex_S2);
+    SP1frcoul_S2 = SP1qq_S2 * fma(SP1ewcorr_S2, SP1brsq_S2, SP1rinv_ex_S2);
+    SP2frcoul_S2 = SP2qq_S2 * fma(SP2ewcorr_S2, SP2brsq_S2, SP2rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    SP0vc_sub_S0 = beta_S * pmePotentialCorrection(SP0brsq_S0);
+    SP1vc_sub_S0 = beta_S * pmePotentialCorrection(SP1brsq_S0);
+    SP2vc_sub_S0 = beta_S * pmePotentialCorrection(SP2brsq_S0);
+    SP0vc_sub_S2 = beta_S * pmePotentialCorrection(SP0brsq_S2);
+    SP1vc_sub_S2 = beta_S * pmePotentialCorrection(SP1brsq_S2);
+    SP2vc_sub_S2 = beta_S * pmePotentialCorrection(SP2brsq_S2);
+#        endif
+
+#    endif /* CALC_COUL_EWALD */
+
+#    ifdef CALC_COUL_TAB
+    /* Electrostatic interactions */
+    SP0r_S0 = SP0rsq_S0 * SP0rinv_S0;
+    SP1r_S0 = SP1rsq_S0 * SP1rinv_S0;
+    SP2r_S0 = SP2rsq_S0 * SP2rinv_S0;
+    SP0r_S2 = SP0rsq_S2 * SP0rinv_S2;
+    SP1r_S2 = SP1rsq_S2 * SP1rinv_S2;
+    SP2r_S2 = SP2rsq_S2 * SP2rinv_S2;
+    /* Convert r to scaled table units */
+    SP0rs_S0 = SP0r_S0 * invtsp_S;
+    SP1rs_S0 = SP1r_S0 * invtsp_S;
+    SP2rs_S0 = SP2r_S0 * invtsp_S;
+    SP0rs_S2 = SP0r_S2 * invtsp_S;
+    SP1rs_S2 = SP1r_S2 * invtsp_S;
+    SP2rs_S2 = SP2r_S2 * invtsp_S;
+    /* Truncate scaled r to an int */
+    SP0ti_S0 = cvttR2I(SP0rs_S0);
+    SP1ti_S0 = cvttR2I(SP1rs_S0);
+    SP2ti_S0 = cvttR2I(SP2rs_S0);
+    SP0ti_S2 = cvttR2I(SP0rs_S2);
+    SP1ti_S2 = cvttR2I(SP1rs_S2);
+    SP2ti_S2 = cvttR2I(SP2rs_S2);
+
+    SP0rf_S0 = trunc(SP0rs_S0);
+    SP1rf_S0 = trunc(SP1rs_S0);
+    SP2rf_S0 = trunc(SP2rs_S0);
+    SP0rf_S2 = trunc(SP0rs_S2);
+    SP1rf_S2 = trunc(SP1rs_S2);
+    SP2rf_S2 = trunc(SP2rs_S2);
+
+    SP0frac_S0 = SP0rs_S0 - SP0rf_S0;
+    SP1frac_S0 = SP1rs_S0 - SP1rf_S0;
+    SP2frac_S0 = SP2rs_S0 - SP2rf_S0;
+    SP0frac_S2 = SP0rs_S2 - SP0rf_S2;
+    SP1frac_S2 = SP1rs_S2 - SP1rf_S2;
+    SP2frac_S2 = SP2rs_S2 - SP2rf_S2;
+
+    /* Load and interpolate table forces and possibly energies.
+     * Force and energy can be combined in one table, stride 4: FDV0
+     * or in two separate tables with stride 1: F and V
+     * Currently single precision uses FDV0, double F and V.
+     */
+#        ifndef CALC_ENERGIES
+#            ifdef TAB_FDV0
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S0, &ctab0_S0, &ctab1_S0);
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S2, &ctab0_S2, &ctab1_S2);
+#            else
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S0, &SP0ctab0_S0, &SP0ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S0, &SP1ctab0_S0, &SP1ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP2ti_S0, &SP2ctab0_S0, &SP2ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S2, &SP0ctab0_S2, &SP0ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S2, &SP1ctab0_S2, &SP1ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP2ti_S2, &SP2ctab0_S2, &SP2ctab1_S2);
+    SP0ctab1_S0   = SP0ctab1_S0 - SP0ctab0_S0;
+    SP1ctab1_S0   = SP1ctab1_S0 - SP1ctab0_S0;
+    SP2ctab1_S0   = SP2ctab1_S0 - SP2ctab0_S0;
+    SP0ctab1_S2   = SP0ctab1_S2 - SP0ctab0_S2;
+    SP1ctab1_S2   = SP1ctab1_S2 - SP1ctab0_S2;
+    SP2ctab1_S2   = SP2ctab1_S2 - SP2ctab0_S2;
+#            endif
+#        else
+#            ifdef TAB_FDV0
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S0, &ctab0_S0, &ctab1_S0, &ctabv_S0, &dum_S0);
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S2, &ctab0_S2, &ctab1_S2, &ctabv_S2, &dum_S2);
+#            else
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S0, &SP0ctab0_S0, &SP0ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S0, &SP1ctab0_S0, &SP1ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP2ti_S0, &SP2ctab0_S0, &SP2ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP0ti_S0, &SP0ctabv_S0, &SP0dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP1ti_S0, &SP1ctabv_S0, &SP1dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP2ti_S0, &SP2ctabv_S0, &SP2dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S2, &SP0ctab0_S2, &SP0ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S2, &SP1ctab0_S2, &SP1ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP2ti_S2, &SP2ctab0_S2, &SP2ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP0ti_S2, &SP0ctabv_S2, &SP0dum_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP1ti_S2, &SP1ctabv_S2, &SP1dum_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP2ti_S2, &SP2ctabv_S2, &SP2dum_S2);
+    SP0ctab1_S0 = SP0ctab1_S0 - SP0ctab0_S0;
+    SP1ctab1_S0 = SP1ctab1_S0 - SP1ctab0_S0;
+    SP2ctab1_S0 = SP2ctab1_S0 - SP2ctab0_S0;
+    SP0ctab1_S2 = SP0ctab1_S2 - SP0ctab0_S2;
+    SP1ctab1_S2 = SP1ctab1_S2 - SP1ctab0_S2;
+    SP2ctab1_S2 = SP2ctab1_S2 - SP2ctab0_S2;
+#            endif
+#        endif
+    SP0fsub_S0   = fma(SP0frac_S0, SP0ctab1_S0, SP0ctab0_S0);
+    SP1fsub_S0   = fma(SP1frac_S0, SP1ctab1_S0, SP1ctab0_S0);
+    SP2fsub_S0   = fma(SP2frac_S0, SP2ctab1_S0, SP2ctab0_S0);
+    SP0fsub_S2   = fma(SP0frac_S2, SP0ctab1_S2, SP0ctab0_S2);
+    SP1fsub_S2   = fma(SP1frac_S2, SP1ctab1_S2, SP1ctab0_S2);
+    SP2fsub_S2   = fma(SP2frac_S2, SP2ctab1_S2, SP2ctab0_S2);
+    SP0frcoul_S0 = SP0qq_S0 * fnma(SP0fsub_S0, SP0r_S0, SP0rinv_ex_S0);
+    SP1frcoul_S0 = SP1qq_S0 * fnma(SP1fsub_S0, SP1r_S0, SP1rinv_ex_S0);
+    SP2frcoul_S0 = SP2qq_S0 * fnma(SP2fsub_S0, SP2r_S0, SP2rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fnma(SP0fsub_S2, SP0r_S2, SP0rinv_ex_S2);
+    SP1frcoul_S2 = SP1qq_S2 * fnma(SP1fsub_S2, SP1r_S2, SP1rinv_ex_S2);
+    SP2frcoul_S2 = SP2qq_S2 * fnma(SP2fsub_S2, SP2r_S2, SP2rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    SP0vc_sub_S0 = fma((mhalfsp_S * SP0frac_S0), (SP0ctab0_S0 + SP0fsub_S0), SP0ctabv_S0);
+    SP1vc_sub_S0 = fma((mhalfsp_S * SP1frac_S0), (SP1ctab0_S0 + SP1fsub_S0), SP1ctabv_S0);
+    SP2vc_sub_S0 = fma((mhalfsp_S * SP2frac_S0), (SP2ctab0_S0 + SP2fsub_S0), SP2ctabv_S0);
+    SP0vc_sub_S2 = fma((mhalfsp_S * SP0frac_S2), (SP0ctab0_S2 + SP0fsub_S2), SP0ctabv_S2);
+    SP1vc_sub_S2 = fma((mhalfsp_S * SP1frac_S2), (SP1ctab0_S2 + SP1fsub_S2), SP1ctabv_S2);
+    SP2vc_sub_S2 = fma((mhalfsp_S * SP2frac_S2), (SP2ctab0_S2 + SP2fsub_S2), SP2ctabv_S2);
+#        endif
+#    endif /* CALC_COUL_TAB */
+
+#    if defined CALC_ENERGIES && (defined CALC_COUL_EWALD || defined CALC_COUL_TAB)
+#        ifndef NO_SHIFT_EWALD
+    /* Add Ewald potential shift to vc_sub for convenience */
+#            ifdef CHECK_EXCLS
+    vc_sub_S0 = maskAdd(vc_sub_S0, sh_ewald_S, interact_S0);
+    vc_sub_S2 = maskAdd(vc_sub_S2, sh_ewald_S, interact_S2);
+#            else
+    SP0vc_sub_S0  = SP0vc_sub_S0 + sh_ewald_S;
+    SP1vc_sub_S0  = SP1vc_sub_S0 + sh_ewald_S;
+    SP2vc_sub_S0  = SP2vc_sub_S0 + sh_ewald_S;
+    SP0vc_sub_S2  = SP0vc_sub_S2 + sh_ewald_S;
+    SP1vc_sub_S2  = SP1vc_sub_S2 + sh_ewald_S;
+    SP2vc_sub_S2  = SP2vc_sub_S2 + sh_ewald_S;
+#            endif
+#        endif
+
+    /* and (merge) mask energy for cut-off and diagonal */
+    SP0vcoul_S0 = maskzMul(SP0qq_S0, SP0rinv_ex_S0 - SP0vc_sub_S0, SP0wco_S0);
+    SP1vcoul_S0 = maskzMul(SP1qq_S0, SP1rinv_ex_S0 - SP1vc_sub_S0, SP1wco_S0);
+    SP2vcoul_S0 = maskzMul(SP2qq_S0, SP2rinv_ex_S0 - SP2vc_sub_S0, SP2wco_S0);
+    SP0vcoul_S2 = maskzMul(SP0qq_S2, SP0rinv_ex_S2 - SP0vc_sub_S2, SP0wco_S2);
+    SP1vcoul_S2 = maskzMul(SP1qq_S2, SP1rinv_ex_S2 - SP1vc_sub_S2, SP1wco_S2);
+    SP2vcoul_S2 = maskzMul(SP2qq_S2, SP2rinv_ex_S2 - SP2vc_sub_S2, SP2wco_S2);
+
+#    endif
+
+#endif /* CALC_COULOMB */
+
+#ifdef CALC_LJ
+    /* Lennard-Jones interaction */
+
+#    ifdef VDW_CUTOFF_CHECK
+    SP0wco_vdw_S0 = (SP0rsq_S0 < rcvdw2_S);
+    SP1wco_vdw_S0 = (SP1rsq_S0 < rcvdw2_S);
+    SP2wco_vdw_S0 = (SP2rsq_S0 < rcvdw2_S);
+#        ifndef HALF_LJ
+    SP0wco_vdw_S2 = (SP0rsq_S2 < rcvdw2_S);
+    SP1wco_vdw_S2 = (SP1rsq_S2 < rcvdw2_S);
+    SP2wco_vdw_S2 = (SP2rsq_S2 < rcvdw2_S);
+#        endif
+#    else
+    /* Same cut-off for Coulomb and VdW, reuse the registers */
+#        define SP0wco_vdw_S0 SP0wco_S0
+#        define SP1wco_vdw_S0 SP1wco_S0
+#        define SP2wco_vdw_S0 SP2wco_S0
+#        define SP0wco_vdw_S2 SP0wco_S2
+#        define SP1wco_vdw_S2 SP1wco_S2
+#        define SP2wco_vdw_S2 SP2wco_S2
+#    endif
+
+#    ifndef LJ_COMB_LB
+    SP0rinvsix_S0 = SP0rinvsq_S0 * SP0rinvsq_S0;
+    SP1rinvsix_S0 = SP1rinvsq_S0 * SP1rinvsq_S0;
+    SP2rinvsix_S0 = SP2rinvsq_S0 * SP2rinvsq_S0;
+#        ifdef EXCL_FORCES
+    rinvsix_S0 = maskzMul(rinvsix_S0, rinvsq_S0, interact_S0);
+#        else
+    SP0rinvsix_S0 = SP0rinvsix_S0 * SP0rinvsq_S0;
+    SP1rinvsix_S0 = SP1rinvsix_S0 * SP1rinvsq_S0;
+    SP2rinvsix_S0 = SP2rinvsix_S0 * SP2rinvsq_S0;
+#        endif
+#        ifndef HALF_LJ
+    SP0rinvsix_S2 = SP0rinvsq_S2 * SP0rinvsq_S2;
+    SP1rinvsix_S2 = SP1rinvsq_S2 * SP1rinvsq_S2;
+    SP2rinvsix_S2 = SP2rinvsq_S2 * SP2rinvsq_S2;
+#            ifdef EXCL_FORCES
+    rinvsix_S2 = maskzMul(rinvsix_S2, rinvsq_S2, interact_S2);
+#            else
+    SP0rinvsix_S2 = SP0rinvsix_S2 * SP0rinvsq_S2;
+    SP1rinvsix_S2 = SP1rinvsix_S2 * SP1rinvsq_S2;
+    SP2rinvsix_S2 = SP2rinvsix_S2 * SP2rinvsq_S2;
+#            endif
+#        endif
+
+#        if defined LJ_CUT || defined LJ_POT_SWITCH
+    /* We have plain LJ or LJ-PME with simple C6/6 C12/12 coefficients */
+    SP0FrLJ6_S0 = SP0c6_S0 * SP0rinvsix_S0;
+    SP1FrLJ6_S0 = SP1c6_S0 * SP1rinvsix_S0;
+    SP2FrLJ6_S0 = SP2c6_S0 * SP2rinvsix_S0;
+#            ifndef HALF_LJ
+    SP0FrLJ6_S2 = SP0c6_S2 * SP0rinvsix_S2;
+    SP1FrLJ6_S2 = SP1c6_S2 * SP1rinvsix_S2;
+    SP2FrLJ6_S2 = SP2c6_S2 * SP2rinvsix_S2;
+#            endif
+    SP0FrLJ12_S0 = SP0c12_S0 * SP0rinvsix_S0 * SP0rinvsix_S0;
+    SP1FrLJ12_S0 = SP1c12_S0 * SP1rinvsix_S0 * SP1rinvsix_S0;
+    SP2FrLJ12_S0 = SP2c12_S0 * SP2rinvsix_S0 * SP2rinvsix_S0;
+#            ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0c12_S2 * SP0rinvsix_S2 * SP0rinvsix_S2;
+    SP1FrLJ12_S2 = SP1c12_S2 * SP1rinvsix_S2 * SP1rinvsix_S2;
+    SP2FrLJ12_S2 = SP2c12_S2 * SP2rinvsix_S2 * SP2rinvsix_S2;
+#            endif
+#        endif
+
+#        if defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    /* We switch the LJ force */
+    SP0r_S0    = SP0rsq_S0 * SP0rinv_S0;
+    SP1r_S0    = SP1rsq_S0 * SP1rinv_S0;
+    SP2r_S0    = SP2rsq_S0 * SP2rinv_S0;
+    SP0rsw_S0  = max(SP0r_S0 - rswitch_S, zero_S);
+    SP1rsw_S0  = max(SP1r_S0 - rswitch_S, zero_S);
+    SP2rsw_S0  = max(SP2r_S0 - rswitch_S, zero_S);
+    SP0rsw2_S0 = SP0rsw_S0 * SP0rsw_S0;
+    SP1rsw2_S0 = SP1rsw_S0 * SP1rsw_S0;
+    SP2rsw2_S0 = SP2rsw_S0 * SP2rsw_S0;
+#            ifndef HALF_LJ
+    SP0r_S2    = SP0rsq_S2 * SP0rinv_S2;
+    SP1r_S2    = SP1rsq_S2 * SP1rinv_S2;
+    SP2r_S2    = SP2rsq_S2 * SP2rinv_S2;
+    SP0rsw_S2  = max(SP0r_S2 - rswitch_S, zero_S);
+    SP1rsw_S2  = max(SP1r_S2 - rswitch_S, zero_S);
+    SP2rsw_S2  = max(SP2r_S2 - rswitch_S, zero_S);
+    SP0rsw2_S2 = SP0rsw_S2 * SP0rsw_S2;
+    SP1rsw2_S2 = SP1rsw_S2 * SP1rsw_S2;
+    SP2rsw2_S2 = SP2rsw_S2 * SP2rsw_S2;
+#            endif
+#        endif
+
+#        ifdef LJ_FORCE_SWITCH
+
+#            define add_fr_switch(fr, rsw, rsw2_r, c2, c3) fma(fma(c3, rsw, c2), rsw2_r, fr)
+    SimdReal SP0rsw2_r_S0 = SP0rsw2_S0 * SP0r_S0;
+    SimdReal SP1rsw2_r_S0 = SP1rsw2_S0 * SP1r_S0;
+    SimdReal SP2rsw2_r_S0 = SP2rsw2_S0 * SP2r_S0;
+    SP0FrLJ6_S0           = SP0c6_S0 * add_fr_switch(SP0rinvsix_S0, SP0rsw_S0, SP0rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+    SP1FrLJ6_S0           = SP1c6_S0 * add_fr_switch(SP1rinvsix_S0, SP1rsw_S0, SP1rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+    SP2FrLJ6_S0           = SP2c6_S0 * add_fr_switch(SP2rinvsix_S0, SP2rsw_S0, SP2rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+#            ifndef HALF_LJ
+    SimdReal SP0rsw2_r_S2 = SP0rsw2_S2 * SP0r_S2;
+    SimdReal SP1rsw2_r_S2 = SP1rsw2_S2 * SP1r_S2;
+    SimdReal SP2rsw2_r_S2 = SP2rsw2_S2 * SP2r_S2;
+    SP0FrLJ6_S2           = SP0c6_S2 * add_fr_switch(SP0rinvsix_S2, SP0rsw_S2, SP0rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+    SP1FrLJ6_S2           = SP1c6_S2 * add_fr_switch(SP1rinvsix_S2, SP1rsw_S2, SP1rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+    SP2FrLJ6_S2           = SP2c6_S2 * add_fr_switch(SP2rinvsix_S2, SP2rsw_S2, SP2rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+#            endif
+    SP0FrLJ12_S0 = SP0c12_S0 * add_fr_switch(SP0rinvsix_S0 * SP0rinvsix_S0, SP0rsw_S0, SP0rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+    SP1FrLJ12_S0 = SP1c12_S0 * add_fr_switch(SP1rinvsix_S0 * SP1rinvsix_S0, SP1rsw_S0, SP1rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+    SP2FrLJ12_S0 = SP2c12_S0 * add_fr_switch(SP2rinvsix_S0 * SP2rinvsix_S0, SP2rsw_S0, SP2rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+#            ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0c12_S2 * add_fr_switch(SP0rinvsix_S2 * SP0rinvsix_S2, SP0rsw_S2, SP0rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+    SP1FrLJ12_S2 = SP1c12_S2 * add_fr_switch(SP1rinvsix_S2 * SP1rinvsix_S2, SP1rsw_S2, SP1rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+    SP2FrLJ12_S2 = SP2c12_S2 * add_fr_switch(SP2rinvsix_S2 * SP2rinvsix_S2, SP2rsw_S2, SP2rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+#            endif
+#            undef add_fr_switch
+#        endif /* LJ_FORCE_SWITCH */
+
+#    endif /* not LJ_COMB_LB */
+
+#    ifdef LJ_COMB_LB
+    SP0sir_S0 = SP0sig_S0 * SP0rinv_S0;
+    SP1sir_S0 = SP1sig_S0 * SP1rinv_S0;
+    SP2sir_S0 = SP2sig_S0 * SP2rinv_S0;
+#        ifndef HALF_LJ
+    SP0sir_S2 = SP0sig_S2 * SP0rinv_S2;
+    SP1sir_S2 = SP1sig_S2 * SP1rinv_S2;
+    SP2sir_S2 = SP2sig_S2 * SP2rinv_S2;
+#        endif
+    SP0sir2_S0 = SP0sir_S0 * SP0sir_S0;
+    SP1sir2_S0 = SP1sir_S0 * SP1sir_S0;
+    SP2sir2_S0 = SP2sir_S0 * SP2sir_S0;
+#        ifndef HALF_LJ
+    SP0sir2_S2 = SP0sir_S2 * SP0sir_S2;
+    SP1sir2_S2 = SP1sir_S2 * SP1sir_S2;
+    SP2sir2_S2 = SP2sir_S2 * SP2sir_S2;
+#        endif
+#        ifdef VDW_CUTOFF_CHECK
+    SP0sir6_S0 = maskzMul(SP0sir2_S0, SP0sir2_S0, SP0wco_vdw_S0);
+    SP1sir6_S0 = maskzMul(SP1sir2_S0, SP1sir2_S0, SP1wco_vdw_S0);
+    SP2sir6_S0 = maskzMul(SP2sir2_S0, SP2sir2_S0, SP2wco_vdw_S0);
+#        else
+    SP0sir6_S0    = SP0sir2_S0 * SP0sir2_S0;
+    SP1sir6_S0    = SP1sir2_S0 * SP1sir2_S0;
+    SP2sir6_S0    = SP2sir2_S0 * SP2sir2_S0;
+#        endif
+#        ifdef EXCL_FORCES
+    sir6_S0 = maskzMul(sir6_S0, sir2_S0, interact_S0);
+#        else
+    SP0sir6_S0    = SP0sir6_S0 * SP0sir2_S0;
+    SP1sir6_S0    = SP1sir6_S0 * SP1sir2_S0;
+    SP2sir6_S0    = SP2sir6_S0 * SP2sir2_S0;
+#        endif
+#        ifndef HALF_LJ
+#            ifdef VDW_CUTOFF_CHECK
+    SP0sir6_S2 = maskzMul(SP0sir2_S2, SP0sir2_S2, SP0wco_vdw_S2);
+    SP1sir6_S2 = maskzMul(SP1sir2_S2, SP1sir2_S2, SP1wco_vdw_S2);
+    SP2sir6_S2 = maskzMul(SP2sir2_S2, SP2sir2_S2, SP2wco_vdw_S2);
+#            else
+    SP0sir6_S2    = SP0sir2_S2 * SP0sir2_S2;
+    SP1sir6_S2    = SP1sir2_S2 * SP1sir2_S2;
+    SP2sir6_S2    = SP2sir2_S2 * SP2sir2_S2;
+#            endif
+#            ifdef EXCL_FORCES
+    sir6_S2 = maskzMul(sir6_S2, sir2_S2, interact_S2);
+#            else
+    SP0sir6_S2    = SP0sir6_S2 * SP0sir2_S2;
+    SP1sir6_S2    = SP1sir6_S2 * SP1sir2_S2;
+    SP2sir6_S2    = SP2sir6_S2 * SP2sir2_S2;
+#            endif
+#        endif
+    SP0FrLJ6_S0 = SP0eps_S0 * SP0sir6_S0;
+    SP1FrLJ6_S0 = SP1eps_S0 * SP1sir6_S0;
+    SP2FrLJ6_S0 = SP2eps_S0 * SP2sir6_S0;
+#        ifndef HALF_LJ
+    SP0FrLJ6_S2 = SP0eps_S2 * SP0sir6_S2;
+    SP1FrLJ6_S2 = SP1eps_S2 * SP1sir6_S2;
+    SP2FrLJ6_S2 = SP2eps_S2 * SP2sir6_S2;
+#        endif
+    SP0FrLJ12_S0 = SP0FrLJ6_S0 * SP0sir6_S0;
+    SP1FrLJ12_S0 = SP1FrLJ6_S0 * SP1sir6_S0;
+    SP2FrLJ12_S0 = SP2FrLJ6_S0 * SP2sir6_S0;
+#        ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0FrLJ6_S2 * SP0sir6_S2;
+    SP1FrLJ12_S2 = SP1FrLJ6_S2 * SP1sir6_S2;
+    SP2FrLJ12_S2 = SP2FrLJ6_S2 * SP2sir6_S2;
+#        endif
+#        if defined CALC_ENERGIES
+    /* We need C6 and C12 to calculate the LJ potential shift */
+    SP0sig2_S0 = SP0sig_S0 * SP0sig_S0;
+    SP1sig2_S0 = SP1sig_S0 * SP1sig_S0;
+    SP2sig2_S0 = SP2sig_S0 * SP2sig_S0;
+#            ifndef HALF_LJ
+    SP0sig2_S2 = SP0sig_S2 * SP0sig_S2;
+    SP1sig2_S2 = SP1sig_S2 * SP1sig_S2;
+    SP2sig2_S2 = SP2sig_S2 * SP2sig_S2;
+#            endif
+    SP0sig6_S0 = SP0sig2_S0 * SP0sig2_S0 * SP0sig2_S0;
+    SP1sig6_S0 = SP1sig2_S0 * SP1sig2_S0 * SP1sig2_S0;
+    SP2sig6_S0 = SP2sig2_S0 * SP2sig2_S0 * SP2sig2_S0;
+#            ifndef HALF_LJ
+    SP0sig6_S2 = SP0sig2_S2 * SP0sig2_S2 * SP0sig2_S2;
+    SP1sig6_S2 = SP1sig2_S2 * SP1sig2_S2 * SP1sig2_S2;
+    SP2sig6_S2 = SP2sig2_S2 * SP2sig2_S2 * SP2sig2_S2;
+#            endif
+    SimdReal SP0c6_S0 = SP0eps_S0 * SP0sig6_S0;
+    SimdReal SP1c6_S0 = SP1eps_S0 * SP1sig6_S0;
+    SimdReal SP2c6_S0 = SP2eps_S0 * SP2sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0c6_S2 = SP0eps_S2 * SP0sig6_S2;
+    SimdReal SP1c6_S2 = SP1eps_S2 * SP1sig6_S2;
+    SimdReal SP2c6_S2 = SP2eps_S2 * SP2sig6_S2;
+#            endif
+    SimdReal SP0c12_S0 = SP0c6_S0 * SP0sig6_S0;
+    SimdReal SP1c12_S0 = SP1c6_S0 * SP1sig6_S0;
+    SimdReal SP2c12_S0 = SP2c6_S0 * SP2sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0c12_S2 = SP0c6_S2 * SP0sig6_S2;
+    SimdReal SP1c12_S2 = SP1c6_S2 * SP1sig6_S2;
+    SimdReal SP2c12_S2 = SP2c6_S2 * SP2sig6_S2;
+#            endif
+#        endif
+#    endif /* LJ_COMB_LB */
+
+    /* Determine the total scalar LJ force*r */
+    SP0frLJ_S0 = SP0FrLJ12_S0 - SP0FrLJ6_S0;
+    SP1frLJ_S0 = SP1FrLJ12_S0 - SP1FrLJ6_S0;
+    SP2frLJ_S0 = SP2FrLJ12_S0 - SP2FrLJ6_S0;
+#    ifndef HALF_LJ
+    SP0frLJ_S2 = SP0FrLJ12_S2 - SP0FrLJ6_S2;
+    SP1frLJ_S2 = SP1FrLJ12_S2 - SP1FrLJ6_S2;
+    SP2frLJ_S2 = SP2FrLJ12_S2 - SP2FrLJ6_S2;
+#    endif
+
+#    if (defined LJ_CUT || defined LJ_FORCE_SWITCH) && defined CALC_ENERGIES
+
+#        ifdef LJ_CUT
+    /* Calculate the LJ energies, with constant potential shift */
+    SimdReal SP0VLJ6_S0 = sixth_S * fma(SP0c6_S0, p6_cpot_S, SP0FrLJ6_S0);
+    SimdReal SP1VLJ6_S0 = sixth_S * fma(SP1c6_S0, p6_cpot_S, SP1FrLJ6_S0);
+    SimdReal SP2VLJ6_S0 = sixth_S * fma(SP2c6_S0, p6_cpot_S, SP2FrLJ6_S0);
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ6_S2 = sixth_S * fma(SP0c6_S2, p6_cpot_S, SP0FrLJ6_S2);
+    SimdReal SP1VLJ6_S2 = sixth_S * fma(SP1c6_S2, p6_cpot_S, SP1FrLJ6_S2);
+    SimdReal SP2VLJ6_S2 = sixth_S * fma(SP2c6_S2, p6_cpot_S, SP2FrLJ6_S2);
+#            endif
+    SimdReal SP0VLJ12_S0 = twelveth_S * fma(SP0c12_S0, p12_cpot_S, SP0FrLJ12_S0);
+    SimdReal SP1VLJ12_S0 = twelveth_S * fma(SP1c12_S0, p12_cpot_S, SP1FrLJ12_S0);
+    SimdReal SP2VLJ12_S0 = twelveth_S * fma(SP2c12_S0, p12_cpot_S, SP2FrLJ12_S0);
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ12_S2 = twelveth_S * fma(SP0c12_S2, p12_cpot_S, SP0FrLJ12_S2);
+    SimdReal SP1VLJ12_S2 = twelveth_S * fma(SP1c12_S2, p12_cpot_S, SP1FrLJ12_S2);
+    SimdReal SP2VLJ12_S2 = twelveth_S * fma(SP2c12_S2, p12_cpot_S, SP2FrLJ12_S2);
+#            endif
+#        endif /* LJ_CUT */
+
+#        ifdef LJ_FORCE_SWITCH
+#            define v_fswitch_pr(rsw, rsw2, c0, c3, c4) fma(fma(c4, rsw, c3), (rsw2) * (rsw), c0)
+
+    SimdReal SP0VLJ6_S0 = SP0c6_S0 * fma(sixth_S, SP0rinvsix_S0, v_fswitch_pr(SP0rsw_S0, SP0rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP1VLJ6_S0 = SP1c6_S0 * fma(sixth_S, SP1rinvsix_S0, v_fswitch_pr(SP1rsw_S0, SP1rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP2VLJ6_S0 = SP2c6_S0 * fma(sixth_S, SP2rinvsix_S0, v_fswitch_pr(SP2rsw_S0, SP2rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ6_S2 = SP0c6_S2 * fma(sixth_S, SP0rinvsix_S2, v_fswitch_pr(SP0rsw_S2, SP0rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP1VLJ6_S2 = SP1c6_S2 * fma(sixth_S, SP1rinvsix_S2, v_fswitch_pr(SP1rsw_S2, SP1rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP2VLJ6_S2 = SP2c6_S2 * fma(sixth_S, SP2rinvsix_S2, v_fswitch_pr(SP2rsw_S2, SP2rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+#            endif
+    SimdReal SP0VLJ12_S0 = SP0c12_S0 * fma(twelveth_S, SP0rinvsix_S0 * SP0rinvsix_S0, v_fswitch_pr(SP0rsw_S0, SP0rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP1VLJ12_S0 = SP1c12_S0 * fma(twelveth_S, SP1rinvsix_S0 * SP1rinvsix_S0, v_fswitch_pr(SP1rsw_S0, SP1rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP2VLJ12_S0 = SP2c12_S0 * fma(twelveth_S, SP2rinvsix_S0 * SP2rinvsix_S0, v_fswitch_pr(SP2rsw_S0, SP2rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ12_S2 = SP0c12_S2 * fma(twelveth_S, SP0rinvsix_S2 * SP0rinvsix_S2, v_fswitch_pr(SP0rsw_S2, SP0rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP1VLJ12_S2 = SP1c12_S2 * fma(twelveth_S, SP1rinvsix_S2 * SP1rinvsix_S2, v_fswitch_pr(SP1rsw_S2, SP1rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP2VLJ12_S2 = SP2c12_S2 * fma(twelveth_S, SP2rinvsix_S2 * SP2rinvsix_S2, v_fswitch_pr(SP2rsw_S2, SP2rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+#            endif
+#            undef v_fswitch_pr
+#        endif /* LJ_FORCE_SWITCH */
+
+    /* Add up the repulsion and dispersion */
+    SimdReal SP0VLJ_S0 = SP0VLJ12_S0 - SP0VLJ6_S0;
+    SimdReal SP1VLJ_S0 = SP1VLJ12_S0 - SP1VLJ6_S0;
+    SimdReal SP2VLJ_S0 = SP2VLJ12_S0 - SP2VLJ6_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0VLJ_S2 = SP0VLJ12_S2 - SP0VLJ6_S2;
+    SimdReal SP1VLJ_S2 = SP1VLJ12_S2 - SP1VLJ6_S2;
+    SimdReal SP2VLJ_S2 = SP2VLJ12_S2 - SP2VLJ6_S2;
+#        endif
+
+#    endif /* (LJ_CUT || LJ_FORCE_SWITCH) && CALC_ENERGIES */
+
+#    ifdef LJ_POT_SWITCH
+    /* We always need the potential, since it is needed for the force */
+    SimdReal SP0VLJ_S0 = fnma(sixth_S, SP0FrLJ6_S0, twelveth_S * SP0FrLJ12_S0);
+    SimdReal SP1VLJ_S0 = fnma(sixth_S, SP1FrLJ6_S0, twelveth_S * SP1FrLJ12_S0);
+    SimdReal SP2VLJ_S0 = fnma(sixth_S, SP2FrLJ6_S0, twelveth_S * SP2FrLJ12_S0);
+#        ifndef HALF_LJ
+    SimdReal SP0VLJ_S2 = fnma(sixth_S, SP0FrLJ6_S2, twelveth_S * SP0FrLJ12_S2);
+    SimdReal SP1VLJ_S2 = fnma(sixth_S, SP1FrLJ6_S2, twelveth_S * SP1FrLJ12_S2);
+    SimdReal SP2VLJ_S2 = fnma(sixth_S, SP2FrLJ6_S2, twelveth_S * SP2FrLJ12_S2);
+#        endif
+
+    {
+        SimdReal SP0sw_S0, SP0dsw_S0;
+        SimdReal SP1sw_S0, SP1dsw_S0;
+        SimdReal SP2sw_S0, SP2dsw_S0;
+#        ifndef HALF_LJ
+        SimdReal SP0sw_S2, SP0dsw_S2;
+        SimdReal SP1sw_S2, SP1dsw_S2;
+        SimdReal SP2sw_S2, SP2dsw_S2;
+#        endif
+
+#        define switch_pr(rsw, rsw2, c3, c4, c5) \
+            fma(fma(fma(c5, rsw, c4), rsw, c3), (rsw2) * (rsw), one_S)
+#        define dswitch_pr(rsw, rsw2, c2, c3, c4) fma(fma(c4, rsw, c3), rsw, c2) * (rsw2)
+
+        SP0sw_S0  = switch_pr(SP0rsw_S0, SP0rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP1sw_S0  = switch_pr(SP1rsw_S0, SP1rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP2sw_S0  = switch_pr(SP2rsw_S0, SP2rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP0dsw_S0 = dswitch_pr(SP0rsw_S0, SP0rsw2_S0, swF2_S, swF3_S, swF4_S);
+        SP1dsw_S0 = dswitch_pr(SP1rsw_S0, SP1rsw2_S0, swF2_S, swF3_S, swF4_S);
+        SP2dsw_S0 = dswitch_pr(SP2rsw_S0, SP2rsw2_S0, swF2_S, swF3_S, swF4_S);
+#        ifndef HALF_LJ
+        SP0sw_S2  = switch_pr(SP0rsw_S2, SP0rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP1sw_S2  = switch_pr(SP1rsw_S2, SP1rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP2sw_S2  = switch_pr(SP2rsw_S2, SP2rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP0dsw_S2 = dswitch_pr(SP0rsw_S2, SP0rsw2_S2, swF2_S, swF3_S, swF4_S);
+        SP1dsw_S2 = dswitch_pr(SP1rsw_S2, SP1rsw2_S2, swF2_S, swF3_S, swF4_S);
+        SP2dsw_S2 = dswitch_pr(SP2rsw_S2, SP2rsw2_S2, swF2_S, swF3_S, swF4_S);
+#        endif
+        SP0frLJ_S0 = fnma(SP0dsw_S0 * SP0VLJ_S0, SP0r_S0, SP0sw_S0 * SP0frLJ_S0);
+        SP1frLJ_S0 = fnma(SP1dsw_S0 * SP1VLJ_S0, SP1r_S0, SP1sw_S0 * SP1frLJ_S0);
+        SP2frLJ_S0 = fnma(SP2dsw_S0 * SP2VLJ_S0, SP2r_S0, SP2sw_S0 * SP2frLJ_S0);
+#        ifndef HALF_LJ
+        SP0frLJ_S2 = fnma(SP0dsw_S2 * SP0VLJ_S2, SP0r_S2, SP0sw_S2 * SP0frLJ_S2);
+        SP1frLJ_S2 = fnma(SP1dsw_S2 * SP1VLJ_S2, SP1r_S2, SP1sw_S2 * SP1frLJ_S2);
+        SP2frLJ_S2 = fnma(SP2dsw_S2 * SP2VLJ_S2, SP2r_S2, SP2sw_S2 * SP2frLJ_S2);
+#        endif
+#        ifdef CALC_ENERGIES
+        SP0VLJ_S0 = SP0sw_S0 * SP0VLJ_S0;
+        SP1VLJ_S0 = SP1sw_S0 * SP1VLJ_S0;
+        SP2VLJ_S0 = SP2sw_S0 * SP2VLJ_S0;
+#            ifndef HALF_LJ
+        SP0VLJ_S2 = SP0sw_S2 * SP0VLJ_S2;
+        SP1VLJ_S2 = SP1sw_S2 * SP1VLJ_S2;
+        SP2VLJ_S2 = SP2sw_S2 * SP2VLJ_S2;
+#            endif
+#        endif
+
+#        undef switch_pr
+#        undef dswitch_pr
+    }
+#    endif /* LJ_POT_SWITCH */
+
+#    if defined CALC_ENERGIES && defined CHECK_EXCLS
+    /* The potential shift should be removed for excluded pairs */
+    VLJ_S0 = selectByMask(VLJ_S0, interact_S0);
+#        ifndef HALF_LJ
+    VLJ_S2 = selectByMask(VLJ_S2, interact_S2);
+#        endif
+#    endif
+
+#    ifdef LJ_EWALD_GEOM
+    {
+        SimdReal SP0c6s_j_S;
+        SimdReal SP1c6s_j_S;
+        SimdReal SP2c6s_j_S;
+        SimdReal SP0c6grid_S0, SP0rinvsix_nm_S0, SP0cr2_S0, SP0expmcr2_S0, SP0poly_S0;
+        SimdReal SP1c6grid_S0, SP1rinvsix_nm_S0, SP1cr2_S0, SP1expmcr2_S0, SP1poly_S0;
+        SimdReal SP2c6grid_S0, SP2rinvsix_nm_S0, SP2cr2_S0, SP2expmcr2_S0, SP2poly_S0;
+#        ifndef HALF_LJ
+        SimdReal SP0c6grid_S2, SP0rinvsix_nm_S2, SP0cr2_S2, SP0expmcr2_S2, SP0poly_S2;
+        SimdReal SP1c6grid_S2, SP1rinvsix_nm_S2, SP1cr2_S2, SP1expmcr2_S2, SP1poly_S2;
+        SimdReal SP2c6grid_S2, SP2rinvsix_nm_S2, SP2cr2_S2, SP2expmcr2_S2, SP2poly_S2;
+#        endif
+#        ifdef CALC_ENERGIES
+        SimdReal SP0sh_mask_S0;
+        SimdReal SP1sh_mask_S0;
+        SimdReal SP2sh_mask_S0;
+#            ifndef HALF_LJ
+        SimdReal SP0sh_mask_S2;
+        SimdReal SP1sh_mask_S2;
+        SimdReal SP2sh_mask_S2;
+#            endif
+#        endif
+
+        /* Determine C6 for the grid using the geometric combination rule */
+        SP0c6s_j_S   = loadDuplicateHsimd(ljc + SP0aj2);
+        SP1c6s_j_S   = loadDuplicateHsimd(ljc + SP1aj2);
+        SP2c6s_j_S   = loadDuplicateHsimd(ljc + SP2aj2);
+        SP0c6grid_S0 = c6s_S0 * SP0c6s_j_S;
+        SP1c6grid_S0 = c6s_S0 * SP1c6s_j_S;
+        SP2c6grid_S0 = c6s_S0 * SP2c6s_j_S;
+#        ifndef HALF_LJ
+        SP0c6grid_S2 = c6s_S2 * SP0c6s_j_S;
+        SP1c6grid_S2 = c6s_S2 * SP1c6s_j_S;
+        SP2c6grid_S2 = c6s_S2 * SP2c6s_j_S;
+#        endif
+
+#        ifdef CHECK_EXCLS
+        /* Recalculate rinvsix without exclusion mask (compiler might optimize) */
+        rinvsix_nm_S0 = rinvsq_S0 * rinvsq_S0 * rinvsq_S0;
+#            ifndef HALF_LJ
+        rinvsix_nm_S2 = rinvsq_S2 * rinvsq_S2 * rinvsq_S2;
+#            endif
+#        else
+        /* We didn't use a mask, so we can copy */
+        SP0rinvsix_nm_S0 = SP0rinvsix_S0;
+        SP1rinvsix_nm_S0 = SP1rinvsix_S0;
+        SP2rinvsix_nm_S0 = SP2rinvsix_S0;
+#            ifndef HALF_LJ
+        SP0rinvsix_nm_S2 = SP0rinvsix_S2;
+        SP1rinvsix_nm_S2 = SP1rinvsix_S2;
+        SP2rinvsix_nm_S2 = SP2rinvsix_S2;
+#            endif
+#        endif
+
+        /* Mask for the cut-off to avoid overflow of cr2^2 */
+        SP0cr2_S0 = maskzMul(lje_c2_S, SP0rsq_S0, SP0wco_vdw_S0);
+        SP1cr2_S0 = maskzMul(lje_c2_S, SP1rsq_S0, SP1wco_vdw_S0);
+        SP2cr2_S0 = maskzMul(lje_c2_S, SP2rsq_S0, SP2wco_vdw_S0);
+#        ifndef HALF_LJ
+        SP0cr2_S2 = maskzMul(lje_c2_S, SP0rsq_S2, SP0wco_vdw_S2);
+        SP1cr2_S2 = maskzMul(lje_c2_S, SP1rsq_S2, SP1wco_vdw_S2);
+        SP2cr2_S2 = maskzMul(lje_c2_S, SP2rsq_S2, SP2wco_vdw_S2);
+#        endif
+        // Unsafe version of our exp() should be fine, since these arguments should never
+        // be smaller than -127 for any reasonable choice of cutoff or ewald coefficients.
+        SP0expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP0cr2_S0);
+        SP1expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP1cr2_S0);
+        SP2expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP2cr2_S0);
+#        ifndef HALF_LJ
+        SP0expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP0cr2_S2);
+        SP1expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP1cr2_S2);
+        SP2expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP2cr2_S2);
+#        endif
+
+        /* 1 + cr2 + 1/2*cr2^2 */
+        SP0poly_S0 = fma(fma(half_S, SP0cr2_S0, one_S), SP0cr2_S0, one_S);
+        SP1poly_S0 = fma(fma(half_S, SP1cr2_S0, one_S), SP1cr2_S0, one_S);
+        SP2poly_S0 = fma(fma(half_S, SP2cr2_S0, one_S), SP2cr2_S0, one_S);
+#        ifndef HALF_LJ
+        SP0poly_S2 = fma(fma(half_S, SP0cr2_S2, one_S), SP0cr2_S2, one_S);
+        SP1poly_S2 = fma(fma(half_S, SP1cr2_S2, one_S), SP1cr2_S2, one_S);
+        SP2poly_S2 = fma(fma(half_S, SP2cr2_S2, one_S), SP2cr2_S2, one_S);
+#        endif
+
+        /* We calculate LJ F*r = (6*C6)*(r^-6 - F_mesh/6), we use:
+         * r^-6*cexp*(1 + cr2 + cr2^2/2 + cr2^3/6) = cexp*(r^-6*poly + c^6/6)
+         */
+        SP0frLJ_S0 = fma(SP0c6grid_S0, fnma(SP0expmcr2_S0, fma(SP0rinvsix_nm_S0, SP0poly_S0, lje_c6_6_S), SP0rinvsix_nm_S0), SP0frLJ_S0);
+        SP1frLJ_S0 = fma(SP1c6grid_S0, fnma(SP1expmcr2_S0, fma(SP1rinvsix_nm_S0, SP1poly_S0, lje_c6_6_S), SP1rinvsix_nm_S0), SP1frLJ_S0);
+        SP2frLJ_S0 = fma(SP2c6grid_S0, fnma(SP2expmcr2_S0, fma(SP2rinvsix_nm_S0, SP2poly_S0, lje_c6_6_S), SP2rinvsix_nm_S0), SP2frLJ_S0);
+#        ifndef HALF_LJ
+        SP0frLJ_S2 = fma(SP0c6grid_S2, fnma(SP0expmcr2_S2, fma(SP0rinvsix_nm_S2, SP0poly_S2, lje_c6_6_S), SP0rinvsix_nm_S2), SP0frLJ_S2);
+        SP1frLJ_S2 = fma(SP1c6grid_S2, fnma(SP1expmcr2_S2, fma(SP1rinvsix_nm_S2, SP1poly_S2, lje_c6_6_S), SP1rinvsix_nm_S2), SP1frLJ_S2);
+        SP2frLJ_S2 = fma(SP2c6grid_S2, fnma(SP2expmcr2_S2, fma(SP2rinvsix_nm_S2, SP2poly_S2, lje_c6_6_S), SP2rinvsix_nm_S2), SP2frLJ_S2);
+#        endif
+
+#        ifdef CALC_ENERGIES
+#            ifdef CHECK_EXCLS
+        sh_mask_S0 = selectByMask(lje_vc_S, interact_S0);
+#                ifndef HALF_LJ
+        sh_mask_S2 = selectByMask(lje_vc_S, interact_S2);
+#                endif
+#            else
+        SP0sh_mask_S0 = lje_vc_S;
+        SP1sh_mask_S0 = lje_vc_S;
+        SP2sh_mask_S0 = lje_vc_S;
+#                ifndef HALF_LJ
+        SP0sh_mask_S2 = lje_vc_S;
+        SP1sh_mask_S2 = lje_vc_S;
+        SP2sh_mask_S2 = lje_vc_S;
+#                endif
+#            endif
+
+        SP0VLJ_S0 = fma(sixth_S * SP0c6grid_S0, fma(SP0rinvsix_nm_S0, fnma(SP0expmcr2_S0, SP0poly_S0, one_S), SP0sh_mask_S0), SP0VLJ_S0);
+        SP1VLJ_S0 = fma(sixth_S * SP1c6grid_S0, fma(SP1rinvsix_nm_S0, fnma(SP1expmcr2_S0, SP1poly_S0, one_S), SP1sh_mask_S0), SP1VLJ_S0);
+        SP2VLJ_S0 = fma(sixth_S * SP2c6grid_S0, fma(SP2rinvsix_nm_S0, fnma(SP2expmcr2_S0, SP2poly_S0, one_S), SP2sh_mask_S0), SP2VLJ_S0);
+#            ifndef HALF_LJ
+        SP0VLJ_S2 = fma(sixth_S * SP0c6grid_S2, fma(SP0rinvsix_nm_S2, fnma(SP0expmcr2_S2, SP0poly_S2, one_S), SP0sh_mask_S2), SP0VLJ_S2);
+        SP1VLJ_S2 = fma(sixth_S * SP1c6grid_S2, fma(SP1rinvsix_nm_S2, fnma(SP1expmcr2_S2, SP1poly_S2, one_S), SP1sh_mask_S2), SP1VLJ_S2);
+        SP2VLJ_S2 = fma(sixth_S * SP2c6grid_S2, fma(SP2rinvsix_nm_S2, fnma(SP2expmcr2_S2, SP2poly_S2, one_S), SP2sh_mask_S2), SP2VLJ_S2);
+#            endif
+#        endif /* CALC_ENERGIES */
+    }
+#    endif /* LJ_EWALD_GEOM */
+
+#    if defined VDW_CUTOFF_CHECK
+    /* frLJ is multiplied later by rinvsq, which is masked for the Coulomb
+     * cut-off, but if the VdW cut-off is shorter, we need to mask with that.
+     */
+    SP0frLJ_S0 = selectByMask(SP0frLJ_S0, SP0wco_vdw_S0);
+    SP1frLJ_S0 = selectByMask(SP1frLJ_S0, SP1wco_vdw_S0);
+    SP2frLJ_S0 = selectByMask(SP2frLJ_S0, SP2wco_vdw_S0);
+#        ifndef HALF_LJ
+    SP0frLJ_S2 = selectByMask(SP0frLJ_S2, SP0wco_vdw_S2);
+    SP1frLJ_S2 = selectByMask(SP1frLJ_S2, SP1wco_vdw_S2);
+    SP2frLJ_S2 = selectByMask(SP2frLJ_S2, SP2wco_vdw_S2);
+#        endif
+#    endif
+
+#    ifdef CALC_ENERGIES
+    /* The potential shift should be removed for pairs beyond cut-off */
+    SP0VLJ_S0 = selectByMask(SP0VLJ_S0, SP0wco_vdw_S0);
+    SP1VLJ_S0 = selectByMask(SP1VLJ_S0, SP1wco_vdw_S0);
+    SP2VLJ_S0 = selectByMask(SP2VLJ_S0, SP2wco_vdw_S0);
+#        ifndef HALF_LJ
+    SP0VLJ_S2 = selectByMask(SP0VLJ_S2, SP0wco_vdw_S2);
+    SP1VLJ_S2 = selectByMask(SP1VLJ_S2, SP1wco_vdw_S2);
+    SP2VLJ_S2 = selectByMask(SP2VLJ_S2, SP2wco_vdw_S2);
+#        endif
+#    endif
+
+#endif /* CALC_LJ */
+
+#ifdef CALC_ENERGIES
+#    ifdef ENERGY_GROUPS
+    /* Extract the group pair index per j pair.
+     * Energy groups are stored per i-cluster, so things get
+     * complicated when the i- and j-cluster size don't match.
+     */
+    {
+#        if UNROLLJ == 2
+#error
+        const int egps_j    = nbatParams.energrp[cj >> 1];
+        egp_jj[0]       = ((egps_j >> ((cj & 1) * egps_jshift)) & egps_jmask) * egps_jstride;
+#        else
+        /* We assume UNROLLI <= UNROLLJ */
+        for (int jdi = 0; jdi < UNROLLJ / UNROLLI; jdi++)
+        {
+            const int SP0egps_j = nbatParams.energrp[SP0cj * (UNROLLJ / UNROLLI) + jdi];
+            const int SP1egps_j = nbatParams.energrp[SP1cj * (UNROLLJ / UNROLLI) + jdi];
+            const int SP2egps_j = nbatParams.energrp[SP2cj * (UNROLLJ / UNROLLI) + jdi];
+            for (int jj = 0; jj < (UNROLLI / 2); jj++)
+            {
+                SP0egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP0egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+                SP1egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP1egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+                SP2egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP2egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+            }
+        }
+#        endif
+    }
+#    endif
+
+#    ifdef CALC_COULOMB
+#        ifndef ENERGY_GROUPS
+    vctot_S = vctot_S + SP0vcoul_S0 + SP0vcoul_S2;
+    vctot_S = vctot_S + SP1vcoul_S0 + SP1vcoul_S2;
+    vctot_S = vctot_S + SP2vcoul_S0 + SP2vcoul_S2;
+#        else
+    add_ener_grp_halves(SP0vcoul_S0, vctp[0], vctp[1], SP0egp_jj);
+    add_ener_grp_halves(SP1vcoul_S0, vctp[0], vctp[1], SP1egp_jj);
+    add_ener_grp_halves(SP2vcoul_S0, vctp[0], vctp[1], SP2egp_jj);
+    add_ener_grp_halves(SP0vcoul_S2, vctp[2], vctp[3], SP0egp_jj);
+    add_ener_grp_halves(SP1vcoul_S2, vctp[2], vctp[3], SP1egp_jj);
+    add_ener_grp_halves(SP2vcoul_S2, vctp[2], vctp[3], SP2egp_jj);
+#        endif
+#    endif
+
+#    ifdef CALC_LJ
+#        ifndef ENERGY_GROUPS
+    Vvdwtot_S = Vvdwtot_S
+                + SP0VLJ_S0
+                + SP1VLJ_S0
+                + SP2VLJ_S0
+#            ifndef HALF_LJ
+                + SP0VLJ_S2
+                + SP1VLJ_S2
+                + SP2VLJ_S2
+#            endif
+            ;
+#        else
+    add_ener_grp_halves(SP0VLJ_S0, vvdwtp[0], vvdwtp[1], SP0egp_jj);
+    add_ener_grp_halves(SP1VLJ_S0, vvdwtp[0], vvdwtp[1], SP1egp_jj);
+    add_ener_grp_halves(SP2VLJ_S0, vvdwtp[0], vvdwtp[1], SP2egp_jj);
+#            ifndef HALF_LJ
+    add_ener_grp_halves(SP0VLJ_S2, vvdwtp[2], vvdwtp[3], SP0egp_jj);
+    add_ener_grp_halves(SP1VLJ_S2, vvdwtp[2], vvdwtp[3], SP1egp_jj);
+    add_ener_grp_halves(SP2VLJ_S2, vvdwtp[2], vvdwtp[3], SP2egp_jj);
+#            endif
+#        endif
+#    endif /* CALC_LJ */
+#endif     /* CALC_ENERGIES */
+
+#ifdef CALC_LJ
+#    ifdef CALC_COULOMB
+    SP0fscal_S0 = SP0rinvsq_S0 * (SP0frcoul_S0 + SP0frLJ_S0);
+    SP1fscal_S0 = SP1rinvsq_S0 * (SP1frcoul_S0 + SP1frLJ_S0);
+    SP2fscal_S0 = SP2rinvsq_S0 * (SP2frcoul_S0 + SP2frLJ_S0);
+#    else
+    SP0fscal_S0 = SP0rinvsq_S0 * SP0frLJ_S0;
+    SP1fscal_S0 = SP1rinvsq_S0 * SP1frLJ_S0;
+    SP2fscal_S0 = SP2rinvsq_S0 * SP2frLJ_S0;
+#    endif
+#else
+    fscal_S0 = rinvsq_S0 * frcoul_S0;
+#endif /* CALC_LJ */
+#if defined CALC_LJ && !defined HALF_LJ
+#    ifdef CALC_COULOMB
+    SP0fscal_S2 = SP0rinvsq_S2 * (SP0frcoul_S2 + SP0frLJ_S2);
+    SP1fscal_S2 = SP1rinvsq_S2 * (SP1frcoul_S2 + SP1frLJ_S2);
+    SP2fscal_S2 = SP2rinvsq_S2 * (SP2frcoul_S2 + SP2frLJ_S2);
+#    else
+    SP0fscal_S2 = SP0rinvsq_S2 * SP0frLJ_S2;
+    SP1fscal_S2 = SP1rinvsq_S2 * SP1frLJ_S2;
+    SP2fscal_S2 = SP2rinvsq_S2 * SP2frLJ_S2;
+#    endif
+#else
+    /* Atom 2 and 3 don't have LJ, so only add Coulomb forces */
+    SP0fscal_S2 = SP0rinvsq_S2 * SP0frcoul_S2;
+    SP1fscal_S2 = SP1rinvsq_S2 * SP1frcoul_S2;
+    SP2fscal_S2 = SP2rinvsq_S2 * SP2frcoul_S2;
+#endif
+
+    /* Calculate temporary vectorial force */
+    SP0tx_S0 = SP0fscal_S0 * SP0dx_S0;
+    SP1tx_S0 = SP1fscal_S0 * SP1dx_S0;
+    SP2tx_S0 = SP2fscal_S0 * SP2dx_S0;
+    SP0tx_S2 = SP0fscal_S2 * SP0dx_S2;
+    SP1tx_S2 = SP1fscal_S2 * SP1dx_S2;
+    SP2tx_S2 = SP2fscal_S2 * SP2dx_S2;
+    SP0ty_S0 = SP0fscal_S0 * SP0dy_S0;
+    SP1ty_S0 = SP1fscal_S0 * SP1dy_S0;
+    SP2ty_S0 = SP2fscal_S0 * SP2dy_S0;
+    SP0ty_S2 = SP0fscal_S2 * SP0dy_S2;
+    SP1ty_S2 = SP1fscal_S2 * SP1dy_S2;
+    SP2ty_S2 = SP2fscal_S2 * SP2dy_S2;
+    SP0tz_S0 = SP0fscal_S0 * SP0dz_S0;
+    SP1tz_S0 = SP1fscal_S0 * SP1dz_S0;
+    SP2tz_S0 = SP2fscal_S0 * SP2dz_S0;
+    SP0tz_S2 = SP0fscal_S2 * SP0dz_S2;
+    SP1tz_S2 = SP1fscal_S2 * SP1dz_S2;
+    SP2tz_S2 = SP2fscal_S2 * SP2dz_S2;
+
+    /* Increment i atom force */
+    fix_S0 = fix_S0 + SP0tx_S0;
+    fix_S0 = fix_S0 + SP1tx_S0;
+    fix_S0 = fix_S0 + SP2tx_S0;
+    fix_S2 = fix_S2 + SP0tx_S2;
+    fix_S2 = fix_S2 + SP1tx_S2;
+    fix_S2 = fix_S2 + SP2tx_S2;
+    fiy_S0 = fiy_S0 + SP0ty_S0;
+    fiy_S0 = fiy_S0 + SP1ty_S0;
+    fiy_S0 = fiy_S0 + SP2ty_S0;
+    fiy_S2 = fiy_S2 + SP0ty_S2;
+    fiy_S2 = fiy_S2 + SP1ty_S2;
+    fiy_S2 = fiy_S2 + SP2ty_S2;
+    fiz_S0 = fiz_S0 + SP0tz_S0;
+    fiz_S0 = fiz_S0 + SP1tz_S0;
+    fiz_S0 = fiz_S0 + SP2tz_S0;
+    fiz_S2 = fiz_S2 + SP0tz_S2;
+    fiz_S2 = fiz_S2 + SP1tz_S2;
+    fiz_S2 = fiz_S2 + SP2tz_S2;
+
+    /* Decrement j atom force */
+    decr3Hsimd(f + SP0aj * DIM, SP0tx_S0 + SP0tx_S2, SP0ty_S0 + SP0ty_S2, SP0tz_S0 + SP0tz_S2);
+    decr3Hsimd(f + SP1aj * DIM, SP1tx_S0 + SP1tx_S2, SP1ty_S0 + SP1ty_S2, SP1tz_S0 + SP1tz_S2);
+    decr3Hsimd(f + SP2aj * DIM, SP2tx_S0 + SP2tx_S2, SP2ty_S0 + SP2ty_S2, SP2tz_S0 + SP2tz_S2);
+
+#undef SP0rinv_ex_S0
+#undef SP1rinv_ex_S0
+#undef SP2rinv_ex_S0
+#undef SP0rinv_ex_S2
+#undef SP1rinv_ex_S2
+#undef SP2rinv_ex_S2
+
+#undef SP0wco_vdw_S0
+#undef SP1wco_vdw_S0
+#undef SP2wco_vdw_S0
+#undef SP0wco_vdw_S2
+#undef SP1wco_vdw_S2
+#undef SP2wco_vdw_S2
+
+#undef EXCL_FORCES
+#undef SKIP_INVSQRT
diff -ruN orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_3.h gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_3.h
--- orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_3.h	1970-01-01 09:00:00.000000000 +0900
+++ gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner_3.h	2022-03-08 10:54:48.000000000 +0900
@@ -0,0 +1,1696 @@
+/*
+ * This file is part of the GROMACS molecular simulation package.
+ *
+ * Copyright 2012- The GROMACS Authors
+ * and the project initiators Erik Lindahl, Berk Hess and David van der Spoel.
+ * Consult the AUTHORS/COPYING files and https://www.gromacs.org for details.
+ *
+ * GROMACS is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public License
+ * as published by the Free Software Foundation; either version 2.1
+ * of the License, or (at your option) any later version.
+ *
+ * GROMACS is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with GROMACS; if not, see
+ * https://www.gnu.org/licenses, or write to the Free Software Foundation,
+ * Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA.
+ *
+ * If you want to redistribute modifications to GROMACS, please
+ * consider that scientific software is very special. Version
+ * control is crucial - bugs must be traceable. We will be happy to
+ * consider code for inclusion in the official distribution, but
+ * derived work must not be called official GROMACS. Details are found
+ * in the README & COPYING files - if they are missing, get the
+ * official version at https://www.gromacs.org.
+ *
+ * To help us fund GROMACS development, we humbly ask that you cite
+ * the research papers on the package. Check out https://www.gromacs.org.
+ */
+
+/* This is the innermost loop contents for the 4 x N atom simd kernel.
+ * This flavor of the kernel duplicates the data for N j-particles in
+ * 2xN wide simd registers to do operate on 2 i-particles at once.
+ * This leads to 4/2=2 sets of most instructions. Therefore we call
+ * this kernel 2x(N+N) = 2xnn
+ *
+ * This 2xnn kernel is basically the 4xn equivalent with half the registers
+ * and instructions removed.
+ *
+ * An alternative would be to load to different cluster of N j-particles
+ * into simd registers, giving a 4x(N+N) kernel. This doubles the amount
+ * of instructions, which could lead to better scheduling. But we actually
+ * observed worse scheduling for the AVX-256 4x8 normal analytical PME
+ * kernel, which has a lower pair throughput than 2x(4+4) with gcc 4.7.
+ * It could be worth trying this option, but it takes some more effort.
+ * This 2xnn kernel is basically the 4xn equivalent with
+ */
+
+
+/* When calculating RF or Ewald interactions we calculate the electrostatic/LJ
+ * forces on excluded atom pairs here in the non-bonded loops.
+ * But when energies and/or virial is required we calculate them
+ * separately to as then it is easier to separate the energy and virial
+ * contributions.
+ */
+#if defined CHECK_EXCLS && (defined CALC_COULOMB || defined LJ_EWALD_GEOM)
+#    define EXCL_FORCES
+#endif
+#if defined CALC_COULOMB || defined CALC_COUL_TAB || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH \
+        || defined LJ_COULOMB_LB || defined HALF_LJ || defined EXCL_FORCES || defined LJ_COMB_LB
+#    define SKIP_INVSQRT 0
+#else
+#    define SKIP_INVSQRT 1
+#endif
+
+#ifdef ENERGY_GROUPS
+    /* Energy group indices for two atoms packed into one int */
+    int SP0egp_jj[UNROLLJ / 2];
+    int SP1egp_jj[UNROLLJ / 2];
+    int SP2egp_jj[UNROLLJ / 2];
+    int SP3egp_jj[UNROLLJ / 2];
+#endif
+
+#ifdef CHECK_EXCLS
+    /* Interaction (non-exclusion) mask of all 1's or 0's */
+    SimdBool SP0interact_S0;
+    SimdBool SP1interact_S0;
+    SimdBool SP2interact_S0;
+    SimdBool SP3interact_S0;
+    SimdBool SP0interact_S2;
+    SimdBool SP1interact_S2;
+    SimdBool SP2interact_S2;
+    SimdBool SP3interact_S2;
+#endif
+
+    SimdReal SP0jx_S, SP0jy_S, SP0jz_S;
+    SimdReal SP1jx_S, SP1jy_S, SP1jz_S;
+    SimdReal SP2jx_S, SP2jy_S, SP2jz_S;
+    SimdReal SP3jx_S, SP3jy_S, SP3jz_S;
+    SimdReal SP0dx_S0, SP0dy_S0, SP0dz_S0;
+    SimdReal SP1dx_S0, SP1dy_S0, SP1dz_S0;
+    SimdReal SP2dx_S0, SP2dy_S0, SP2dz_S0;
+    SimdReal SP3dx_S0, SP3dy_S0, SP3dz_S0;
+    SimdReal SP0dx_S2, SP0dy_S2, SP0dz_S2;
+    SimdReal SP1dx_S2, SP1dy_S2, SP1dz_S2;
+    SimdReal SP2dx_S2, SP2dy_S2, SP2dz_S2;
+    SimdReal SP3dx_S2, SP3dy_S2, SP3dz_S2;
+    SimdReal SP0tx_S0, SP0ty_S0, SP0tz_S0;
+    SimdReal SP1tx_S0, SP1ty_S0, SP1tz_S0;
+    SimdReal SP2tx_S0, SP2ty_S0, SP2tz_S0;
+    SimdReal SP3tx_S0, SP3ty_S0, SP3tz_S0;
+    SimdReal SP0tx_S2, SP0ty_S2, SP0tz_S2;
+    SimdReal SP1tx_S2, SP1ty_S2, SP1tz_S2;
+    SimdReal SP2tx_S2, SP2ty_S2, SP2tz_S2;
+    SimdReal SP3tx_S2, SP3ty_S2, SP3tz_S2;
+    SimdReal SP0rsq_S0, SP0rinvsq_S0;
+    SimdReal SP1rsq_S0, SP1rinvsq_S0;
+    SimdReal SP2rsq_S0, SP2rinvsq_S0;
+    SimdReal SP3rsq_S0, SP3rinvsq_S0;
+    SimdReal SP0rsq_S2, SP0rinvsq_S2;
+    SimdReal SP1rsq_S2, SP1rinvsq_S2;
+    SimdReal SP2rsq_S2, SP2rinvsq_S2;
+    SimdReal SP3rsq_S2, SP3rinvsq_S2;
+#if !SKIP_INVSQRT
+    SimdReal SP0rinv_S0, SP0rinv_S2;
+    SimdReal SP1rinv_S0, SP1rinv_S2;
+    SimdReal SP2rinv_S0, SP2rinv_S2;
+    SimdReal SP3rinv_S0, SP3rinv_S2;
+#endif
+    /* wco: within cut-off, mask of all 1's or 0's */
+    SimdBool SP0wco_S0;
+    SimdBool SP1wco_S0;
+    SimdBool SP2wco_S0;
+    SimdBool SP3wco_S0;
+    SimdBool SP0wco_S2;
+    SimdBool SP1wco_S2;
+    SimdBool SP2wco_S2;
+    SimdBool SP3wco_S2;
+#ifdef VDW_CUTOFF_CHECK
+    SimdBool SP0wco_vdw_S0;
+    SimdBool SP1wco_vdw_S0;
+    SimdBool SP2wco_vdw_S0;
+    SimdBool SP3wco_vdw_S0;
+#    ifndef HALF_LJ
+    SimdBool SP0wco_vdw_S2;
+    SimdBool SP1wco_vdw_S2;
+    SimdBool SP2wco_vdw_S2;
+    SimdBool SP3wco_vdw_S2;
+#    endif
+#endif
+
+#if (defined CALC_COULOMB && defined CALC_COUL_TAB) || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    SimdReal                                                        SP0r_S0;
+    SimdReal                                                        SP1r_S0;
+    SimdReal                                                        SP2r_S0;
+    SimdReal                                                        SP3r_S0;
+#    if (defined CALC_COULOMB && defined CALC_COUL_TAB) || !defined HALF_LJ
+    SimdReal                                                        SP0r_S2;
+    SimdReal                                                        SP1r_S2;
+    SimdReal                                                        SP2r_S2;
+    SimdReal                                                        SP3r_S2;
+#    endif
+#endif
+
+#if defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    SimdReal SP0rsw_S0, SP0rsw2_S0;
+    SimdReal SP1rsw_S0, SP1rsw2_S0;
+    SimdReal SP2rsw_S0, SP2rsw2_S0;
+    SimdReal SP3rsw_S0, SP3rsw2_S0;
+#    ifndef HALF_LJ
+    SimdReal SP0rsw_S2, SP0rsw2_S2;
+    SimdReal SP1rsw_S2, SP1rsw2_S2;
+    SimdReal SP2rsw_S2, SP2rsw2_S2;
+    SimdReal SP3rsw_S2, SP3rsw2_S2;
+#    endif
+#endif
+
+#ifdef CALC_COULOMB
+#    ifdef CHECK_EXCLS
+    /* 1/r masked with the interaction mask */
+    SimdReal SP0rinv_ex_S0;
+    SimdReal SP1rinv_ex_S0;
+    SimdReal SP2rinv_ex_S0;
+    SimdReal SP3rinv_ex_S0;
+    SimdReal SP0rinv_ex_S2;
+    SimdReal SP1rinv_ex_S2;
+    SimdReal SP2rinv_ex_S2;
+    SimdReal SP3rinv_ex_S2;
+#    endif
+    SimdReal SP0jq_S;
+    SimdReal SP1jq_S;
+    SimdReal SP2jq_S;
+    SimdReal SP3jq_S;
+    SimdReal SP0qq_S0;
+    SimdReal SP1qq_S0;
+    SimdReal SP2qq_S0;
+    SimdReal SP3qq_S0;
+    SimdReal SP0qq_S2;
+    SimdReal SP1qq_S2;
+    SimdReal SP2qq_S2;
+    SimdReal SP3qq_S2;
+#    ifdef CALC_COUL_TAB
+    /* The force (PME mesh force) we need to subtract from 1/r^2 */
+    SimdReal SP0fsub_S0;
+    SimdReal SP1fsub_S0;
+    SimdReal SP2fsub_S0;
+    SimdReal SP3fsub_S0;
+    SimdReal SP0fsub_S2;
+    SimdReal SP1fsub_S2;
+    SimdReal SP2fsub_S2;
+    SimdReal SP3fsub_S2;
+#    endif
+#    ifdef CALC_COUL_EWALD
+    SimdReal SP0brsq_S0, SP0brsq_S2;
+    SimdReal SP1brsq_S0, SP1brsq_S2;
+    SimdReal SP2brsq_S0, SP2brsq_S2;
+    SimdReal SP3brsq_S0, SP3brsq_S2;
+    SimdReal SP0ewcorr_S0, SP0ewcorr_S2;
+    SimdReal SP1ewcorr_S0, SP1ewcorr_S2;
+    SimdReal SP2ewcorr_S0, SP2ewcorr_S2;
+    SimdReal SP3ewcorr_S0, SP3ewcorr_S2;
+#    endif
+
+    /* frcoul = (1/r - fsub)*r */
+    SimdReal SP0frcoul_S0;
+    SimdReal SP1frcoul_S0;
+    SimdReal SP2frcoul_S0;
+    SimdReal SP3frcoul_S0;
+    SimdReal SP0frcoul_S2;
+    SimdReal SP1frcoul_S2;
+    SimdReal SP2frcoul_S2;
+    SimdReal SP3frcoul_S2;
+#    ifdef CALC_COUL_TAB
+    /* For tables: r, rs=r/sp, rf=floor(rs), frac=rs-rf */
+    SimdReal SP0rs_S0, SP0rf_S0, SP0frac_S0;
+    SimdReal SP1rs_S0, SP1rf_S0, SP1frac_S0;
+    SimdReal SP2rs_S0, SP2rf_S0, SP2frac_S0;
+    SimdReal SP3rs_S0, SP3rf_S0, SP3frac_S0;
+    SimdReal SP0rs_S2, SP0rf_S2, SP0frac_S2;
+    SimdReal SP1rs_S2, SP1rf_S2, SP1frac_S2;
+    SimdReal SP2rs_S2, SP2rf_S2, SP2frac_S2;
+    SimdReal SP3rs_S2, SP3rf_S2, SP3frac_S2;
+    /* Table index: rs truncated to an int */
+    SimdInt32 SP0ti_S0, SP0ti_S2;
+    SimdInt32 SP1ti_S0, SP1ti_S2;
+    SimdInt32 SP2ti_S0, SP2ti_S2;
+    SimdInt32 SP3ti_S0, SP3ti_S2;
+    /* Linear force table values */
+    SimdReal SP0ctab0_S0, SP0ctab1_S0;
+    SimdReal SP1ctab0_S0, SP1ctab1_S0;
+    SimdReal SP2ctab0_S0, SP2ctab1_S0;
+    SimdReal SP3ctab0_S0, SP3ctab1_S0;
+    SimdReal SP0ctab0_S2, SP0ctab1_S2;
+    SimdReal SP1ctab0_S2, SP1ctab1_S2;
+    SimdReal SP2ctab0_S2, SP2ctab1_S2;
+    SimdReal SP3ctab0_S2, SP3ctab1_S2;
+#        ifdef CALC_ENERGIES
+    /* Quadratic energy table value */
+    SimdReal SP0ctabv_S0, SP0dum_S0;
+    SimdReal SP1ctabv_S0, SP1dum_S0;
+    SimdReal SP2ctabv_S0, SP2dum_S0;
+    SimdReal SP3ctabv_S0, SP3dum_S0;
+    SimdReal SP0ctabv_S2, SP0dum_S2;
+    SimdReal SP1ctabv_S2, SP1dum_S2;
+    SimdReal SP2ctabv_S2, SP2dum_S2;
+    SimdReal SP3ctabv_S2, SP3dum_S2;
+#        endif
+#    endif
+#    if defined CALC_ENERGIES && (defined CALC_COUL_EWALD || defined CALC_COUL_TAB)
+    /* The potential (PME mesh) we need to subtract from 1/r */
+    SimdReal SP0vc_sub_S0;
+    SimdReal SP1vc_sub_S0;
+    SimdReal SP2vc_sub_S0;
+    SimdReal SP3vc_sub_S0;
+    SimdReal SP0vc_sub_S2;
+    SimdReal SP1vc_sub_S2;
+    SimdReal SP2vc_sub_S2;
+    SimdReal SP3vc_sub_S2;
+#    endif
+#    ifdef CALC_ENERGIES
+    /* Electrostatic potential */
+    SimdReal SP0vcoul_S0;
+    SimdReal SP1vcoul_S0;
+    SimdReal SP2vcoul_S0;
+    SimdReal SP3vcoul_S0;
+    SimdReal SP0vcoul_S2;
+    SimdReal SP1vcoul_S2;
+    SimdReal SP2vcoul_S2;
+    SimdReal SP3vcoul_S2;
+#    endif
+#endif
+    /* The force times 1/r */
+    SimdReal SP0fscal_S0;
+    SimdReal SP1fscal_S0;
+    SimdReal SP2fscal_S0;
+    SimdReal SP3fscal_S0;
+    SimdReal SP0fscal_S2;
+    SimdReal SP1fscal_S2;
+    SimdReal SP2fscal_S2;
+    SimdReal SP3fscal_S2;
+
+#ifdef CALC_LJ
+#    ifdef LJ_COMB_LB
+    /* LJ sigma_j/2 and sqrt(epsilon_j) */
+    SimdReal SP0hsig_j_S, SP0seps_j_S;
+    SimdReal SP1hsig_j_S, SP1seps_j_S;
+    SimdReal SP2hsig_j_S, SP2seps_j_S;
+    SimdReal SP3hsig_j_S, SP3seps_j_S;
+    /* LJ sigma_ij and epsilon_ij */
+    SimdReal SP0sig_S0, SP0eps_S0;
+    SimdReal SP1sig_S0, SP1eps_S0;
+    SimdReal SP2sig_S0, SP2eps_S0;
+    SimdReal SP3sig_S0, SP3eps_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0sig_S2, SP0eps_S2;
+    SimdReal SP1sig_S2, SP1eps_S2;
+    SimdReal SP2sig_S2, SP2eps_S2;
+    SimdReal SP3sig_S2, SP3eps_S2;
+#        endif
+#        ifdef CALC_ENERGIES
+    SimdReal SP0sig2_S0, SP0sig6_S0;
+    SimdReal SP1sig2_S0, SP1sig6_S0;
+    SimdReal SP2sig2_S0, SP2sig6_S0;
+    SimdReal SP3sig2_S0, SP3sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0sig2_S2, SP0sig6_S2;
+    SimdReal SP1sig2_S2, SP1sig6_S2;
+    SimdReal SP2sig2_S2, SP2sig6_S2;
+    SimdReal SP3sig2_S2, SP3sig6_S2;
+#            endif
+#        endif /* LJ_COMB_LB */
+#    endif     /* CALC_LJ */
+
+#    ifdef LJ_COMB_GEOM
+    SimdReal SP0c6s_j_S, SP0c12s_j_S;
+    SimdReal SP1c6s_j_S, SP1c12s_j_S;
+    SimdReal SP2c6s_j_S, SP2c12s_j_S;
+    SimdReal SP3c6s_j_S, SP3c12s_j_S;
+#    endif
+
+    /* Intermediate variables for LJ calculation */
+#    ifndef LJ_COMB_LB
+    SimdReal SP0rinvsix_S0;
+    SimdReal SP1rinvsix_S0;
+    SimdReal SP2rinvsix_S0;
+    SimdReal SP3rinvsix_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0rinvsix_S2;
+    SimdReal SP1rinvsix_S2;
+    SimdReal SP2rinvsix_S2;
+    SimdReal SP3rinvsix_S2;
+#        endif
+#    endif
+#    ifdef LJ_COMB_LB
+    SimdReal SP0sir_S0, SP0sir2_S0, SP0sir6_S0;
+    SimdReal SP1sir_S0, SP1sir2_S0, SP1sir6_S0;
+    SimdReal SP2sir_S0, SP2sir2_S0, SP2sir6_S0;
+    SimdReal SP3sir_S0, SP3sir2_S0, SP3sir6_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0sir_S2, SP0sir2_S2, SP0sir6_S2;
+    SimdReal SP1sir_S2, SP1sir2_S2, SP1sir6_S2;
+    SimdReal SP2sir_S2, SP2sir2_S2, SP2sir6_S2;
+    SimdReal SP3sir_S2, SP3sir2_S2, SP3sir6_S2;
+#        endif
+#    endif
+
+    SimdReal SP0FrLJ6_S0, SP0FrLJ12_S0, SP0frLJ_S0;
+    SimdReal SP1FrLJ6_S0, SP1FrLJ12_S0, SP1frLJ_S0;
+    SimdReal SP2FrLJ6_S0, SP2FrLJ12_S0, SP2frLJ_S0;
+    SimdReal SP3FrLJ6_S0, SP3FrLJ12_S0, SP3frLJ_S0;
+#    ifndef HALF_LJ
+    SimdReal SP0FrLJ6_S2, SP0FrLJ12_S2, SP0frLJ_S2;
+    SimdReal SP1FrLJ6_S2, SP1FrLJ12_S2, SP1frLJ_S2;
+    SimdReal SP2FrLJ6_S2, SP2FrLJ12_S2, SP2frLJ_S2;
+    SimdReal SP3FrLJ6_S2, SP3FrLJ12_S2, SP3frLJ_S2;
+#    endif
+#endif /* CALC_LJ */
+
+    /* j-cluster index */
+    const int SP0cj = l_cj[cjind+0].cj;
+    const int SP1cj = l_cj[cjind+1].cj;
+    const int SP2cj = l_cj[cjind+2].cj;
+    const int SP3cj = l_cj[cjind+3].cj;
+
+    /* Atom indices (of the first atom in the cluster) */
+    const int SP0aj = SP0cj * UNROLLJ;
+    const int SP1aj = SP1cj * UNROLLJ;
+    const int SP2aj = SP2cj * UNROLLJ;
+    const int SP3aj = SP3cj * UNROLLJ;
+#if defined CALC_LJ && (defined LJ_COMB_GEOM || defined LJ_COMB_LB || defined LJ_EWALD_GEOM)
+    /* Index for loading LJ parameters, complicated when interleaving */
+    const int SP0aj2 = SP0aj * 2;
+    const int SP1aj2 = SP1aj * 2;
+    const int SP2aj2 = SP2aj * 2;
+    const int SP3aj2 = SP3aj * 2;
+#endif
+
+#ifdef CHECK_EXCLS
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+0].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+1].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+2].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+3].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+#endif /* CHECK_EXCLS */
+
+    /* load j atom coordinates */
+    loadDuplicate3Hsimd<STRIDE>(x + SP0aj * DIM, &SP0jx_S, &SP0jy_S, &SP0jz_S);
+    loadDuplicate3Hsimd<STRIDE>(x + SP1aj * DIM, &SP1jx_S, &SP1jy_S, &SP1jz_S);
+    loadDuplicate3Hsimd<STRIDE>(x + SP2aj * DIM, &SP2jx_S, &SP2jy_S, &SP2jz_S);
+    loadDuplicate3Hsimd<STRIDE>(x + SP3aj * DIM, &SP3jx_S, &SP3jy_S, &SP3jz_S);
+
+    /* Calculate distance */
+    SP0dx_S0 = ix_S0 - SP0jx_S;
+    SP1dx_S0 = ix_S0 - SP1jx_S;
+    SP2dx_S0 = ix_S0 - SP2jx_S;
+    SP3dx_S0 = ix_S0 - SP3jx_S;
+    SP0dy_S0 = iy_S0 - SP0jy_S;
+    SP1dy_S0 = iy_S0 - SP1jy_S;
+    SP2dy_S0 = iy_S0 - SP2jy_S;
+    SP3dy_S0 = iy_S0 - SP3jy_S;
+    SP0dz_S0 = iz_S0 - SP0jz_S;
+    SP1dz_S0 = iz_S0 - SP1jz_S;
+    SP2dz_S0 = iz_S0 - SP2jz_S;
+    SP3dz_S0 = iz_S0 - SP3jz_S;
+    SP0dx_S2 = ix_S2 - SP0jx_S;
+    SP1dx_S2 = ix_S2 - SP1jx_S;
+    SP2dx_S2 = ix_S2 - SP2jx_S;
+    SP3dx_S2 = ix_S2 - SP3jx_S;
+    SP0dy_S2 = iy_S2 - SP0jy_S;
+    SP1dy_S2 = iy_S2 - SP1jy_S;
+    SP2dy_S2 = iy_S2 - SP2jy_S;
+    SP3dy_S2 = iy_S2 - SP3jy_S;
+    SP0dz_S2 = iz_S2 - SP0jz_S;
+    SP1dz_S2 = iz_S2 - SP1jz_S;
+    SP2dz_S2 = iz_S2 - SP2jz_S;
+    SP3dz_S2 = iz_S2 - SP3jz_S;
+
+    /* rsq = dx*dx+dy*dy+dz*dz */
+    SP0rsq_S0 = norm2(SP0dx_S0, SP0dy_S0, SP0dz_S0);
+    SP1rsq_S0 = norm2(SP1dx_S0, SP1dy_S0, SP1dz_S0);
+    SP2rsq_S0 = norm2(SP2dx_S0, SP2dy_S0, SP2dz_S0);
+    SP3rsq_S0 = norm2(SP3dx_S0, SP3dy_S0, SP3dz_S0);
+    SP0rsq_S2 = norm2(SP0dx_S2, SP0dy_S2, SP0dz_S2);
+    SP1rsq_S2 = norm2(SP1dx_S2, SP1dy_S2, SP1dz_S2);
+    SP2rsq_S2 = norm2(SP2dx_S2, SP2dy_S2, SP2dz_S2);
+    SP3rsq_S2 = norm2(SP3dx_S2, SP3dy_S2, SP3dz_S2);
+
+    /* Do the cut-off check */
+    SP0wco_S0 = (SP0rsq_S0 < rc2_S);
+    SP1wco_S0 = (SP1rsq_S0 < rc2_S);
+    SP2wco_S0 = (SP2rsq_S0 < rc2_S);
+    SP3wco_S0 = (SP3rsq_S0 < rc2_S);
+    SP0wco_S2 = (SP0rsq_S2 < rc2_S);
+    SP1wco_S2 = (SP1rsq_S2 < rc2_S);
+    SP2wco_S2 = (SP2rsq_S2 < rc2_S);
+    SP3wco_S2 = (SP3rsq_S2 < rc2_S);
+
+#ifdef CHECK_EXCLS
+#    ifdef EXCL_FORCES
+    /* Only remove the (sub-)diagonal to avoid double counting */
+#        if UNROLLJ == UNROLLI
+    if (cj == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask_S0;
+        wco_S2 = wco_S2 && diagonal_mask_S2;
+    }
+#        else
+#            if UNROLLJ == 2 * UNROLLI
+    if (cj * 2 == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask0_S0;
+        wco_S2 = wco_S2 && diagonal_mask0_S2;
+    }
+    else if (cj * 2 + 1 == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask1_S0;
+        wco_S2 = wco_S2 && diagonal_mask1_S2;
+    }
+#            else
+#                error "only UNROLLJ == UNROLLI*(1 or 2) currently supported in 2xnn kernels"
+#            endif
+#        endif
+#    else /* EXCL_FORCES */
+    /* No exclusion forces: remove all excluded atom pairs from the list */
+    wco_S0 = wco_S0 && interact_S0;
+    wco_S2 = wco_S2 && interact_S2;
+#    endif
+#endif
+
+#ifdef COUNT_PAIRS
+    {
+        int                              i, j;
+        alignas(GMX_SIMD_ALIGNMENT) real tmp[GMX_SIMD_REAL_WIDTH];
+
+        for (i = 0; i < UNROLLI; i += 2)
+        {
+            store(tmp, rc2_S - (i == 0 ? rsq_S0 : rsq_S2));
+            for (j = 0; j < 2 * UNROLLJ; j++)
+            {
+                if (tmp[j] >= 0)
+                {
+                    npair++;
+                }
+            }
+        }
+    }
+#endif
+
+    // Ensure the distances do not fall below the limit where r^-12 overflows.
+    // This should never happen for normal interactions.
+    SP0rsq_S0 = max(SP0rsq_S0, minRsq_S);
+    SP1rsq_S0 = max(SP1rsq_S0, minRsq_S);
+    SP2rsq_S0 = max(SP2rsq_S0, minRsq_S);
+    SP3rsq_S0 = max(SP3rsq_S0, minRsq_S);
+    SP0rsq_S2 = max(SP0rsq_S2, minRsq_S);
+    SP1rsq_S2 = max(SP1rsq_S2, minRsq_S);
+    SP2rsq_S2 = max(SP2rsq_S2, minRsq_S);
+    SP3rsq_S2 = max(SP3rsq_S2, minRsq_S);
+
+    /* Calculate 1/r */
+#if SKIP_INVSQRT
+    SP0rinvsq_S0 = invMask(SP0rsq_S0, SP0wco_S0);
+    SP1rinvsq_S0 = invMask(SP1rsq_S0, SP1wco_S0);
+    SP2rinvsq_S0 = invMask(SP2rsq_S0, SP2wco_S0);
+    SP3rinvsq_S0 = invMask(SP3rsq_S0, SP3wco_S0);
+    SP0rinvsq_S2 = invMask(SP0rsq_S2, SP0wco_S2);
+    SP1rinvsq_S2 = invMask(SP1rsq_S2, SP1wco_S2);
+    SP2rinvsq_S2 = invMask(SP2rsq_S2, SP2wco_S2);
+    SP3rinvsq_S2 = invMask(SP3rsq_S2, SP3wco_S2);
+#else
+    /* and set rinv to zero for r beyond the cut-off */
+    SP0rinv_S0 = invsqrtMask(SP0rsq_S0, SP0wco_S0);
+    SP1rinv_S0 = invsqrtMask(SP1rsq_S0, SP1wco_S0);
+    SP2rinv_S0 = invsqrtMask(SP2rsq_S0, SP2wco_S0);
+    SP3rinv_S0 = invsqrtMask(SP3rsq_S0, SP3wco_S0);
+    SP0rinv_S2 = invsqrtMask(SP0rsq_S2, SP0wco_S2);
+    SP1rinv_S2 = invsqrtMask(SP1rsq_S2, SP1wco_S2);
+    SP2rinv_S2 = invsqrtMask(SP2rsq_S2, SP2wco_S2);
+    SP3rinv_S2 = invsqrtMask(SP3rsq_S2, SP3wco_S2);
+#endif
+
+#ifdef CALC_COULOMB
+    /* Load parameters for j atom */
+    SP0jq_S = loadDuplicateHsimd(q + SP0aj);
+    SP1jq_S = loadDuplicateHsimd(q + SP1aj);
+    SP2jq_S = loadDuplicateHsimd(q + SP2aj);
+    SP3jq_S = loadDuplicateHsimd(q + SP3aj);
+    SP0qq_S0 = iq_S0 * SP0jq_S;
+    SP1qq_S0 = iq_S0 * SP1jq_S;
+    SP2qq_S0 = iq_S0 * SP2jq_S;
+    SP3qq_S0 = iq_S0 * SP3jq_S;
+    SP0qq_S2 = iq_S2 * SP0jq_S;
+    SP1qq_S2 = iq_S2 * SP1jq_S;
+    SP2qq_S2 = iq_S2 * SP2jq_S;
+    SP3qq_S2 = iq_S2 * SP3jq_S;
+#endif
+
+#ifdef CALC_LJ
+#    if !defined LJ_COMB_GEOM && !defined LJ_COMB_LB && !defined FIX_LJ_C
+    SimdReal                                                     SP0c6_S0, SP0c12_S0;
+    SimdReal                                                     SP1c6_S0, SP1c12_S0;
+    SimdReal                                                     SP2c6_S0, SP2c12_S0;
+    SimdReal                                                     SP3c6_S0, SP3c12_S0;
+#        ifdef HALF_LJ
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP0aj, &SP0c6_S0, &SP0c12_S0);
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP1aj, &SP1c6_S0, &SP1c12_S0);
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP2aj, &SP2c6_S0, &SP2c12_S0);
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP3aj, &SP3c6_S0, &SP3c12_S0);
+#        else
+    SimdReal SP0c6_S2, SP0c12_S2;
+    SimdReal SP1c6_S2, SP1c12_S2;
+    SimdReal SP2c6_S2, SP2c12_S2;
+    SimdReal SP3c6_S2, SP3c12_S2;
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP0aj, &SP0c6_S0, &SP0c12_S0, &SP0c6_S2, &SP0c12_S2);
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP1aj, &SP1c6_S0, &SP1c12_S0, &SP1c6_S2, &SP1c12_S2);
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP2aj, &SP2c6_S0, &SP2c12_S0, &SP2c6_S2, &SP2c12_S2);
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP3aj, &SP3c6_S0, &SP3c12_S0, &SP3c6_S2, &SP3c12_S2);
+#        endif
+#    endif /* not defined any LJ rule */
+
+#    ifdef LJ_COMB_GEOM
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP0aj2, &SP0c6s_j_S, &SP0c12s_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP1aj2, &SP1c6s_j_S, &SP1c12s_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP2aj2, &SP2c6s_j_S, &SP2c12s_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP3aj2, &SP3c6s_j_S, &SP3c12s_j_S);
+    SimdReal SP0c6_S0 = c6s_S0 * SP0c6s_j_S;
+    SimdReal SP1c6_S0 = c6s_S0 * SP1c6s_j_S;
+    SimdReal SP2c6_S0 = c6s_S0 * SP2c6s_j_S;
+    SimdReal SP3c6_S0 = c6s_S0 * SP3c6s_j_S;
+#        ifndef HALF_LJ
+    SimdReal SP0c6_S2 = c6s_S2 * SP0c6s_j_S;
+    SimdReal SP1c6_S2 = c6s_S2 * SP1c6s_j_S;
+    SimdReal SP2c6_S2 = c6s_S2 * SP2c6s_j_S;
+    SimdReal SP3c6_S2 = c6s_S2 * SP3c6s_j_S;
+#        endif
+    SimdReal SP0c12_S0 = c12s_S0 * SP0c12s_j_S;
+    SimdReal SP1c12_S0 = c12s_S0 * SP1c12s_j_S;
+    SimdReal SP2c12_S0 = c12s_S0 * SP2c12s_j_S;
+    SimdReal SP3c12_S0 = c12s_S0 * SP3c12s_j_S;
+#        ifndef HALF_LJ
+    SimdReal SP0c12_S2 = c12s_S2 * SP0c12s_j_S;
+    SimdReal SP1c12_S2 = c12s_S2 * SP1c12s_j_S;
+    SimdReal SP2c12_S2 = c12s_S2 * SP2c12s_j_S;
+    SimdReal SP3c12_S2 = c12s_S2 * SP3c12s_j_S;
+#        endif
+#    endif /* LJ_COMB_GEOM */
+
+#    ifdef LJ_COMB_LB
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP0aj2, &SP0hsig_j_S, &SP0seps_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP1aj2, &SP1hsig_j_S, &SP1seps_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP2aj2, &SP2hsig_j_S, &SP2seps_j_S);
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP3aj2, &SP3hsig_j_S, &SP3seps_j_S);
+
+    SP0sig_S0 = hsig_i_S0 + SP0hsig_j_S;
+    SP1sig_S0 = hsig_i_S0 + SP1hsig_j_S;
+    SP2sig_S0 = hsig_i_S0 + SP2hsig_j_S;
+    SP3sig_S0 = hsig_i_S0 + SP3hsig_j_S;
+    SP0eps_S0 = seps_i_S0 * SP0seps_j_S;
+    SP1eps_S0 = seps_i_S0 * SP1seps_j_S;
+    SP2eps_S0 = seps_i_S0 * SP2seps_j_S;
+    SP3eps_S0 = seps_i_S0 * SP3seps_j_S;
+#        ifndef HALF_LJ
+    SP0sig_S2 = hsig_i_S2 + SP0hsig_j_S;
+    SP1sig_S2 = hsig_i_S2 + SP1hsig_j_S;
+    SP2sig_S2 = hsig_i_S2 + SP2hsig_j_S;
+    SP3sig_S2 = hsig_i_S2 + SP3hsig_j_S;
+    SP0eps_S2 = seps_i_S2 * SP0seps_j_S;
+    SP1eps_S2 = seps_i_S2 * SP1seps_j_S;
+    SP2eps_S2 = seps_i_S2 * SP2seps_j_S;
+    SP3eps_S2 = seps_i_S2 * SP3seps_j_S;
+#        endif
+#    endif /* LJ_COMB_LB */
+
+#endif /* CALC_LJ */
+
+#if !SKIP_INVSQRT
+    SP0rinvsq_S0 = SP0rinv_S0 * SP0rinv_S0;
+    SP1rinvsq_S0 = SP1rinv_S0 * SP1rinv_S0;
+    SP2rinvsq_S0 = SP2rinv_S0 * SP2rinv_S0;
+    SP3rinvsq_S0 = SP3rinv_S0 * SP3rinv_S0;
+    SP0rinvsq_S2 = SP0rinv_S2 * SP0rinv_S2;
+    SP1rinvsq_S2 = SP1rinv_S2 * SP1rinv_S2;
+    SP2rinvsq_S2 = SP2rinv_S2 * SP2rinv_S2;
+    SP3rinvsq_S2 = SP3rinv_S2 * SP3rinv_S2;
+#endif
+
+#ifdef CALC_COULOMB
+    /* Note that here we calculate force*r, not the usual force/r.
+     * This allows avoiding masking the reaction-field contribution,
+     * as frcoul is later multiplied by rinvsq which has been
+     * masked with the cut-off check.
+     */
+
+#    ifdef EXCL_FORCES
+    /* Only add 1/r for non-excluded atom pairs */
+    rinv_ex_S0 = selectByMask(rinv_S0, interact_S0);
+    rinv_ex_S2 = selectByMask(rinv_S2, interact_S2);
+#    else
+    /* No exclusion forces, we always need 1/r */
+#        define SP0rinv_ex_S0 SP0rinv_S0
+#        define SP1rinv_ex_S0 SP1rinv_S0
+#        define SP2rinv_ex_S0 SP2rinv_S0
+#        define SP3rinv_ex_S0 SP3rinv_S0
+#        define SP0rinv_ex_S2 SP0rinv_S2
+#        define SP1rinv_ex_S2 SP1rinv_S2
+#        define SP2rinv_ex_S2 SP2rinv_S2
+#        define SP3rinv_ex_S2 SP3rinv_S2
+#    endif
+
+#    ifdef CALC_COUL_RF
+    /* Electrostatic interactions */
+    SP0frcoul_S0 = SP0qq_S0 * fma(SP0rsq_S0, mrc_3_S, SP0rinv_ex_S0);
+    SP1frcoul_S0 = SP1qq_S0 * fma(SP1rsq_S0, mrc_3_S, SP1rinv_ex_S0);
+    SP2frcoul_S0 = SP2qq_S0 * fma(SP2rsq_S0, mrc_3_S, SP2rinv_ex_S0);
+    SP3frcoul_S0 = SP3qq_S0 * fma(SP3rsq_S0, mrc_3_S, SP3rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fma(SP0rsq_S2, mrc_3_S, SP0rinv_ex_S2);
+    SP1frcoul_S2 = SP1qq_S2 * fma(SP1rsq_S2, mrc_3_S, SP1rinv_ex_S2);
+    SP2frcoul_S2 = SP2qq_S2 * fma(SP2rsq_S2, mrc_3_S, SP2rinv_ex_S2);
+    SP3frcoul_S2 = SP3qq_S2 * fma(SP3rsq_S2, mrc_3_S, SP3rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    /* and (merge) mask energy for cut-off and diagonal */
+    SP0vcoul_S0 = maskzMul(SP0qq_S0, SP0rinv_ex_S0 + fma(SP0rsq_S0, hrc_3_S, moh_rc_S), SP0wco_S0);
+    SP1vcoul_S0 = maskzMul(SP1qq_S0, SP1rinv_ex_S0 + fma(SP1rsq_S0, hrc_3_S, moh_rc_S), SP1wco_S0);
+    SP2vcoul_S0 = maskzMul(SP2qq_S0, SP2rinv_ex_S0 + fma(SP2rsq_S0, hrc_3_S, moh_rc_S), SP2wco_S0);
+    SP3vcoul_S0 = maskzMul(SP3qq_S0, SP3rinv_ex_S0 + fma(SP3rsq_S0, hrc_3_S, moh_rc_S), SP3wco_S0);
+    SP0vcoul_S2 = maskzMul(SP0qq_S2, SP0rinv_ex_S2 + fma(SP0rsq_S2, hrc_3_S, moh_rc_S), SP0wco_S2);
+    SP1vcoul_S2 = maskzMul(SP1qq_S2, SP1rinv_ex_S2 + fma(SP1rsq_S2, hrc_3_S, moh_rc_S), SP1wco_S2);
+    SP2vcoul_S2 = maskzMul(SP2qq_S2, SP2rinv_ex_S2 + fma(SP2rsq_S2, hrc_3_S, moh_rc_S), SP2wco_S2);
+    SP3vcoul_S2 = maskzMul(SP3qq_S2, SP3rinv_ex_S2 + fma(SP3rsq_S2, hrc_3_S, moh_rc_S), SP3wco_S2);
+#        endif
+#    endif
+
+#    ifdef CALC_COUL_EWALD
+    /* We need to mask (or limit) rsq for the cut-off,
+     * as large distances can cause an overflow in gmx_pmecorrF/V.
+     */
+    SP0brsq_S0   = maskzMul(beta2_S, SP0rsq_S0, SP0wco_S0);
+    SP1brsq_S0   = maskzMul(beta2_S, SP1rsq_S0, SP1wco_S0);
+    SP2brsq_S0   = maskzMul(beta2_S, SP2rsq_S0, SP2wco_S0);
+    SP3brsq_S0   = maskzMul(beta2_S, SP3rsq_S0, SP3wco_S0);
+    SP0brsq_S2   = maskzMul(beta2_S, SP0rsq_S2, SP0wco_S2);
+    SP1brsq_S2   = maskzMul(beta2_S, SP1rsq_S2, SP1wco_S2);
+    SP2brsq_S2   = maskzMul(beta2_S, SP2rsq_S2, SP2wco_S2);
+    SP3brsq_S2   = maskzMul(beta2_S, SP3rsq_S2, SP3wco_S2);
+    SP0ewcorr_S0 = beta_S * pmeForceCorrection(SP0brsq_S0);
+    SP1ewcorr_S0 = beta_S * pmeForceCorrection(SP1brsq_S0);
+    SP2ewcorr_S0 = beta_S * pmeForceCorrection(SP2brsq_S0);
+    SP3ewcorr_S0 = beta_S * pmeForceCorrection(SP3brsq_S0);
+    SP0ewcorr_S2 = beta_S * pmeForceCorrection(SP0brsq_S2);
+    SP1ewcorr_S2 = beta_S * pmeForceCorrection(SP1brsq_S2);
+    SP2ewcorr_S2 = beta_S * pmeForceCorrection(SP2brsq_S2);
+    SP3ewcorr_S2 = beta_S * pmeForceCorrection(SP3brsq_S2);
+    SP0frcoul_S0 = SP0qq_S0 * fma(SP0ewcorr_S0, SP0brsq_S0, SP0rinv_ex_S0);
+    SP1frcoul_S0 = SP1qq_S0 * fma(SP1ewcorr_S0, SP1brsq_S0, SP1rinv_ex_S0);
+    SP2frcoul_S0 = SP2qq_S0 * fma(SP2ewcorr_S0, SP2brsq_S0, SP2rinv_ex_S0);
+    SP3frcoul_S0 = SP3qq_S0 * fma(SP3ewcorr_S0, SP3brsq_S0, SP3rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fma(SP0ewcorr_S2, SP0brsq_S2, SP0rinv_ex_S2);
+    SP1frcoul_S2 = SP1qq_S2 * fma(SP1ewcorr_S2, SP1brsq_S2, SP1rinv_ex_S2);
+    SP2frcoul_S2 = SP2qq_S2 * fma(SP2ewcorr_S2, SP2brsq_S2, SP2rinv_ex_S2);
+    SP3frcoul_S2 = SP3qq_S2 * fma(SP3ewcorr_S2, SP3brsq_S2, SP3rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    SP0vc_sub_S0 = beta_S * pmePotentialCorrection(SP0brsq_S0);
+    SP1vc_sub_S0 = beta_S * pmePotentialCorrection(SP1brsq_S0);
+    SP2vc_sub_S0 = beta_S * pmePotentialCorrection(SP2brsq_S0);
+    SP3vc_sub_S0 = beta_S * pmePotentialCorrection(SP3brsq_S0);
+    SP0vc_sub_S2 = beta_S * pmePotentialCorrection(SP0brsq_S2);
+    SP1vc_sub_S2 = beta_S * pmePotentialCorrection(SP1brsq_S2);
+    SP2vc_sub_S2 = beta_S * pmePotentialCorrection(SP2brsq_S2);
+    SP3vc_sub_S2 = beta_S * pmePotentialCorrection(SP3brsq_S2);
+#        endif
+
+#    endif /* CALC_COUL_EWALD */
+
+#    ifdef CALC_COUL_TAB
+    /* Electrostatic interactions */
+    SP0r_S0 = SP0rsq_S0 * SP0rinv_S0;
+    SP1r_S0 = SP1rsq_S0 * SP1rinv_S0;
+    SP2r_S0 = SP2rsq_S0 * SP2rinv_S0;
+    SP3r_S0 = SP3rsq_S0 * SP3rinv_S0;
+    SP0r_S2 = SP0rsq_S2 * SP0rinv_S2;
+    SP1r_S2 = SP1rsq_S2 * SP1rinv_S2;
+    SP2r_S2 = SP2rsq_S2 * SP2rinv_S2;
+    SP3r_S2 = SP3rsq_S2 * SP3rinv_S2;
+    /* Convert r to scaled table units */
+    SP0rs_S0 = SP0r_S0 * invtsp_S;
+    SP1rs_S0 = SP1r_S0 * invtsp_S;
+    SP2rs_S0 = SP2r_S0 * invtsp_S;
+    SP3rs_S0 = SP3r_S0 * invtsp_S;
+    SP0rs_S2 = SP0r_S2 * invtsp_S;
+    SP1rs_S2 = SP1r_S2 * invtsp_S;
+    SP2rs_S2 = SP2r_S2 * invtsp_S;
+    SP3rs_S2 = SP3r_S2 * invtsp_S;
+    /* Truncate scaled r to an int */
+    SP0ti_S0 = cvttR2I(SP0rs_S0);
+    SP1ti_S0 = cvttR2I(SP1rs_S0);
+    SP2ti_S0 = cvttR2I(SP2rs_S0);
+    SP3ti_S0 = cvttR2I(SP3rs_S0);
+    SP0ti_S2 = cvttR2I(SP0rs_S2);
+    SP1ti_S2 = cvttR2I(SP1rs_S2);
+    SP2ti_S2 = cvttR2I(SP2rs_S2);
+    SP3ti_S2 = cvttR2I(SP3rs_S2);
+
+    SP0rf_S0 = trunc(SP0rs_S0);
+    SP1rf_S0 = trunc(SP1rs_S0);
+    SP2rf_S0 = trunc(SP2rs_S0);
+    SP3rf_S0 = trunc(SP3rs_S0);
+    SP0rf_S2 = trunc(SP0rs_S2);
+    SP1rf_S2 = trunc(SP1rs_S2);
+    SP2rf_S2 = trunc(SP2rs_S2);
+    SP3rf_S2 = trunc(SP3rs_S2);
+
+    SP0frac_S0 = SP0rs_S0 - SP0rf_S0;
+    SP1frac_S0 = SP1rs_S0 - SP1rf_S0;
+    SP2frac_S0 = SP2rs_S0 - SP2rf_S0;
+    SP3frac_S0 = SP3rs_S0 - SP3rf_S0;
+    SP0frac_S2 = SP0rs_S2 - SP0rf_S2;
+    SP1frac_S2 = SP1rs_S2 - SP1rf_S2;
+    SP2frac_S2 = SP2rs_S2 - SP2rf_S2;
+    SP3frac_S2 = SP3rs_S2 - SP3rf_S2;
+
+    /* Load and interpolate table forces and possibly energies.
+     * Force and energy can be combined in one table, stride 4: FDV0
+     * or in two separate tables with stride 1: F and V
+     * Currently single precision uses FDV0, double F and V.
+     */
+#        ifndef CALC_ENERGIES
+#            ifdef TAB_FDV0
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S0, &ctab0_S0, &ctab1_S0);
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S2, &ctab0_S2, &ctab1_S2);
+#            else
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S0, &SP0ctab0_S0, &SP0ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S0, &SP1ctab0_S0, &SP1ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP2ti_S0, &SP2ctab0_S0, &SP2ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP3ti_S0, &SP3ctab0_S0, &SP3ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S2, &SP0ctab0_S2, &SP0ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S2, &SP1ctab0_S2, &SP1ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP2ti_S2, &SP2ctab0_S2, &SP2ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP3ti_S2, &SP3ctab0_S2, &SP3ctab1_S2);
+    SP0ctab1_S0   = SP0ctab1_S0 - SP0ctab0_S0;
+    SP1ctab1_S0   = SP1ctab1_S0 - SP1ctab0_S0;
+    SP2ctab1_S0   = SP2ctab1_S0 - SP2ctab0_S0;
+    SP3ctab1_S0   = SP3ctab1_S0 - SP3ctab0_S0;
+    SP0ctab1_S2   = SP0ctab1_S2 - SP0ctab0_S2;
+    SP1ctab1_S2   = SP1ctab1_S2 - SP1ctab0_S2;
+    SP2ctab1_S2   = SP2ctab1_S2 - SP2ctab0_S2;
+    SP3ctab1_S2   = SP3ctab1_S2 - SP3ctab0_S2;
+#            endif
+#        else
+#            ifdef TAB_FDV0
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S0, &ctab0_S0, &ctab1_S0, &ctabv_S0, &dum_S0);
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S2, &ctab0_S2, &ctab1_S2, &ctabv_S2, &dum_S2);
+#            else
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S0, &SP0ctab0_S0, &SP0ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S0, &SP1ctab0_S0, &SP1ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP2ti_S0, &SP2ctab0_S0, &SP2ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP3ti_S0, &SP3ctab0_S0, &SP3ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP0ti_S0, &SP0ctabv_S0, &SP0dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP1ti_S0, &SP1ctabv_S0, &SP1dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP2ti_S0, &SP2ctabv_S0, &SP2dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP3ti_S0, &SP3ctabv_S0, &SP3dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP0ti_S2, &SP0ctab0_S2, &SP0ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP1ti_S2, &SP1ctab0_S2, &SP1ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP2ti_S2, &SP2ctab0_S2, &SP2ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP3ti_S2, &SP3ctab0_S2, &SP3ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP0ti_S2, &SP0ctabv_S2, &SP0dum_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP1ti_S2, &SP1ctabv_S2, &SP1dum_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP2ti_S2, &SP2ctabv_S2, &SP2dum_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP3ti_S2, &SP3ctabv_S2, &SP3dum_S2);
+    SP0ctab1_S0 = SP0ctab1_S0 - SP0ctab0_S0;
+    SP1ctab1_S0 = SP1ctab1_S0 - SP1ctab0_S0;
+    SP2ctab1_S0 = SP2ctab1_S0 - SP2ctab0_S0;
+    SP3ctab1_S0 = SP3ctab1_S0 - SP3ctab0_S0;
+    SP0ctab1_S2 = SP0ctab1_S2 - SP0ctab0_S2;
+    SP1ctab1_S2 = SP1ctab1_S2 - SP1ctab0_S2;
+    SP2ctab1_S2 = SP2ctab1_S2 - SP2ctab0_S2;
+    SP3ctab1_S2 = SP3ctab1_S2 - SP3ctab0_S2;
+#            endif
+#        endif
+    SP0fsub_S0   = fma(SP0frac_S0, SP0ctab1_S0, SP0ctab0_S0);
+    SP1fsub_S0   = fma(SP1frac_S0, SP1ctab1_S0, SP1ctab0_S0);
+    SP2fsub_S0   = fma(SP2frac_S0, SP2ctab1_S0, SP2ctab0_S0);
+    SP3fsub_S0   = fma(SP3frac_S0, SP3ctab1_S0, SP3ctab0_S0);
+    SP0fsub_S2   = fma(SP0frac_S2, SP0ctab1_S2, SP0ctab0_S2);
+    SP1fsub_S2   = fma(SP1frac_S2, SP1ctab1_S2, SP1ctab0_S2);
+    SP2fsub_S2   = fma(SP2frac_S2, SP2ctab1_S2, SP2ctab0_S2);
+    SP3fsub_S2   = fma(SP3frac_S2, SP3ctab1_S2, SP3ctab0_S2);
+    SP0frcoul_S0 = SP0qq_S0 * fnma(SP0fsub_S0, SP0r_S0, SP0rinv_ex_S0);
+    SP1frcoul_S0 = SP1qq_S0 * fnma(SP1fsub_S0, SP1r_S0, SP1rinv_ex_S0);
+    SP2frcoul_S0 = SP2qq_S0 * fnma(SP2fsub_S0, SP2r_S0, SP2rinv_ex_S0);
+    SP3frcoul_S0 = SP3qq_S0 * fnma(SP3fsub_S0, SP3r_S0, SP3rinv_ex_S0);
+    SP0frcoul_S2 = SP0qq_S2 * fnma(SP0fsub_S2, SP0r_S2, SP0rinv_ex_S2);
+    SP1frcoul_S2 = SP1qq_S2 * fnma(SP1fsub_S2, SP1r_S2, SP1rinv_ex_S2);
+    SP2frcoul_S2 = SP2qq_S2 * fnma(SP2fsub_S2, SP2r_S2, SP2rinv_ex_S2);
+    SP3frcoul_S2 = SP3qq_S2 * fnma(SP3fsub_S2, SP3r_S2, SP3rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    SP0vc_sub_S0 = fma((mhalfsp_S * SP0frac_S0), (SP0ctab0_S0 + SP0fsub_S0), SP0ctabv_S0);
+    SP1vc_sub_S0 = fma((mhalfsp_S * SP1frac_S0), (SP1ctab0_S0 + SP1fsub_S0), SP1ctabv_S0);
+    SP2vc_sub_S0 = fma((mhalfsp_S * SP2frac_S0), (SP2ctab0_S0 + SP2fsub_S0), SP2ctabv_S0);
+    SP3vc_sub_S0 = fma((mhalfsp_S * SP3frac_S0), (SP3ctab0_S0 + SP3fsub_S0), SP3ctabv_S0);
+    SP0vc_sub_S2 = fma((mhalfsp_S * SP0frac_S2), (SP0ctab0_S2 + SP0fsub_S2), SP0ctabv_S2);
+    SP1vc_sub_S2 = fma((mhalfsp_S * SP1frac_S2), (SP1ctab0_S2 + SP1fsub_S2), SP1ctabv_S2);
+    SP2vc_sub_S2 = fma((mhalfsp_S * SP2frac_S2), (SP2ctab0_S2 + SP2fsub_S2), SP2ctabv_S2);
+    SP3vc_sub_S2 = fma((mhalfsp_S * SP3frac_S2), (SP3ctab0_S2 + SP3fsub_S2), SP3ctabv_S2);
+#        endif
+#    endif /* CALC_COUL_TAB */
+
+#    if defined CALC_ENERGIES && (defined CALC_COUL_EWALD || defined CALC_COUL_TAB)
+#        ifndef NO_SHIFT_EWALD
+    /* Add Ewald potential shift to vc_sub for convenience */
+#            ifdef CHECK_EXCLS
+    vc_sub_S0 = maskAdd(vc_sub_S0, sh_ewald_S, interact_S0);
+    vc_sub_S2 = maskAdd(vc_sub_S2, sh_ewald_S, interact_S2);
+#            else
+    SP0vc_sub_S0  = SP0vc_sub_S0 + sh_ewald_S;
+    SP1vc_sub_S0  = SP1vc_sub_S0 + sh_ewald_S;
+    SP2vc_sub_S0  = SP2vc_sub_S0 + sh_ewald_S;
+    SP3vc_sub_S0  = SP3vc_sub_S0 + sh_ewald_S;
+    SP0vc_sub_S2  = SP0vc_sub_S2 + sh_ewald_S;
+    SP1vc_sub_S2  = SP1vc_sub_S2 + sh_ewald_S;
+    SP2vc_sub_S2  = SP2vc_sub_S2 + sh_ewald_S;
+    SP3vc_sub_S2  = SP3vc_sub_S2 + sh_ewald_S;
+#            endif
+#        endif
+
+    /* and (merge) mask energy for cut-off and diagonal */
+    SP0vcoul_S0 = maskzMul(SP0qq_S0, SP0rinv_ex_S0 - SP0vc_sub_S0, SP0wco_S0);
+    SP1vcoul_S0 = maskzMul(SP1qq_S0, SP1rinv_ex_S0 - SP1vc_sub_S0, SP1wco_S0);
+    SP2vcoul_S0 = maskzMul(SP2qq_S0, SP2rinv_ex_S0 - SP2vc_sub_S0, SP2wco_S0);
+    SP3vcoul_S0 = maskzMul(SP3qq_S0, SP3rinv_ex_S0 - SP3vc_sub_S0, SP3wco_S0);
+    SP0vcoul_S2 = maskzMul(SP0qq_S2, SP0rinv_ex_S2 - SP0vc_sub_S2, SP0wco_S2);
+    SP1vcoul_S2 = maskzMul(SP1qq_S2, SP1rinv_ex_S2 - SP1vc_sub_S2, SP1wco_S2);
+    SP2vcoul_S2 = maskzMul(SP2qq_S2, SP2rinv_ex_S2 - SP2vc_sub_S2, SP2wco_S2);
+    SP3vcoul_S2 = maskzMul(SP3qq_S2, SP3rinv_ex_S2 - SP3vc_sub_S2, SP3wco_S2);
+
+#    endif
+
+#endif /* CALC_COULOMB */
+
+#ifdef CALC_LJ
+    /* Lennard-Jones interaction */
+
+#    ifdef VDW_CUTOFF_CHECK
+    SP0wco_vdw_S0 = (SP0rsq_S0 < rcvdw2_S);
+    SP1wco_vdw_S0 = (SP1rsq_S0 < rcvdw2_S);
+    SP2wco_vdw_S0 = (SP2rsq_S0 < rcvdw2_S);
+    SP3wco_vdw_S0 = (SP3rsq_S0 < rcvdw2_S);
+#        ifndef HALF_LJ
+    SP0wco_vdw_S2 = (SP0rsq_S2 < rcvdw2_S);
+    SP1wco_vdw_S2 = (SP1rsq_S2 < rcvdw2_S);
+    SP2wco_vdw_S2 = (SP2rsq_S2 < rcvdw2_S);
+    SP3wco_vdw_S2 = (SP3rsq_S2 < rcvdw2_S);
+#        endif
+#    else
+    /* Same cut-off for Coulomb and VdW, reuse the registers */
+#        define SP0wco_vdw_S0 SP0wco_S0
+#        define SP1wco_vdw_S0 SP1wco_S0
+#        define SP2wco_vdw_S0 SP2wco_S0
+#        define SP3wco_vdw_S0 SP3wco_S0
+#        define SP0wco_vdw_S2 SP0wco_S2
+#        define SP1wco_vdw_S2 SP1wco_S2
+#        define SP2wco_vdw_S2 SP2wco_S2
+#        define SP3wco_vdw_S2 SP3wco_S2
+#    endif
+
+#    ifndef LJ_COMB_LB
+    SP0rinvsix_S0 = SP0rinvsq_S0 * SP0rinvsq_S0;
+    SP1rinvsix_S0 = SP1rinvsq_S0 * SP1rinvsq_S0;
+    SP2rinvsix_S0 = SP2rinvsq_S0 * SP2rinvsq_S0;
+    SP3rinvsix_S0 = SP3rinvsq_S0 * SP3rinvsq_S0;
+#        ifdef EXCL_FORCES
+    rinvsix_S0 = maskzMul(rinvsix_S0, rinvsq_S0, interact_S0);
+#        else
+    SP0rinvsix_S0 = SP0rinvsix_S0 * SP0rinvsq_S0;
+    SP1rinvsix_S0 = SP1rinvsix_S0 * SP1rinvsq_S0;
+    SP2rinvsix_S0 = SP2rinvsix_S0 * SP2rinvsq_S0;
+    SP3rinvsix_S0 = SP3rinvsix_S0 * SP3rinvsq_S0;
+#        endif
+#        ifndef HALF_LJ
+    SP0rinvsix_S2 = SP0rinvsq_S2 * SP0rinvsq_S2;
+    SP1rinvsix_S2 = SP1rinvsq_S2 * SP1rinvsq_S2;
+    SP2rinvsix_S2 = SP2rinvsq_S2 * SP2rinvsq_S2;
+    SP3rinvsix_S2 = SP3rinvsq_S2 * SP3rinvsq_S2;
+#            ifdef EXCL_FORCES
+    rinvsix_S2 = maskzMul(rinvsix_S2, rinvsq_S2, interact_S2);
+#            else
+    SP0rinvsix_S2 = SP0rinvsix_S2 * SP0rinvsq_S2;
+    SP1rinvsix_S2 = SP1rinvsix_S2 * SP1rinvsq_S2;
+    SP2rinvsix_S2 = SP2rinvsix_S2 * SP2rinvsq_S2;
+    SP3rinvsix_S2 = SP3rinvsix_S2 * SP3rinvsq_S2;
+#            endif
+#        endif
+
+#        if defined LJ_CUT || defined LJ_POT_SWITCH
+    /* We have plain LJ or LJ-PME with simple C6/6 C12/12 coefficients */
+    SP0FrLJ6_S0 = SP0c6_S0 * SP0rinvsix_S0;
+    SP1FrLJ6_S0 = SP1c6_S0 * SP1rinvsix_S0;
+    SP2FrLJ6_S0 = SP2c6_S0 * SP2rinvsix_S0;
+    SP3FrLJ6_S0 = SP3c6_S0 * SP3rinvsix_S0;
+#            ifndef HALF_LJ
+    SP0FrLJ6_S2 = SP0c6_S2 * SP0rinvsix_S2;
+    SP1FrLJ6_S2 = SP1c6_S2 * SP1rinvsix_S2;
+    SP2FrLJ6_S2 = SP2c6_S2 * SP2rinvsix_S2;
+    SP3FrLJ6_S2 = SP3c6_S2 * SP3rinvsix_S2;
+#            endif
+    SP0FrLJ12_S0 = SP0c12_S0 * SP0rinvsix_S0 * SP0rinvsix_S0;
+    SP1FrLJ12_S0 = SP1c12_S0 * SP1rinvsix_S0 * SP1rinvsix_S0;
+    SP2FrLJ12_S0 = SP2c12_S0 * SP2rinvsix_S0 * SP2rinvsix_S0;
+    SP3FrLJ12_S0 = SP3c12_S0 * SP3rinvsix_S0 * SP3rinvsix_S0;
+#            ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0c12_S2 * SP0rinvsix_S2 * SP0rinvsix_S2;
+    SP1FrLJ12_S2 = SP1c12_S2 * SP1rinvsix_S2 * SP1rinvsix_S2;
+    SP2FrLJ12_S2 = SP2c12_S2 * SP2rinvsix_S2 * SP2rinvsix_S2;
+    SP3FrLJ12_S2 = SP3c12_S2 * SP3rinvsix_S2 * SP3rinvsix_S2;
+#            endif
+#        endif
+
+#        if defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    /* We switch the LJ force */
+    SP0r_S0    = SP0rsq_S0 * SP0rinv_S0;
+    SP1r_S0    = SP1rsq_S0 * SP1rinv_S0;
+    SP2r_S0    = SP2rsq_S0 * SP2rinv_S0;
+    SP3r_S0    = SP3rsq_S0 * SP3rinv_S0;
+    SP0rsw_S0  = max(SP0r_S0 - rswitch_S, zero_S);
+    SP1rsw_S0  = max(SP1r_S0 - rswitch_S, zero_S);
+    SP2rsw_S0  = max(SP2r_S0 - rswitch_S, zero_S);
+    SP3rsw_S0  = max(SP3r_S0 - rswitch_S, zero_S);
+    SP0rsw2_S0 = SP0rsw_S0 * SP0rsw_S0;
+    SP1rsw2_S0 = SP1rsw_S0 * SP1rsw_S0;
+    SP2rsw2_S0 = SP2rsw_S0 * SP2rsw_S0;
+    SP3rsw2_S0 = SP3rsw_S0 * SP3rsw_S0;
+#            ifndef HALF_LJ
+    SP0r_S2    = SP0rsq_S2 * SP0rinv_S2;
+    SP1r_S2    = SP1rsq_S2 * SP1rinv_S2;
+    SP2r_S2    = SP2rsq_S2 * SP2rinv_S2;
+    SP3r_S2    = SP3rsq_S2 * SP3rinv_S2;
+    SP0rsw_S2  = max(SP0r_S2 - rswitch_S, zero_S);
+    SP1rsw_S2  = max(SP1r_S2 - rswitch_S, zero_S);
+    SP2rsw_S2  = max(SP2r_S2 - rswitch_S, zero_S);
+    SP3rsw_S2  = max(SP3r_S2 - rswitch_S, zero_S);
+    SP0rsw2_S2 = SP0rsw_S2 * SP0rsw_S2;
+    SP1rsw2_S2 = SP1rsw_S2 * SP1rsw_S2;
+    SP2rsw2_S2 = SP2rsw_S2 * SP2rsw_S2;
+    SP3rsw2_S2 = SP3rsw_S2 * SP3rsw_S2;
+#            endif
+#        endif
+
+#        ifdef LJ_FORCE_SWITCH
+
+#            define add_fr_switch(fr, rsw, rsw2_r, c2, c3) fma(fma(c3, rsw, c2), rsw2_r, fr)
+    SimdReal SP0rsw2_r_S0 = SP0rsw2_S0 * SP0r_S0;
+    SimdReal SP1rsw2_r_S0 = SP1rsw2_S0 * SP1r_S0;
+    SimdReal SP2rsw2_r_S0 = SP2rsw2_S0 * SP2r_S0;
+    SimdReal SP3rsw2_r_S0 = SP3rsw2_S0 * SP3r_S0;
+    SP0FrLJ6_S0           = SP0c6_S0 * add_fr_switch(SP0rinvsix_S0, SP0rsw_S0, SP0rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+    SP1FrLJ6_S0           = SP1c6_S0 * add_fr_switch(SP1rinvsix_S0, SP1rsw_S0, SP1rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+    SP2FrLJ6_S0           = SP2c6_S0 * add_fr_switch(SP2rinvsix_S0, SP2rsw_S0, SP2rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+    SP3FrLJ6_S0           = SP3c6_S0 * add_fr_switch(SP3rinvsix_S0, SP3rsw_S0, SP3rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+#            ifndef HALF_LJ
+    SimdReal SP0rsw2_r_S2 = SP0rsw2_S2 * SP0r_S2;
+    SimdReal SP1rsw2_r_S2 = SP1rsw2_S2 * SP1r_S2;
+    SimdReal SP2rsw2_r_S2 = SP2rsw2_S2 * SP2r_S2;
+    SimdReal SP3rsw2_r_S2 = SP3rsw2_S2 * SP3r_S2;
+    SP0FrLJ6_S2           = SP0c6_S2 * add_fr_switch(SP0rinvsix_S2, SP0rsw_S2, SP0rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+    SP1FrLJ6_S2           = SP1c6_S2 * add_fr_switch(SP1rinvsix_S2, SP1rsw_S2, SP1rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+    SP2FrLJ6_S2           = SP2c6_S2 * add_fr_switch(SP2rinvsix_S2, SP2rsw_S2, SP2rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+    SP3FrLJ6_S2           = SP3c6_S2 * add_fr_switch(SP3rinvsix_S2, SP3rsw_S2, SP3rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+#            endif
+    SP0FrLJ12_S0 = SP0c12_S0 * add_fr_switch(SP0rinvsix_S0 * SP0rinvsix_S0, SP0rsw_S0, SP0rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+    SP1FrLJ12_S0 = SP1c12_S0 * add_fr_switch(SP1rinvsix_S0 * SP1rinvsix_S0, SP1rsw_S0, SP1rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+    SP2FrLJ12_S0 = SP2c12_S0 * add_fr_switch(SP2rinvsix_S0 * SP2rinvsix_S0, SP2rsw_S0, SP2rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+    SP3FrLJ12_S0 = SP3c12_S0 * add_fr_switch(SP3rinvsix_S0 * SP3rinvsix_S0, SP3rsw_S0, SP3rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+#            ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0c12_S2 * add_fr_switch(SP0rinvsix_S2 * SP0rinvsix_S2, SP0rsw_S2, SP0rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+    SP1FrLJ12_S2 = SP1c12_S2 * add_fr_switch(SP1rinvsix_S2 * SP1rinvsix_S2, SP1rsw_S2, SP1rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+    SP2FrLJ12_S2 = SP2c12_S2 * add_fr_switch(SP2rinvsix_S2 * SP2rinvsix_S2, SP2rsw_S2, SP2rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+    SP3FrLJ12_S2 = SP3c12_S2 * add_fr_switch(SP3rinvsix_S2 * SP3rinvsix_S2, SP3rsw_S2, SP3rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+#            endif
+#            undef add_fr_switch
+#        endif /* LJ_FORCE_SWITCH */
+
+#    endif /* not LJ_COMB_LB */
+
+#    ifdef LJ_COMB_LB
+    SP0sir_S0 = SP0sig_S0 * SP0rinv_S0;
+    SP1sir_S0 = SP1sig_S0 * SP1rinv_S0;
+    SP2sir_S0 = SP2sig_S0 * SP2rinv_S0;
+    SP3sir_S0 = SP3sig_S0 * SP3rinv_S0;
+#        ifndef HALF_LJ
+    SP0sir_S2 = SP0sig_S2 * SP0rinv_S2;
+    SP1sir_S2 = SP1sig_S2 * SP1rinv_S2;
+    SP2sir_S2 = SP2sig_S2 * SP2rinv_S2;
+    SP3sir_S2 = SP3sig_S2 * SP3rinv_S2;
+#        endif
+    SP0sir2_S0 = SP0sir_S0 * SP0sir_S0;
+    SP1sir2_S0 = SP1sir_S0 * SP1sir_S0;
+    SP2sir2_S0 = SP2sir_S0 * SP2sir_S0;
+    SP3sir2_S0 = SP3sir_S0 * SP3sir_S0;
+#        ifndef HALF_LJ
+    SP0sir2_S2 = SP0sir_S2 * SP0sir_S2;
+    SP1sir2_S2 = SP1sir_S2 * SP1sir_S2;
+    SP2sir2_S2 = SP2sir_S2 * SP2sir_S2;
+    SP3sir2_S2 = SP3sir_S2 * SP3sir_S2;
+#        endif
+#        ifdef VDW_CUTOFF_CHECK
+    SP0sir6_S0 = maskzMul(SP0sir2_S0, SP0sir2_S0, SP0wco_vdw_S0);
+    SP1sir6_S0 = maskzMul(SP1sir2_S0, SP1sir2_S0, SP1wco_vdw_S0);
+    SP2sir6_S0 = maskzMul(SP2sir2_S0, SP2sir2_S0, SP2wco_vdw_S0);
+    SP3sir6_S0 = maskzMul(SP3sir2_S0, SP3sir2_S0, SP3wco_vdw_S0);
+#        else
+    SP0sir6_S0    = SP0sir2_S0 * SP0sir2_S0;
+    SP1sir6_S0    = SP1sir2_S0 * SP1sir2_S0;
+    SP2sir6_S0    = SP2sir2_S0 * SP2sir2_S0;
+    SP3sir6_S0    = SP3sir2_S0 * SP3sir2_S0;
+#        endif
+#        ifdef EXCL_FORCES
+    sir6_S0 = maskzMul(sir6_S0, sir2_S0, interact_S0);
+#        else
+    SP0sir6_S0    = SP0sir6_S0 * SP0sir2_S0;
+    SP1sir6_S0    = SP1sir6_S0 * SP1sir2_S0;
+    SP2sir6_S0    = SP2sir6_S0 * SP2sir2_S0;
+    SP3sir6_S0    = SP3sir6_S0 * SP3sir2_S0;
+#        endif
+#        ifndef HALF_LJ
+#            ifdef VDW_CUTOFF_CHECK
+    SP0sir6_S2 = maskzMul(SP0sir2_S2, SP0sir2_S2, SP0wco_vdw_S2);
+    SP1sir6_S2 = maskzMul(SP1sir2_S2, SP1sir2_S2, SP1wco_vdw_S2);
+    SP2sir6_S2 = maskzMul(SP2sir2_S2, SP2sir2_S2, SP2wco_vdw_S2);
+    SP3sir6_S2 = maskzMul(SP3sir2_S2, SP3sir2_S2, SP3wco_vdw_S2);
+#            else
+    SP0sir6_S2    = SP0sir2_S2 * SP0sir2_S2;
+    SP1sir6_S2    = SP1sir2_S2 * SP1sir2_S2;
+    SP2sir6_S2    = SP2sir2_S2 * SP2sir2_S2;
+    SP3sir6_S2    = SP3sir2_S2 * SP3sir2_S2;
+#            endif
+#            ifdef EXCL_FORCES
+    sir6_S2 = maskzMul(sir6_S2, sir2_S2, interact_S2);
+#            else
+    SP0sir6_S2    = SP0sir6_S2 * SP0sir2_S2;
+    SP1sir6_S2    = SP1sir6_S2 * SP1sir2_S2;
+    SP2sir6_S2    = SP2sir6_S2 * SP2sir2_S2;
+    SP3sir6_S2    = SP3sir6_S2 * SP3sir2_S2;
+#            endif
+#        endif
+    SP0FrLJ6_S0 = SP0eps_S0 * SP0sir6_S0;
+    SP1FrLJ6_S0 = SP1eps_S0 * SP1sir6_S0;
+    SP2FrLJ6_S0 = SP2eps_S0 * SP2sir6_S0;
+    SP3FrLJ6_S0 = SP3eps_S0 * SP3sir6_S0;
+#        ifndef HALF_LJ
+    SP0FrLJ6_S2 = SP0eps_S2 * SP0sir6_S2;
+    SP1FrLJ6_S2 = SP1eps_S2 * SP1sir6_S2;
+    SP2FrLJ6_S2 = SP2eps_S2 * SP2sir6_S2;
+    SP3FrLJ6_S2 = SP3eps_S2 * SP3sir6_S2;
+#        endif
+    SP0FrLJ12_S0 = SP0FrLJ6_S0 * SP0sir6_S0;
+    SP1FrLJ12_S0 = SP1FrLJ6_S0 * SP1sir6_S0;
+    SP2FrLJ12_S0 = SP2FrLJ6_S0 * SP2sir6_S0;
+    SP3FrLJ12_S0 = SP3FrLJ6_S0 * SP3sir6_S0;
+#        ifndef HALF_LJ
+    SP0FrLJ12_S2 = SP0FrLJ6_S2 * SP0sir6_S2;
+    SP1FrLJ12_S2 = SP1FrLJ6_S2 * SP1sir6_S2;
+    SP2FrLJ12_S2 = SP2FrLJ6_S2 * SP2sir6_S2;
+    SP3FrLJ12_S2 = SP3FrLJ6_S2 * SP3sir6_S2;
+#        endif
+#        if defined CALC_ENERGIES
+    /* We need C6 and C12 to calculate the LJ potential shift */
+    SP0sig2_S0 = SP0sig_S0 * SP0sig_S0;
+    SP1sig2_S0 = SP1sig_S0 * SP1sig_S0;
+    SP2sig2_S0 = SP2sig_S0 * SP2sig_S0;
+    SP3sig2_S0 = SP3sig_S0 * SP3sig_S0;
+#            ifndef HALF_LJ
+    SP0sig2_S2 = SP0sig_S2 * SP0sig_S2;
+    SP1sig2_S2 = SP1sig_S2 * SP1sig_S2;
+    SP2sig2_S2 = SP2sig_S2 * SP2sig_S2;
+    SP3sig2_S2 = SP3sig_S2 * SP3sig_S2;
+#            endif
+    SP0sig6_S0 = SP0sig2_S0 * SP0sig2_S0 * SP0sig2_S0;
+    SP1sig6_S0 = SP1sig2_S0 * SP1sig2_S0 * SP1sig2_S0;
+    SP2sig6_S0 = SP2sig2_S0 * SP2sig2_S0 * SP2sig2_S0;
+    SP3sig6_S0 = SP3sig2_S0 * SP3sig2_S0 * SP3sig2_S0;
+#            ifndef HALF_LJ
+    SP0sig6_S2 = SP0sig2_S2 * SP0sig2_S2 * SP0sig2_S2;
+    SP1sig6_S2 = SP1sig2_S2 * SP1sig2_S2 * SP1sig2_S2;
+    SP2sig6_S2 = SP2sig2_S2 * SP2sig2_S2 * SP2sig2_S2;
+    SP3sig6_S2 = SP3sig2_S2 * SP3sig2_S2 * SP3sig2_S2;
+#            endif
+    SimdReal SP0c6_S0 = SP0eps_S0 * SP0sig6_S0;
+    SimdReal SP1c6_S0 = SP1eps_S0 * SP1sig6_S0;
+    SimdReal SP2c6_S0 = SP2eps_S0 * SP2sig6_S0;
+    SimdReal SP3c6_S0 = SP3eps_S0 * SP3sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0c6_S2 = SP0eps_S2 * SP0sig6_S2;
+    SimdReal SP1c6_S2 = SP1eps_S2 * SP1sig6_S2;
+    SimdReal SP2c6_S2 = SP2eps_S2 * SP2sig6_S2;
+    SimdReal SP3c6_S2 = SP3eps_S2 * SP3sig6_S2;
+#            endif
+    SimdReal SP0c12_S0 = SP0c6_S0 * SP0sig6_S0;
+    SimdReal SP1c12_S0 = SP1c6_S0 * SP1sig6_S0;
+    SimdReal SP2c12_S0 = SP2c6_S0 * SP2sig6_S0;
+    SimdReal SP3c12_S0 = SP3c6_S0 * SP3sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP0c12_S2 = SP0c6_S2 * SP0sig6_S2;
+    SimdReal SP1c12_S2 = SP1c6_S2 * SP1sig6_S2;
+    SimdReal SP2c12_S2 = SP2c6_S2 * SP2sig6_S2;
+    SimdReal SP3c12_S2 = SP3c6_S2 * SP3sig6_S2;
+#            endif
+#        endif
+#    endif /* LJ_COMB_LB */
+
+    /* Determine the total scalar LJ force*r */
+    SP0frLJ_S0 = SP0FrLJ12_S0 - SP0FrLJ6_S0;
+    SP1frLJ_S0 = SP1FrLJ12_S0 - SP1FrLJ6_S0;
+    SP2frLJ_S0 = SP2FrLJ12_S0 - SP2FrLJ6_S0;
+    SP3frLJ_S0 = SP3FrLJ12_S0 - SP3FrLJ6_S0;
+#    ifndef HALF_LJ
+    SP0frLJ_S2 = SP0FrLJ12_S2 - SP0FrLJ6_S2;
+    SP1frLJ_S2 = SP1FrLJ12_S2 - SP1FrLJ6_S2;
+    SP2frLJ_S2 = SP2FrLJ12_S2 - SP2FrLJ6_S2;
+    SP3frLJ_S2 = SP3FrLJ12_S2 - SP3FrLJ6_S2;
+#    endif
+
+#    if (defined LJ_CUT || defined LJ_FORCE_SWITCH) && defined CALC_ENERGIES
+
+#        ifdef LJ_CUT
+    /* Calculate the LJ energies, with constant potential shift */
+    SimdReal SP0VLJ6_S0 = sixth_S * fma(SP0c6_S0, p6_cpot_S, SP0FrLJ6_S0);
+    SimdReal SP1VLJ6_S0 = sixth_S * fma(SP1c6_S0, p6_cpot_S, SP1FrLJ6_S0);
+    SimdReal SP2VLJ6_S0 = sixth_S * fma(SP2c6_S0, p6_cpot_S, SP2FrLJ6_S0);
+    SimdReal SP3VLJ6_S0 = sixth_S * fma(SP3c6_S0, p6_cpot_S, SP3FrLJ6_S0);
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ6_S2 = sixth_S * fma(SP0c6_S2, p6_cpot_S, SP0FrLJ6_S2);
+    SimdReal SP1VLJ6_S2 = sixth_S * fma(SP1c6_S2, p6_cpot_S, SP1FrLJ6_S2);
+    SimdReal SP2VLJ6_S2 = sixth_S * fma(SP2c6_S2, p6_cpot_S, SP2FrLJ6_S2);
+    SimdReal SP3VLJ6_S2 = sixth_S * fma(SP3c6_S2, p6_cpot_S, SP3FrLJ6_S2);
+#            endif
+    SimdReal SP0VLJ12_S0 = twelveth_S * fma(SP0c12_S0, p12_cpot_S, SP0FrLJ12_S0);
+    SimdReal SP1VLJ12_S0 = twelveth_S * fma(SP1c12_S0, p12_cpot_S, SP1FrLJ12_S0);
+    SimdReal SP2VLJ12_S0 = twelveth_S * fma(SP2c12_S0, p12_cpot_S, SP2FrLJ12_S0);
+    SimdReal SP3VLJ12_S0 = twelveth_S * fma(SP3c12_S0, p12_cpot_S, SP3FrLJ12_S0);
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ12_S2 = twelveth_S * fma(SP0c12_S2, p12_cpot_S, SP0FrLJ12_S2);
+    SimdReal SP1VLJ12_S2 = twelveth_S * fma(SP1c12_S2, p12_cpot_S, SP1FrLJ12_S2);
+    SimdReal SP2VLJ12_S2 = twelveth_S * fma(SP2c12_S2, p12_cpot_S, SP2FrLJ12_S2);
+    SimdReal SP3VLJ12_S2 = twelveth_S * fma(SP3c12_S2, p12_cpot_S, SP3FrLJ12_S2);
+#            endif
+#        endif /* LJ_CUT */
+
+#        ifdef LJ_FORCE_SWITCH
+#            define v_fswitch_pr(rsw, rsw2, c0, c3, c4) fma(fma(c4, rsw, c3), (rsw2) * (rsw), c0)
+
+    SimdReal SP0VLJ6_S0 = SP0c6_S0 * fma(sixth_S, SP0rinvsix_S0, v_fswitch_pr(SP0rsw_S0, SP0rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP1VLJ6_S0 = SP1c6_S0 * fma(sixth_S, SP1rinvsix_S0, v_fswitch_pr(SP1rsw_S0, SP1rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP2VLJ6_S0 = SP2c6_S0 * fma(sixth_S, SP2rinvsix_S0, v_fswitch_pr(SP2rsw_S0, SP2rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP3VLJ6_S0 = SP3c6_S0 * fma(sixth_S, SP3rinvsix_S0, v_fswitch_pr(SP3rsw_S0, SP3rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ6_S2 = SP0c6_S2 * fma(sixth_S, SP0rinvsix_S2, v_fswitch_pr(SP0rsw_S2, SP0rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP1VLJ6_S2 = SP1c6_S2 * fma(sixth_S, SP1rinvsix_S2, v_fswitch_pr(SP1rsw_S2, SP1rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP2VLJ6_S2 = SP2c6_S2 * fma(sixth_S, SP2rinvsix_S2, v_fswitch_pr(SP2rsw_S2, SP2rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+    SimdReal SP3VLJ6_S2 = SP3c6_S2 * fma(sixth_S, SP3rinvsix_S2, v_fswitch_pr(SP3rsw_S2, SP3rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+#            endif
+    SimdReal SP0VLJ12_S0 = SP0c12_S0 * fma(twelveth_S, SP0rinvsix_S0 * SP0rinvsix_S0, v_fswitch_pr(SP0rsw_S0, SP0rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP1VLJ12_S0 = SP1c12_S0 * fma(twelveth_S, SP1rinvsix_S0 * SP1rinvsix_S0, v_fswitch_pr(SP1rsw_S0, SP1rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP2VLJ12_S0 = SP2c12_S0 * fma(twelveth_S, SP2rinvsix_S0 * SP2rinvsix_S0, v_fswitch_pr(SP2rsw_S0, SP2rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP3VLJ12_S0 = SP3c12_S0 * fma(twelveth_S, SP3rinvsix_S0 * SP3rinvsix_S0, v_fswitch_pr(SP3rsw_S0, SP3rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+#            ifndef HALF_LJ
+    SimdReal SP0VLJ12_S2 = SP0c12_S2 * fma(twelveth_S, SP0rinvsix_S2 * SP0rinvsix_S2, v_fswitch_pr(SP0rsw_S2, SP0rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP1VLJ12_S2 = SP1c12_S2 * fma(twelveth_S, SP1rinvsix_S2 * SP1rinvsix_S2, v_fswitch_pr(SP1rsw_S2, SP1rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP2VLJ12_S2 = SP2c12_S2 * fma(twelveth_S, SP2rinvsix_S2 * SP2rinvsix_S2, v_fswitch_pr(SP2rsw_S2, SP2rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+    SimdReal SP3VLJ12_S2 = SP3c12_S2 * fma(twelveth_S, SP3rinvsix_S2 * SP3rinvsix_S2, v_fswitch_pr(SP3rsw_S2, SP3rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+#            endif
+#            undef v_fswitch_pr
+#        endif /* LJ_FORCE_SWITCH */
+
+    /* Add up the repulsion and dispersion */
+    SimdReal SP0VLJ_S0 = SP0VLJ12_S0 - SP0VLJ6_S0;
+    SimdReal SP1VLJ_S0 = SP1VLJ12_S0 - SP1VLJ6_S0;
+    SimdReal SP2VLJ_S0 = SP2VLJ12_S0 - SP2VLJ6_S0;
+    SimdReal SP3VLJ_S0 = SP3VLJ12_S0 - SP3VLJ6_S0;
+#        ifndef HALF_LJ
+    SimdReal SP0VLJ_S2 = SP0VLJ12_S2 - SP0VLJ6_S2;
+    SimdReal SP1VLJ_S2 = SP1VLJ12_S2 - SP1VLJ6_S2;
+    SimdReal SP2VLJ_S2 = SP2VLJ12_S2 - SP2VLJ6_S2;
+    SimdReal SP3VLJ_S2 = SP3VLJ12_S2 - SP3VLJ6_S2;
+#        endif
+
+#    endif /* (LJ_CUT || LJ_FORCE_SWITCH) && CALC_ENERGIES */
+
+#    ifdef LJ_POT_SWITCH
+    /* We always need the potential, since it is needed for the force */
+    SimdReal SP0VLJ_S0 = fnma(sixth_S, SP0FrLJ6_S0, twelveth_S * SP0FrLJ12_S0);
+    SimdReal SP1VLJ_S0 = fnma(sixth_S, SP1FrLJ6_S0, twelveth_S * SP1FrLJ12_S0);
+    SimdReal SP2VLJ_S0 = fnma(sixth_S, SP2FrLJ6_S0, twelveth_S * SP2FrLJ12_S0);
+    SimdReal SP3VLJ_S0 = fnma(sixth_S, SP3FrLJ6_S0, twelveth_S * SP3FrLJ12_S0);
+#        ifndef HALF_LJ
+    SimdReal SP0VLJ_S2 = fnma(sixth_S, SP0FrLJ6_S2, twelveth_S * SP0FrLJ12_S2);
+    SimdReal SP1VLJ_S2 = fnma(sixth_S, SP1FrLJ6_S2, twelveth_S * SP1FrLJ12_S2);
+    SimdReal SP2VLJ_S2 = fnma(sixth_S, SP2FrLJ6_S2, twelveth_S * SP2FrLJ12_S2);
+    SimdReal SP3VLJ_S2 = fnma(sixth_S, SP3FrLJ6_S2, twelveth_S * SP3FrLJ12_S2);
+#        endif
+
+    {
+        SimdReal SP0sw_S0, SP0dsw_S0;
+        SimdReal SP1sw_S0, SP1dsw_S0;
+        SimdReal SP2sw_S0, SP2dsw_S0;
+        SimdReal SP3sw_S0, SP3dsw_S0;
+#        ifndef HALF_LJ
+        SimdReal SP0sw_S2, SP0dsw_S2;
+        SimdReal SP1sw_S2, SP1dsw_S2;
+        SimdReal SP2sw_S2, SP2dsw_S2;
+        SimdReal SP3sw_S2, SP3dsw_S2;
+#        endif
+
+#        define switch_pr(rsw, rsw2, c3, c4, c5) \
+            fma(fma(fma(c5, rsw, c4), rsw, c3), (rsw2) * (rsw), one_S)
+#        define dswitch_pr(rsw, rsw2, c2, c3, c4) fma(fma(c4, rsw, c3), rsw, c2) * (rsw2)
+
+        SP0sw_S0  = switch_pr(SP0rsw_S0, SP0rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP1sw_S0  = switch_pr(SP1rsw_S0, SP1rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP2sw_S0  = switch_pr(SP2rsw_S0, SP2rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP3sw_S0  = switch_pr(SP3rsw_S0, SP3rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP0dsw_S0 = dswitch_pr(SP0rsw_S0, SP0rsw2_S0, swF2_S, swF3_S, swF4_S);
+        SP1dsw_S0 = dswitch_pr(SP1rsw_S0, SP1rsw2_S0, swF2_S, swF3_S, swF4_S);
+        SP2dsw_S0 = dswitch_pr(SP2rsw_S0, SP2rsw2_S0, swF2_S, swF3_S, swF4_S);
+        SP3dsw_S0 = dswitch_pr(SP3rsw_S0, SP3rsw2_S0, swF2_S, swF3_S, swF4_S);
+#        ifndef HALF_LJ
+        SP0sw_S2  = switch_pr(SP0rsw_S2, SP0rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP1sw_S2  = switch_pr(SP1rsw_S2, SP1rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP2sw_S2  = switch_pr(SP2rsw_S2, SP2rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP3sw_S2  = switch_pr(SP3rsw_S2, SP3rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP0dsw_S2 = dswitch_pr(SP0rsw_S2, SP0rsw2_S2, swF2_S, swF3_S, swF4_S);
+        SP1dsw_S2 = dswitch_pr(SP1rsw_S2, SP1rsw2_S2, swF2_S, swF3_S, swF4_S);
+        SP2dsw_S2 = dswitch_pr(SP2rsw_S2, SP2rsw2_S2, swF2_S, swF3_S, swF4_S);
+        SP3dsw_S2 = dswitch_pr(SP3rsw_S2, SP3rsw2_S2, swF2_S, swF3_S, swF4_S);
+#        endif
+        SP0frLJ_S0 = fnma(SP0dsw_S0 * SP0VLJ_S0, SP0r_S0, SP0sw_S0 * SP0frLJ_S0);
+        SP1frLJ_S0 = fnma(SP1dsw_S0 * SP1VLJ_S0, SP1r_S0, SP1sw_S0 * SP1frLJ_S0);
+        SP2frLJ_S0 = fnma(SP2dsw_S0 * SP2VLJ_S0, SP2r_S0, SP2sw_S0 * SP2frLJ_S0);
+        SP3frLJ_S0 = fnma(SP3dsw_S0 * SP3VLJ_S0, SP3r_S0, SP3sw_S0 * SP3frLJ_S0);
+#        ifndef HALF_LJ
+        SP0frLJ_S2 = fnma(SP0dsw_S2 * SP0VLJ_S2, SP0r_S2, SP0sw_S2 * SP0frLJ_S2);
+        SP1frLJ_S2 = fnma(SP1dsw_S2 * SP1VLJ_S2, SP1r_S2, SP1sw_S2 * SP1frLJ_S2);
+        SP2frLJ_S2 = fnma(SP2dsw_S2 * SP2VLJ_S2, SP2r_S2, SP2sw_S2 * SP2frLJ_S2);
+        SP3frLJ_S2 = fnma(SP3dsw_S2 * SP3VLJ_S2, SP3r_S2, SP3sw_S2 * SP3frLJ_S2);
+#        endif
+#        ifdef CALC_ENERGIES
+        SP0VLJ_S0 = SP0sw_S0 * SP0VLJ_S0;
+        SP1VLJ_S0 = SP1sw_S0 * SP1VLJ_S0;
+        SP2VLJ_S0 = SP2sw_S0 * SP2VLJ_S0;
+        SP3VLJ_S0 = SP3sw_S0 * SP3VLJ_S0;
+#            ifndef HALF_LJ
+        SP0VLJ_S2 = SP0sw_S2 * SP0VLJ_S2;
+        SP1VLJ_S2 = SP1sw_S2 * SP1VLJ_S2;
+        SP2VLJ_S2 = SP2sw_S2 * SP2VLJ_S2;
+        SP3VLJ_S2 = SP3sw_S2 * SP3VLJ_S2;
+#            endif
+#        endif
+
+#        undef switch_pr
+#        undef dswitch_pr
+    }
+#    endif /* LJ_POT_SWITCH */
+
+#    if defined CALC_ENERGIES && defined CHECK_EXCLS
+    /* The potential shift should be removed for excluded pairs */
+    VLJ_S0 = selectByMask(VLJ_S0, interact_S0);
+#        ifndef HALF_LJ
+    VLJ_S2 = selectByMask(VLJ_S2, interact_S2);
+#        endif
+#    endif
+
+#    ifdef LJ_EWALD_GEOM
+    {
+        SimdReal SP0c6s_j_S;
+        SimdReal SP1c6s_j_S;
+        SimdReal SP2c6s_j_S;
+        SimdReal SP3c6s_j_S;
+        SimdReal SP0c6grid_S0, SP0rinvsix_nm_S0, SP0cr2_S0, SP0expmcr2_S0, SP0poly_S0;
+        SimdReal SP1c6grid_S0, SP1rinvsix_nm_S0, SP1cr2_S0, SP1expmcr2_S0, SP1poly_S0;
+        SimdReal SP2c6grid_S0, SP2rinvsix_nm_S0, SP2cr2_S0, SP2expmcr2_S0, SP2poly_S0;
+        SimdReal SP3c6grid_S0, SP3rinvsix_nm_S0, SP3cr2_S0, SP3expmcr2_S0, SP3poly_S0;
+#        ifndef HALF_LJ
+        SimdReal SP0c6grid_S2, SP0rinvsix_nm_S2, SP0cr2_S2, SP0expmcr2_S2, SP0poly_S2;
+        SimdReal SP1c6grid_S2, SP1rinvsix_nm_S2, SP1cr2_S2, SP1expmcr2_S2, SP1poly_S2;
+        SimdReal SP2c6grid_S2, SP2rinvsix_nm_S2, SP2cr2_S2, SP2expmcr2_S2, SP2poly_S2;
+        SimdReal SP3c6grid_S2, SP3rinvsix_nm_S2, SP3cr2_S2, SP3expmcr2_S2, SP3poly_S2;
+#        endif
+#        ifdef CALC_ENERGIES
+        SimdReal SP0sh_mask_S0;
+        SimdReal SP1sh_mask_S0;
+        SimdReal SP2sh_mask_S0;
+        SimdReal SP3sh_mask_S0;
+#            ifndef HALF_LJ
+        SimdReal SP0sh_mask_S2;
+        SimdReal SP1sh_mask_S2;
+        SimdReal SP2sh_mask_S2;
+        SimdReal SP3sh_mask_S2;
+#            endif
+#        endif
+
+        /* Determine C6 for the grid using the geometric combination rule */
+        SP0c6s_j_S   = loadDuplicateHsimd(ljc + SP0aj2);
+        SP1c6s_j_S   = loadDuplicateHsimd(ljc + SP1aj2);
+        SP2c6s_j_S   = loadDuplicateHsimd(ljc + SP2aj2);
+        SP3c6s_j_S   = loadDuplicateHsimd(ljc + SP3aj2);
+        SP0c6grid_S0 = c6s_S0 * SP0c6s_j_S;
+        SP1c6grid_S0 = c6s_S0 * SP1c6s_j_S;
+        SP2c6grid_S0 = c6s_S0 * SP2c6s_j_S;
+        SP3c6grid_S0 = c6s_S0 * SP3c6s_j_S;
+#        ifndef HALF_LJ
+        SP0c6grid_S2 = c6s_S2 * SP0c6s_j_S;
+        SP1c6grid_S2 = c6s_S2 * SP1c6s_j_S;
+        SP2c6grid_S2 = c6s_S2 * SP2c6s_j_S;
+        SP3c6grid_S2 = c6s_S2 * SP3c6s_j_S;
+#        endif
+
+#        ifdef CHECK_EXCLS
+        /* Recalculate rinvsix without exclusion mask (compiler might optimize) */
+        rinvsix_nm_S0 = rinvsq_S0 * rinvsq_S0 * rinvsq_S0;
+#            ifndef HALF_LJ
+        rinvsix_nm_S2 = rinvsq_S2 * rinvsq_S2 * rinvsq_S2;
+#            endif
+#        else
+        /* We didn't use a mask, so we can copy */
+        SP0rinvsix_nm_S0 = SP0rinvsix_S0;
+        SP1rinvsix_nm_S0 = SP1rinvsix_S0;
+        SP2rinvsix_nm_S0 = SP2rinvsix_S0;
+        SP3rinvsix_nm_S0 = SP3rinvsix_S0;
+#            ifndef HALF_LJ
+        SP0rinvsix_nm_S2 = SP0rinvsix_S2;
+        SP1rinvsix_nm_S2 = SP1rinvsix_S2;
+        SP2rinvsix_nm_S2 = SP2rinvsix_S2;
+        SP3rinvsix_nm_S2 = SP3rinvsix_S2;
+#            endif
+#        endif
+
+        /* Mask for the cut-off to avoid overflow of cr2^2 */
+        SP0cr2_S0 = maskzMul(lje_c2_S, SP0rsq_S0, SP0wco_vdw_S0);
+        SP1cr2_S0 = maskzMul(lje_c2_S, SP1rsq_S0, SP1wco_vdw_S0);
+        SP2cr2_S0 = maskzMul(lje_c2_S, SP2rsq_S0, SP2wco_vdw_S0);
+        SP3cr2_S0 = maskzMul(lje_c2_S, SP3rsq_S0, SP3wco_vdw_S0);
+#        ifndef HALF_LJ
+        SP0cr2_S2 = maskzMul(lje_c2_S, SP0rsq_S2, SP0wco_vdw_S2);
+        SP1cr2_S2 = maskzMul(lje_c2_S, SP1rsq_S2, SP1wco_vdw_S2);
+        SP2cr2_S2 = maskzMul(lje_c2_S, SP2rsq_S2, SP2wco_vdw_S2);
+        SP3cr2_S2 = maskzMul(lje_c2_S, SP3rsq_S2, SP3wco_vdw_S2);
+#        endif
+        // Unsafe version of our exp() should be fine, since these arguments should never
+        // be smaller than -127 for any reasonable choice of cutoff or ewald coefficients.
+        SP0expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP0cr2_S0);
+        SP1expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP1cr2_S0);
+        SP2expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP2cr2_S0);
+        SP3expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP3cr2_S0);
+#        ifndef HALF_LJ
+        SP0expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP0cr2_S2);
+        SP1expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP1cr2_S2);
+        SP2expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP2cr2_S2);
+        SP3expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP3cr2_S2);
+#        endif
+
+        /* 1 + cr2 + 1/2*cr2^2 */
+        SP0poly_S0 = fma(fma(half_S, SP0cr2_S0, one_S), SP0cr2_S0, one_S);
+        SP1poly_S0 = fma(fma(half_S, SP1cr2_S0, one_S), SP1cr2_S0, one_S);
+        SP2poly_S0 = fma(fma(half_S, SP2cr2_S0, one_S), SP2cr2_S0, one_S);
+        SP3poly_S0 = fma(fma(half_S, SP3cr2_S0, one_S), SP3cr2_S0, one_S);
+#        ifndef HALF_LJ
+        SP0poly_S2 = fma(fma(half_S, SP0cr2_S2, one_S), SP0cr2_S2, one_S);
+        SP1poly_S2 = fma(fma(half_S, SP1cr2_S2, one_S), SP1cr2_S2, one_S);
+        SP2poly_S2 = fma(fma(half_S, SP2cr2_S2, one_S), SP2cr2_S2, one_S);
+        SP3poly_S2 = fma(fma(half_S, SP3cr2_S2, one_S), SP3cr2_S2, one_S);
+#        endif
+
+        /* We calculate LJ F*r = (6*C6)*(r^-6 - F_mesh/6), we use:
+         * r^-6*cexp*(1 + cr2 + cr2^2/2 + cr2^3/6) = cexp*(r^-6*poly + c^6/6)
+         */
+        SP0frLJ_S0 = fma(SP0c6grid_S0, fnma(SP0expmcr2_S0, fma(SP0rinvsix_nm_S0, SP0poly_S0, lje_c6_6_S), SP0rinvsix_nm_S0), SP0frLJ_S0);
+        SP1frLJ_S0 = fma(SP1c6grid_S0, fnma(SP1expmcr2_S0, fma(SP1rinvsix_nm_S0, SP1poly_S0, lje_c6_6_S), SP1rinvsix_nm_S0), SP1frLJ_S0);
+        SP2frLJ_S0 = fma(SP2c6grid_S0, fnma(SP2expmcr2_S0, fma(SP2rinvsix_nm_S0, SP2poly_S0, lje_c6_6_S), SP2rinvsix_nm_S0), SP2frLJ_S0);
+        SP3frLJ_S0 = fma(SP3c6grid_S0, fnma(SP3expmcr2_S0, fma(SP3rinvsix_nm_S0, SP3poly_S0, lje_c6_6_S), SP3rinvsix_nm_S0), SP3frLJ_S0);
+#        ifndef HALF_LJ
+        SP0frLJ_S2 = fma(SP0c6grid_S2, fnma(SP0expmcr2_S2, fma(SP0rinvsix_nm_S2, SP0poly_S2, lje_c6_6_S), SP0rinvsix_nm_S2), SP0frLJ_S2);
+        SP1frLJ_S2 = fma(SP1c6grid_S2, fnma(SP1expmcr2_S2, fma(SP1rinvsix_nm_S2, SP1poly_S2, lje_c6_6_S), SP1rinvsix_nm_S2), SP1frLJ_S2);
+        SP2frLJ_S2 = fma(SP2c6grid_S2, fnma(SP2expmcr2_S2, fma(SP2rinvsix_nm_S2, SP2poly_S2, lje_c6_6_S), SP2rinvsix_nm_S2), SP2frLJ_S2);
+        SP3frLJ_S2 = fma(SP3c6grid_S2, fnma(SP3expmcr2_S2, fma(SP3rinvsix_nm_S2, SP3poly_S2, lje_c6_6_S), SP3rinvsix_nm_S2), SP3frLJ_S2);
+#        endif
+
+#        ifdef CALC_ENERGIES
+#            ifdef CHECK_EXCLS
+        sh_mask_S0 = selectByMask(lje_vc_S, interact_S0);
+#                ifndef HALF_LJ
+        sh_mask_S2 = selectByMask(lje_vc_S, interact_S2);
+#                endif
+#            else
+        SP0sh_mask_S0 = lje_vc_S;
+        SP1sh_mask_S0 = lje_vc_S;
+        SP2sh_mask_S0 = lje_vc_S;
+        SP3sh_mask_S0 = lje_vc_S;
+#                ifndef HALF_LJ
+        SP0sh_mask_S2 = lje_vc_S;
+        SP1sh_mask_S2 = lje_vc_S;
+        SP2sh_mask_S2 = lje_vc_S;
+        SP3sh_mask_S2 = lje_vc_S;
+#                endif
+#            endif
+
+        SP0VLJ_S0 = fma(sixth_S * SP0c6grid_S0, fma(SP0rinvsix_nm_S0, fnma(SP0expmcr2_S0, SP0poly_S0, one_S), SP0sh_mask_S0), SP0VLJ_S0);
+        SP1VLJ_S0 = fma(sixth_S * SP1c6grid_S0, fma(SP1rinvsix_nm_S0, fnma(SP1expmcr2_S0, SP1poly_S0, one_S), SP1sh_mask_S0), SP1VLJ_S0);
+        SP2VLJ_S0 = fma(sixth_S * SP2c6grid_S0, fma(SP2rinvsix_nm_S0, fnma(SP2expmcr2_S0, SP2poly_S0, one_S), SP2sh_mask_S0), SP2VLJ_S0);
+        SP3VLJ_S0 = fma(sixth_S * SP3c6grid_S0, fma(SP3rinvsix_nm_S0, fnma(SP3expmcr2_S0, SP3poly_S0, one_S), SP3sh_mask_S0), SP3VLJ_S0);
+#            ifndef HALF_LJ
+        SP0VLJ_S2 = fma(sixth_S * SP0c6grid_S2, fma(SP0rinvsix_nm_S2, fnma(SP0expmcr2_S2, SP0poly_S2, one_S), SP0sh_mask_S2), SP0VLJ_S2);
+        SP1VLJ_S2 = fma(sixth_S * SP1c6grid_S2, fma(SP1rinvsix_nm_S2, fnma(SP1expmcr2_S2, SP1poly_S2, one_S), SP1sh_mask_S2), SP1VLJ_S2);
+        SP2VLJ_S2 = fma(sixth_S * SP2c6grid_S2, fma(SP2rinvsix_nm_S2, fnma(SP2expmcr2_S2, SP2poly_S2, one_S), SP2sh_mask_S2), SP2VLJ_S2);
+        SP3VLJ_S2 = fma(sixth_S * SP3c6grid_S2, fma(SP3rinvsix_nm_S2, fnma(SP3expmcr2_S2, SP3poly_S2, one_S), SP3sh_mask_S2), SP3VLJ_S2);
+#            endif
+#        endif /* CALC_ENERGIES */
+    }
+#    endif /* LJ_EWALD_GEOM */
+
+#    if defined VDW_CUTOFF_CHECK
+    /* frLJ is multiplied later by rinvsq, which is masked for the Coulomb
+     * cut-off, but if the VdW cut-off is shorter, we need to mask with that.
+     */
+    SP0frLJ_S0 = selectByMask(SP0frLJ_S0, SP0wco_vdw_S0);
+    SP1frLJ_S0 = selectByMask(SP1frLJ_S0, SP1wco_vdw_S0);
+    SP2frLJ_S0 = selectByMask(SP2frLJ_S0, SP2wco_vdw_S0);
+    SP3frLJ_S0 = selectByMask(SP3frLJ_S0, SP3wco_vdw_S0);
+#        ifndef HALF_LJ
+    SP0frLJ_S2 = selectByMask(SP0frLJ_S2, SP0wco_vdw_S2);
+    SP1frLJ_S2 = selectByMask(SP1frLJ_S2, SP1wco_vdw_S2);
+    SP2frLJ_S2 = selectByMask(SP2frLJ_S2, SP2wco_vdw_S2);
+    SP3frLJ_S2 = selectByMask(SP3frLJ_S2, SP3wco_vdw_S2);
+#        endif
+#    endif
+
+#    ifdef CALC_ENERGIES
+    /* The potential shift should be removed for pairs beyond cut-off */
+    SP0VLJ_S0 = selectByMask(SP0VLJ_S0, SP0wco_vdw_S0);
+    SP1VLJ_S0 = selectByMask(SP1VLJ_S0, SP1wco_vdw_S0);
+    SP2VLJ_S0 = selectByMask(SP2VLJ_S0, SP2wco_vdw_S0);
+    SP3VLJ_S0 = selectByMask(SP3VLJ_S0, SP3wco_vdw_S0);
+#        ifndef HALF_LJ
+    SP0VLJ_S2 = selectByMask(SP0VLJ_S2, SP0wco_vdw_S2);
+    SP1VLJ_S2 = selectByMask(SP1VLJ_S2, SP1wco_vdw_S2);
+    SP2VLJ_S2 = selectByMask(SP2VLJ_S2, SP2wco_vdw_S2);
+    SP3VLJ_S2 = selectByMask(SP3VLJ_S2, SP3wco_vdw_S2);
+#        endif
+#    endif
+
+#endif /* CALC_LJ */
+
+#ifdef CALC_ENERGIES
+#    ifdef ENERGY_GROUPS
+    /* Extract the group pair index per j pair.
+     * Energy groups are stored per i-cluster, so things get
+     * complicated when the i- and j-cluster size don't match.
+     */
+    {
+#        if UNROLLJ == 2
+#error
+        const int egps_j    = nbatParams.energrp[cj >> 1];
+        egp_jj[0]       = ((egps_j >> ((cj & 1) * egps_jshift)) & egps_jmask) * egps_jstride;
+#        else
+        /* We assume UNROLLI <= UNROLLJ */
+        for (int jdi = 0; jdi < UNROLLJ / UNROLLI; jdi++)
+        {
+            const int SP0egps_j = nbatParams.energrp[SP0cj * (UNROLLJ / UNROLLI) + jdi];
+            const int SP1egps_j = nbatParams.energrp[SP1cj * (UNROLLJ / UNROLLI) + jdi];
+            const int SP2egps_j = nbatParams.energrp[SP2cj * (UNROLLJ / UNROLLI) + jdi];
+            const int SP3egps_j = nbatParams.energrp[SP3cj * (UNROLLJ / UNROLLI) + jdi];
+            for (int jj = 0; jj < (UNROLLI / 2); jj++)
+            {
+                SP0egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP0egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+                SP1egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP1egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+                SP2egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP2egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+                SP3egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP3egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+            }
+        }
+#        endif
+    }
+#    endif
+
+#    ifdef CALC_COULOMB
+#        ifndef ENERGY_GROUPS
+    vctot_S = vctot_S + SP0vcoul_S0 + SP0vcoul_S2;
+    vctot_S = vctot_S + SP1vcoul_S0 + SP1vcoul_S2;
+    vctot_S = vctot_S + SP2vcoul_S0 + SP2vcoul_S2;
+    vctot_S = vctot_S + SP3vcoul_S0 + SP3vcoul_S2;
+#        else
+    add_ener_grp_halves(SP0vcoul_S0, vctp[0], vctp[1], SP0egp_jj);
+    add_ener_grp_halves(SP1vcoul_S0, vctp[0], vctp[1], SP1egp_jj);
+    add_ener_grp_halves(SP2vcoul_S0, vctp[0], vctp[1], SP2egp_jj);
+    add_ener_grp_halves(SP3vcoul_S0, vctp[0], vctp[1], SP3egp_jj);
+    add_ener_grp_halves(SP0vcoul_S2, vctp[2], vctp[3], SP0egp_jj);
+    add_ener_grp_halves(SP1vcoul_S2, vctp[2], vctp[3], SP1egp_jj);
+    add_ener_grp_halves(SP2vcoul_S2, vctp[2], vctp[3], SP2egp_jj);
+    add_ener_grp_halves(SP3vcoul_S2, vctp[2], vctp[3], SP3egp_jj);
+#        endif
+#    endif
+
+#    ifdef CALC_LJ
+#        ifndef ENERGY_GROUPS
+    Vvdwtot_S = Vvdwtot_S
+                + SP0VLJ_S0
+                + SP1VLJ_S0
+                + SP2VLJ_S0
+                + SP3VLJ_S0
+#            ifndef HALF_LJ
+                + SP0VLJ_S2
+                + SP1VLJ_S2
+                + SP2VLJ_S2
+                + SP3VLJ_S2
+#            endif
+            ;
+#        else
+    add_ener_grp_halves(SP0VLJ_S0, vvdwtp[0], vvdwtp[1], SP0egp_jj);
+    add_ener_grp_halves(SP1VLJ_S0, vvdwtp[0], vvdwtp[1], SP1egp_jj);
+    add_ener_grp_halves(SP2VLJ_S0, vvdwtp[0], vvdwtp[1], SP2egp_jj);
+    add_ener_grp_halves(SP3VLJ_S0, vvdwtp[0], vvdwtp[1], SP3egp_jj);
+#            ifndef HALF_LJ
+    add_ener_grp_halves(SP0VLJ_S2, vvdwtp[2], vvdwtp[3], SP0egp_jj);
+    add_ener_grp_halves(SP1VLJ_S2, vvdwtp[2], vvdwtp[3], SP1egp_jj);
+    add_ener_grp_halves(SP2VLJ_S2, vvdwtp[2], vvdwtp[3], SP2egp_jj);
+    add_ener_grp_halves(SP3VLJ_S2, vvdwtp[2], vvdwtp[3], SP3egp_jj);
+#            endif
+#        endif
+#    endif /* CALC_LJ */
+#endif     /* CALC_ENERGIES */
+
+#ifdef CALC_LJ
+#    ifdef CALC_COULOMB
+    SP0fscal_S0 = SP0rinvsq_S0 * (SP0frcoul_S0 + SP0frLJ_S0);
+    SP1fscal_S0 = SP1rinvsq_S0 * (SP1frcoul_S0 + SP1frLJ_S0);
+    SP2fscal_S0 = SP2rinvsq_S0 * (SP2frcoul_S0 + SP2frLJ_S0);
+    SP3fscal_S0 = SP3rinvsq_S0 * (SP3frcoul_S0 + SP3frLJ_S0);
+#    else
+    SP0fscal_S0 = SP0rinvsq_S0 * SP0frLJ_S0;
+    SP1fscal_S0 = SP1rinvsq_S0 * SP1frLJ_S0;
+    SP2fscal_S0 = SP2rinvsq_S0 * SP2frLJ_S0;
+    SP3fscal_S0 = SP3rinvsq_S0 * SP3frLJ_S0;
+#    endif
+#else
+    fscal_S0 = rinvsq_S0 * frcoul_S0;
+#endif /* CALC_LJ */
+#if defined CALC_LJ && !defined HALF_LJ
+#    ifdef CALC_COULOMB
+    SP0fscal_S2 = SP0rinvsq_S2 * (SP0frcoul_S2 + SP0frLJ_S2);
+    SP1fscal_S2 = SP1rinvsq_S2 * (SP1frcoul_S2 + SP1frLJ_S2);
+    SP2fscal_S2 = SP2rinvsq_S2 * (SP2frcoul_S2 + SP2frLJ_S2);
+    SP3fscal_S2 = SP3rinvsq_S2 * (SP3frcoul_S2 + SP3frLJ_S2);
+#    else
+    SP0fscal_S2 = SP0rinvsq_S2 * SP0frLJ_S2;
+    SP1fscal_S2 = SP1rinvsq_S2 * SP1frLJ_S2;
+    SP2fscal_S2 = SP2rinvsq_S2 * SP2frLJ_S2;
+    SP3fscal_S2 = SP3rinvsq_S2 * SP3frLJ_S2;
+#    endif
+#else
+    /* Atom 2 and 3 don't have LJ, so only add Coulomb forces */
+    SP0fscal_S2 = SP0rinvsq_S2 * SP0frcoul_S2;
+    SP1fscal_S2 = SP1rinvsq_S2 * SP1frcoul_S2;
+    SP2fscal_S2 = SP2rinvsq_S2 * SP2frcoul_S2;
+    SP3fscal_S2 = SP3rinvsq_S2 * SP3frcoul_S2;
+#endif
+
+    /* Calculate temporary vectorial force */
+    SP0tx_S0 = SP0fscal_S0 * SP0dx_S0;
+    SP1tx_S0 = SP1fscal_S0 * SP1dx_S0;
+    SP2tx_S0 = SP2fscal_S0 * SP2dx_S0;
+    SP3tx_S0 = SP3fscal_S0 * SP3dx_S0;
+    SP0tx_S2 = SP0fscal_S2 * SP0dx_S2;
+    SP1tx_S2 = SP1fscal_S2 * SP1dx_S2;
+    SP2tx_S2 = SP2fscal_S2 * SP2dx_S2;
+    SP3tx_S2 = SP3fscal_S2 * SP3dx_S2;
+    SP0ty_S0 = SP0fscal_S0 * SP0dy_S0;
+    SP1ty_S0 = SP1fscal_S0 * SP1dy_S0;
+    SP2ty_S0 = SP2fscal_S0 * SP2dy_S0;
+    SP3ty_S0 = SP3fscal_S0 * SP3dy_S0;
+    SP0ty_S2 = SP0fscal_S2 * SP0dy_S2;
+    SP1ty_S2 = SP1fscal_S2 * SP1dy_S2;
+    SP2ty_S2 = SP2fscal_S2 * SP2dy_S2;
+    SP3ty_S2 = SP3fscal_S2 * SP3dy_S2;
+    SP0tz_S0 = SP0fscal_S0 * SP0dz_S0;
+    SP1tz_S0 = SP1fscal_S0 * SP1dz_S0;
+    SP2tz_S0 = SP2fscal_S0 * SP2dz_S0;
+    SP3tz_S0 = SP3fscal_S0 * SP3dz_S0;
+    SP0tz_S2 = SP0fscal_S2 * SP0dz_S2;
+    SP1tz_S2 = SP1fscal_S2 * SP1dz_S2;
+    SP2tz_S2 = SP2fscal_S2 * SP2dz_S2;
+    SP3tz_S2 = SP3fscal_S2 * SP3dz_S2;
+
+    /* Increment i atom force */
+    fix_S0 = fix_S0 + SP0tx_S0;
+    fix_S0 = fix_S0 + SP1tx_S0;
+    fix_S0 = fix_S0 + SP2tx_S0;
+    fix_S0 = fix_S0 + SP3tx_S0;
+    fix_S2 = fix_S2 + SP0tx_S2;
+    fix_S2 = fix_S2 + SP1tx_S2;
+    fix_S2 = fix_S2 + SP2tx_S2;
+    fix_S2 = fix_S2 + SP3tx_S2;
+    fiy_S0 = fiy_S0 + SP0ty_S0;
+    fiy_S0 = fiy_S0 + SP1ty_S0;
+    fiy_S0 = fiy_S0 + SP2ty_S0;
+    fiy_S0 = fiy_S0 + SP3ty_S0;
+    fiy_S2 = fiy_S2 + SP0ty_S2;
+    fiy_S2 = fiy_S2 + SP1ty_S2;
+    fiy_S2 = fiy_S2 + SP2ty_S2;
+    fiy_S2 = fiy_S2 + SP3ty_S2;
+    fiz_S0 = fiz_S0 + SP0tz_S0;
+    fiz_S0 = fiz_S0 + SP1tz_S0;
+    fiz_S0 = fiz_S0 + SP2tz_S0;
+    fiz_S0 = fiz_S0 + SP3tz_S0;
+    fiz_S2 = fiz_S2 + SP0tz_S2;
+    fiz_S2 = fiz_S2 + SP1tz_S2;
+    fiz_S2 = fiz_S2 + SP2tz_S2;
+    fiz_S2 = fiz_S2 + SP3tz_S2;
+
+    /* Decrement j atom force */
+    decr3Hsimd(f + SP0aj * DIM, SP0tx_S0 + SP0tx_S2, SP0ty_S0 + SP0ty_S2, SP0tz_S0 + SP0tz_S2);
+    decr3Hsimd(f + SP1aj * DIM, SP1tx_S0 + SP1tx_S2, SP1ty_S0 + SP1ty_S2, SP1tz_S0 + SP1tz_S2);
+    decr3Hsimd(f + SP2aj * DIM, SP2tx_S0 + SP2tx_S2, SP2ty_S0 + SP2ty_S2, SP2tz_S0 + SP2tz_S2);
+    decr3Hsimd(f + SP3aj * DIM, SP3tx_S0 + SP3tx_S2, SP3ty_S0 + SP3ty_S2, SP3tz_S0 + SP3tz_S2);
+
+#undef SP0rinv_ex_S0
+#undef SP1rinv_ex_S0
+#undef SP2rinv_ex_S0
+#undef SP3rinv_ex_S0
+#undef SP0rinv_ex_S2
+#undef SP1rinv_ex_S2
+#undef SP2rinv_ex_S2
+#undef SP3rinv_ex_S2
+
+#undef SP0wco_vdw_S0
+#undef SP1wco_vdw_S0
+#undef SP2wco_vdw_S0
+#undef SP3wco_vdw_S0
+#undef SP0wco_vdw_S2
+#undef SP1wco_vdw_S2
+#undef SP2wco_vdw_S2
+#undef SP3wco_vdw_S2
+
+#undef EXCL_FORCES
+#undef SKIP_INVSQRT
diff -ruN orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner.h.in gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner.h.in
--- orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner.h.in	1970-01-01 09:00:00.000000000 +0900
+++ gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_inner.h.in	2022-03-08 10:54:48.000000000 +0900
@@ -0,0 +1,898 @@
+/*
+ * This file is part of the GROMACS molecular simulation package.
+ *
+ * Copyright 2012- The GROMACS Authors
+ * and the project initiators Erik Lindahl, Berk Hess and David van der Spoel.
+ * Consult the AUTHORS/COPYING files and https://www.gromacs.org for details.
+ *
+ * GROMACS is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public License
+ * as published by the Free Software Foundation; either version 2.1
+ * of the License, or (at your option) any later version.
+ *
+ * GROMACS is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with GROMACS; if not, see
+ * https://www.gnu.org/licenses, or write to the Free Software Foundation,
+ * Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA.
+ *
+ * If you want to redistribute modifications to GROMACS, please
+ * consider that scientific software is very special. Version
+ * control is crucial - bugs must be traceable. We will be happy to
+ * consider code for inclusion in the official distribution, but
+ * derived work must not be called official GROMACS. Details are found
+ * in the README & COPYING files - if they are missing, get the
+ * official version at https://www.gromacs.org.
+ *
+ * To help us fund GROMACS development, we humbly ask that you cite
+ * the research papers on the package. Check out https://www.gromacs.org.
+ */
+
+/* This is the innermost loop contents for the 4 x N atom simd kernel.
+ * This flavor of the kernel duplicates the data for N j-particles in
+ * 2xN wide simd registers to do operate on 2 i-particles at once.
+ * This leads to 4/2=2 sets of most instructions. Therefore we call
+ * this kernel 2x(N+N) = 2xnn
+ *
+ * This 2xnn kernel is basically the 4xn equivalent with half the registers
+ * and instructions removed.
+ *
+ * An alternative would be to load to different cluster of N j-particles
+ * into simd registers, giving a 4x(N+N) kernel. This doubles the amount
+ * of instructions, which could lead to better scheduling. But we actually
+ * observed worse scheduling for the AVX-256 4x8 normal analytical PME
+ * kernel, which has a lower pair throughput than 2x(4+4) with gcc 4.7.
+ * It could be worth trying this option, but it takes some more effort.
+ * This 2xnn kernel is basically the 4xn equivalent with
+ */
+
+
+/* When calculating RF or Ewald interactions we calculate the electrostatic/LJ
+ * forces on excluded atom pairs here in the non-bonded loops.
+ * But when energies and/or virial is required we calculate them
+ * separately to as then it is easier to separate the energy and virial
+ * contributions.
+ */
+#if defined CHECK_EXCLS && (defined CALC_COULOMB || defined LJ_EWALD_GEOM)
+#    define EXCL_FORCES
+#endif
+#if defined CALC_COULOMB || defined CALC_COUL_TAB || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH \
+        || defined LJ_COULOMB_LB || defined HALF_LJ || defined EXCL_FORCES || defined LJ_COMB_LB
+#    define SKIP_INVSQRT 0
+#else
+#    define SKIP_INVSQRT 1
+#endif
+
+#ifdef ENERGY_GROUPS
+    /* Energy group indices for two atoms packed into one int */
+    int SP@egp_jj[UNROLLJ / 2];
+#endif
+
+#ifdef CHECK_EXCLS
+    /* Interaction (non-exclusion) mask of all 1's or 0's */
+    SimdBool SP@interact_S0;
+    SimdBool SP@interact_S2;
+#endif
+
+    SimdReal SP@jx_S, SP@jy_S, SP@jz_S;
+    SimdReal SP@dx_S0, SP@dy_S0, SP@dz_S0;
+    SimdReal SP@dx_S2, SP@dy_S2, SP@dz_S2;
+    SimdReal SP@tx_S0, SP@ty_S0, SP@tz_S0;
+    SimdReal SP@tx_S2, SP@ty_S2, SP@tz_S2;
+    SimdReal SP@rsq_S0, SP@rinvsq_S0;
+    SimdReal SP@rsq_S2, SP@rinvsq_S2;
+#if !SKIP_INVSQRT
+    SimdReal SP@rinv_S0, SP@rinv_S2;
+#endif
+    /* wco: within cut-off, mask of all 1's or 0's */
+    SimdBool SP@wco_S0;
+    SimdBool SP@wco_S2;
+#ifdef VDW_CUTOFF_CHECK
+    SimdBool SP@wco_vdw_S0;
+#    ifndef HALF_LJ
+    SimdBool SP@wco_vdw_S2;
+#    endif
+#endif
+
+#if (defined CALC_COULOMB && defined CALC_COUL_TAB) || defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    SimdReal                                                        SP@r_S0;
+#    if (defined CALC_COULOMB && defined CALC_COUL_TAB) || !defined HALF_LJ
+    SimdReal                                                        SP@r_S2;
+#    endif
+#endif
+
+#if defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    SimdReal SP@rsw_S0, SP@rsw2_S0;
+#    ifndef HALF_LJ
+    SimdReal SP@rsw_S2, SP@rsw2_S2;
+#    endif
+#endif
+
+#ifdef CALC_COULOMB
+#    ifdef CHECK_EXCLS
+    /* 1/r masked with the interaction mask */
+    SimdReal SP@rinv_ex_S0;
+    SimdReal SP@rinv_ex_S2;
+#    endif
+    SimdReal SP@jq_S;
+    SimdReal SP@qq_S0;
+    SimdReal SP@qq_S2;
+#    ifdef CALC_COUL_TAB
+    /* The force (PME mesh force) we need to subtract from 1/r^2 */
+    SimdReal SP@fsub_S0;
+    SimdReal SP@fsub_S2;
+#    endif
+#    ifdef CALC_COUL_EWALD
+    SimdReal SP@brsq_S0, SP@brsq_S2;
+    SimdReal SP@ewcorr_S0, SP@ewcorr_S2;
+#    endif
+
+    /* frcoul = (1/r - fsub)*r */
+    SimdReal SP@frcoul_S0;
+    SimdReal SP@frcoul_S2;
+#    ifdef CALC_COUL_TAB
+    /* For tables: r, rs=r/sp, rf=floor(rs), frac=rs-rf */
+    SimdReal SP@rs_S0, SP@rf_S0, SP@frac_S0;
+    SimdReal SP@rs_S2, SP@rf_S2, SP@frac_S2;
+    /* Table index: rs truncated to an int */
+    SimdInt32 SP@ti_S0, SP@ti_S2;
+    /* Linear force table values */
+    SimdReal SP@ctab0_S0, SP@ctab1_S0;
+    SimdReal SP@ctab0_S2, SP@ctab1_S2;
+#        ifdef CALC_ENERGIES
+    /* Quadratic energy table value */
+    SimdReal SP@ctabv_S0, SP@dum_S0;
+    SimdReal SP@ctabv_S2, SP@dum_S2;
+#        endif
+#    endif
+#    if defined CALC_ENERGIES && (defined CALC_COUL_EWALD || defined CALC_COUL_TAB)
+    /* The potential (PME mesh) we need to subtract from 1/r */
+    SimdReal SP@vc_sub_S0;
+    SimdReal SP@vc_sub_S2;
+#    endif
+#    ifdef CALC_ENERGIES
+    /* Electrostatic potential */
+    SimdReal SP@vcoul_S0;
+    SimdReal SP@vcoul_S2;
+#    endif
+#endif
+    /* The force times 1/r */
+    SimdReal SP@fscal_S0;
+    SimdReal SP@fscal_S2;
+
+#ifdef CALC_LJ
+#    ifdef LJ_COMB_LB
+    /* LJ sigma_j/2 and sqrt(epsilon_j) */
+    SimdReal SP@hsig_j_S, SP@seps_j_S;
+    /* LJ sigma_ij and epsilon_ij */
+    SimdReal SP@sig_S0, SP@eps_S0;
+#        ifndef HALF_LJ
+    SimdReal SP@sig_S2, SP@eps_S2;
+#        endif
+#        ifdef CALC_ENERGIES
+    SimdReal SP@sig2_S0, SP@sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP@sig2_S2, SP@sig6_S2;
+#            endif
+#        endif /* LJ_COMB_LB */
+#    endif     /* CALC_LJ */
+
+#    ifdef LJ_COMB_GEOM
+    SimdReal SP@c6s_j_S, SP@c12s_j_S;
+#    endif
+
+    /* Intermediate variables for LJ calculation */
+#    ifndef LJ_COMB_LB
+    SimdReal SP@rinvsix_S0;
+#        ifndef HALF_LJ
+    SimdReal SP@rinvsix_S2;
+#        endif
+#    endif
+#    ifdef LJ_COMB_LB
+    SimdReal SP@sir_S0, SP@sir2_S0, SP@sir6_S0;
+#        ifndef HALF_LJ
+    SimdReal SP@sir_S2, SP@sir2_S2, SP@sir6_S2;
+#        endif
+#    endif
+
+    SimdReal SP@FrLJ6_S0, SP@FrLJ12_S0, SP@frLJ_S0;
+#    ifndef HALF_LJ
+    SimdReal SP@FrLJ6_S2, SP@FrLJ12_S2, SP@frLJ_S2;
+#    endif
+#endif /* CALC_LJ */
+
+    /* j-cluster index */
+    const int SP@cj = l_cj[cjind+@].cj;
+
+    /* Atom indices (of the first atom in the cluster) */
+    const int SP@aj = SP@cj * UNROLLJ;
+#if defined CALC_LJ && (defined LJ_COMB_GEOM || defined LJ_COMB_LB || defined LJ_EWALD_GEOM)
+    /* Index for loading LJ parameters, complicated when interleaving */
+    const int SP@aj2 = SP@aj * 2;
+#endif
+
+#ifdef CHECK_EXCLS
+    gmx_load_simd_2xnn_interactions(static_cast<int>(l_cj[cjind+@].excl), filter_S0, filter_S2, &interact_S0, &interact_S2);
+#endif /* CHECK_EXCLS */
+
+    /* load j atom coordinates */
+    loadDuplicate3Hsimd<STRIDE>(x + SP@aj * DIM, &SP@jx_S, &SP@jy_S, &SP@jz_S);
+
+    /* Calculate distance */
+    SP@dx_S0 = ix_S0 - SP@jx_S;
+    SP@dy_S0 = iy_S0 - SP@jy_S;
+    SP@dz_S0 = iz_S0 - SP@jz_S;
+    SP@dx_S2 = ix_S2 - SP@jx_S;
+    SP@dy_S2 = iy_S2 - SP@jy_S;
+    SP@dz_S2 = iz_S2 - SP@jz_S;
+
+    /* rsq = dx*dx+dy*dy+dz*dz */
+    SP@rsq_S0 = norm2(SP@dx_S0, SP@dy_S0, SP@dz_S0);
+    SP@rsq_S2 = norm2(SP@dx_S2, SP@dy_S2, SP@dz_S2);
+
+    /* Do the cut-off check */
+    SP@wco_S0 = (SP@rsq_S0 < rc2_S);
+    SP@wco_S2 = (SP@rsq_S2 < rc2_S);
+
+#ifdef CHECK_EXCLS
+#    ifdef EXCL_FORCES
+    /* Only remove the (sub-)diagonal to avoid double counting */
+#        if UNROLLJ == UNROLLI
+    if (cj == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask_S0;
+        wco_S2 = wco_S2 && diagonal_mask_S2;
+    }
+#        else
+#            if UNROLLJ == 2 * UNROLLI
+    if (cj * 2 == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask0_S0;
+        wco_S2 = wco_S2 && diagonal_mask0_S2;
+    }
+    else if (cj * 2 + 1 == ci_sh)
+    {
+        wco_S0 = wco_S0 && diagonal_mask1_S0;
+        wco_S2 = wco_S2 && diagonal_mask1_S2;
+    }
+#            else
+#                error "only UNROLLJ == UNROLLI*(1 or 2) currently supported in 2xnn kernels"
+#            endif
+#        endif
+#    else /* EXCL_FORCES */
+    /* No exclusion forces: remove all excluded atom pairs from the list */
+    wco_S0 = wco_S0 && interact_S0;
+    wco_S2 = wco_S2 && interact_S2;
+#    endif
+#endif
+
+#ifdef COUNT_PAIRS
+    {
+        int                              i, j;
+        alignas(GMX_SIMD_ALIGNMENT) real tmp[GMX_SIMD_REAL_WIDTH];
+
+        for (i = 0; i < UNROLLI; i += 2)
+        {
+            store(tmp, rc2_S - (i == 0 ? rsq_S0 : rsq_S2));
+            for (j = 0; j < 2 * UNROLLJ; j++)
+            {
+                if (tmp[j] >= 0)
+                {
+                    npair++;
+                }
+            }
+        }
+    }
+#endif
+
+    // Ensure the distances do not fall below the limit where r^-12 overflows.
+    // This should never happen for normal interactions.
+    SP@rsq_S0 = max(SP@rsq_S0, minRsq_S);
+    SP@rsq_S2 = max(SP@rsq_S2, minRsq_S);
+
+    /* Calculate 1/r */
+#if SKIP_INVSQRT
+    SP@rinvsq_S0 = invMask(SP@rsq_S0, SP@wco_S0);
+    SP@rinvsq_S2 = invMask(SP@rsq_S2, SP@wco_S2);
+#else
+    /* and set rinv to zero for r beyond the cut-off */
+    SP@rinv_S0 = invsqrtMask(SP@rsq_S0, SP@wco_S0);
+    SP@rinv_S2 = invsqrtMask(SP@rsq_S2, SP@wco_S2);
+#endif
+
+#ifdef CALC_COULOMB
+    /* Load parameters for j atom */
+    SP@jq_S = loadDuplicateHsimd(q + SP@aj);
+    SP@qq_S0 = iq_S0 * SP@jq_S;
+    SP@qq_S2 = iq_S2 * SP@jq_S;
+#endif
+
+#ifdef CALC_LJ
+#    if !defined LJ_COMB_GEOM && !defined LJ_COMB_LB && !defined FIX_LJ_C
+    SimdReal                                                     SP@c6_S0, SP@c12_S0;
+#        ifdef HALF_LJ
+    gatherLoadTransposeHsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, type + SP@aj, &SP@c6_S0, &SP@c12_S0);
+#        else
+    SimdReal SP@c6_S2, SP@c12_S2;
+    gatherLoadTranspose2Hsimd<c_simdBestPairAlignment>(nbfp0, nbfp1, nbfp2, nbfp3, type + SP@aj, &SP@c6_S0, &SP@c12_S0, &SP@c6_S2, &SP@c12_S2);
+#        endif
+#    endif /* not defined any LJ rule */
+
+#    ifdef LJ_COMB_GEOM
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP@aj2, &SP@c6s_j_S, &SP@c12s_j_S);
+    SimdReal SP@c6_S0 = c6s_S0 * SP@c6s_j_S;
+#        ifndef HALF_LJ
+    SimdReal SP@c6_S2 = c6s_S2 * SP@c6s_j_S;
+#        endif
+    SimdReal SP@c12_S0 = c12s_S0 * SP@c12s_j_S;
+#        ifndef HALF_LJ
+    SimdReal SP@c12_S2 = c12s_S2 * SP@c12s_j_S;
+#        endif
+#    endif /* LJ_COMB_GEOM */
+
+#    ifdef LJ_COMB_LB
+    loadDuplicate2Hsimd<STRIDE>(ljc + SP@aj2, &SP@hsig_j_S, &SP@seps_j_S);
+
+    SP@sig_S0 = hsig_i_S0 + SP@hsig_j_S;
+    SP@eps_S0 = seps_i_S0 * SP@seps_j_S;
+#        ifndef HALF_LJ
+    SP@sig_S2 = hsig_i_S2 + SP@hsig_j_S;
+    SP@eps_S2 = seps_i_S2 * SP@seps_j_S;
+#        endif
+#    endif /* LJ_COMB_LB */
+
+#endif /* CALC_LJ */
+
+#if !SKIP_INVSQRT
+    SP@rinvsq_S0 = SP@rinv_S0 * SP@rinv_S0;
+    SP@rinvsq_S2 = SP@rinv_S2 * SP@rinv_S2;
+#endif
+
+#ifdef CALC_COULOMB
+    /* Note that here we calculate force*r, not the usual force/r.
+     * This allows avoiding masking the reaction-field contribution,
+     * as frcoul is later multiplied by rinvsq which has been
+     * masked with the cut-off check.
+     */
+
+#    ifdef EXCL_FORCES
+    /* Only add 1/r for non-excluded atom pairs */
+    rinv_ex_S0 = selectByMask(rinv_S0, interact_S0);
+    rinv_ex_S2 = selectByMask(rinv_S2, interact_S2);
+#    else
+    /* No exclusion forces, we always need 1/r */
+#        define SP@rinv_ex_S0 SP@rinv_S0
+#        define SP@rinv_ex_S2 SP@rinv_S2
+#    endif
+
+#    ifdef CALC_COUL_RF
+    /* Electrostatic interactions */
+    SP@frcoul_S0 = SP@qq_S0 * fma(SP@rsq_S0, mrc_3_S, SP@rinv_ex_S0);
+    SP@frcoul_S2 = SP@qq_S2 * fma(SP@rsq_S2, mrc_3_S, SP@rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    /* and (merge) mask energy for cut-off and diagonal */
+    SP@vcoul_S0 = maskzMul(SP@qq_S0, SP@rinv_ex_S0 + fma(SP@rsq_S0, hrc_3_S, moh_rc_S), SP@wco_S0);
+    SP@vcoul_S2 = maskzMul(SP@qq_S2, SP@rinv_ex_S2 + fma(SP@rsq_S2, hrc_3_S, moh_rc_S), SP@wco_S2);
+#        endif
+#    endif
+
+#    ifdef CALC_COUL_EWALD
+    /* We need to mask (or limit) rsq for the cut-off,
+     * as large distances can cause an overflow in gmx_pmecorrF/V.
+     */
+    SP@brsq_S0   = maskzMul(beta2_S, SP@rsq_S0, SP@wco_S0);
+    SP@brsq_S2   = maskzMul(beta2_S, SP@rsq_S2, SP@wco_S2);
+    SP@ewcorr_S0 = beta_S * pmeForceCorrection(SP@brsq_S0);
+    SP@ewcorr_S2 = beta_S * pmeForceCorrection(SP@brsq_S2);
+    SP@frcoul_S0 = SP@qq_S0 * fma(SP@ewcorr_S0, SP@brsq_S0, SP@rinv_ex_S0);
+    SP@frcoul_S2 = SP@qq_S2 * fma(SP@ewcorr_S2, SP@brsq_S2, SP@rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    SP@vc_sub_S0 = beta_S * pmePotentialCorrection(SP@brsq_S0);
+    SP@vc_sub_S2 = beta_S * pmePotentialCorrection(SP@brsq_S2);
+#        endif
+
+#    endif /* CALC_COUL_EWALD */
+
+#    ifdef CALC_COUL_TAB
+    /* Electrostatic interactions */
+    SP@r_S0 = SP@rsq_S0 * SP@rinv_S0;
+    SP@r_S2 = SP@rsq_S2 * SP@rinv_S2;
+    /* Convert r to scaled table units */
+    SP@rs_S0 = SP@r_S0 * invtsp_S;
+    SP@rs_S2 = SP@r_S2 * invtsp_S;
+    /* Truncate scaled r to an int */
+    SP@ti_S0 = cvttR2I(SP@rs_S0);
+    SP@ti_S2 = cvttR2I(SP@rs_S2);
+
+    SP@rf_S0 = trunc(SP@rs_S0);
+    SP@rf_S2 = trunc(SP@rs_S2);
+
+    SP@frac_S0 = SP@rs_S0 - SP@rf_S0;
+    SP@frac_S2 = SP@rs_S2 - SP@rf_S2;
+
+    /* Load and interpolate table forces and possibly energies.
+     * Force and energy can be combined in one table, stride 4: FDV0
+     * or in two separate tables with stride 1: F and V
+     * Currently single precision uses FDV0, double F and V.
+     */
+#        ifndef CALC_ENERGIES
+#            ifdef TAB_FDV0
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S0, &ctab0_S0, &ctab1_S0);
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S2, &ctab0_S2, &ctab1_S2);
+#            else
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP@ti_S0, &SP@ctab0_S0, &SP@ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP@ti_S2, &SP@ctab0_S2, &SP@ctab1_S2);
+    SP@ctab1_S0   = SP@ctab1_S0 - SP@ctab0_S0;
+    SP@ctab1_S2   = SP@ctab1_S2 - SP@ctab0_S2;
+#            endif
+#        else
+#            ifdef TAB_FDV0
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S0, &ctab0_S0, &ctab1_S0, &ctabv_S0, &dum_S0);
+    gatherLoadBySimdIntTranspose<4>(tab_coul_F, ti_S2, &ctab0_S2, &ctab1_S2, &ctabv_S2, &dum_S2);
+#            else
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP@ti_S0, &SP@ctab0_S0, &SP@ctab1_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP@ti_S0, &SP@ctabv_S0, &SP@dum_S0);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_F, SP@ti_S2, &SP@ctab0_S2, &SP@ctab1_S2);
+    gatherLoadUBySimdIntTranspose<1>(tab_coul_V, SP@ti_S2, &SP@ctabv_S2, &SP@dum_S2);
+    SP@ctab1_S0 = SP@ctab1_S0 - SP@ctab0_S0;
+    SP@ctab1_S2 = SP@ctab1_S2 - SP@ctab0_S2;
+#            endif
+#        endif
+    SP@fsub_S0   = fma(SP@frac_S0, SP@ctab1_S0, SP@ctab0_S0);
+    SP@fsub_S2   = fma(SP@frac_S2, SP@ctab1_S2, SP@ctab0_S2);
+    SP@frcoul_S0 = SP@qq_S0 * fnma(SP@fsub_S0, SP@r_S0, SP@rinv_ex_S0);
+    SP@frcoul_S2 = SP@qq_S2 * fnma(SP@fsub_S2, SP@r_S2, SP@rinv_ex_S2);
+
+#        ifdef CALC_ENERGIES
+    SP@vc_sub_S0 = fma((mhalfsp_S * SP@frac_S0), (SP@ctab0_S0 + SP@fsub_S0), SP@ctabv_S0);
+    SP@vc_sub_S2 = fma((mhalfsp_S * SP@frac_S2), (SP@ctab0_S2 + SP@fsub_S2), SP@ctabv_S2);
+#        endif
+#    endif /* CALC_COUL_TAB */
+
+#    if defined CALC_ENERGIES && (defined CALC_COUL_EWALD || defined CALC_COUL_TAB)
+#        ifndef NO_SHIFT_EWALD
+    /* Add Ewald potential shift to vc_sub for convenience */
+#            ifdef CHECK_EXCLS
+    vc_sub_S0 = maskAdd(vc_sub_S0, sh_ewald_S, interact_S0);
+    vc_sub_S2 = maskAdd(vc_sub_S2, sh_ewald_S, interact_S2);
+#            else
+    SP@vc_sub_S0  = SP@vc_sub_S0 + sh_ewald_S;
+    SP@vc_sub_S2  = SP@vc_sub_S2 + sh_ewald_S;
+#            endif
+#        endif
+
+    /* and (merge) mask energy for cut-off and diagonal */
+    SP@vcoul_S0 = maskzMul(SP@qq_S0, SP@rinv_ex_S0 - SP@vc_sub_S0, SP@wco_S0);
+    SP@vcoul_S2 = maskzMul(SP@qq_S2, SP@rinv_ex_S2 - SP@vc_sub_S2, SP@wco_S2);
+
+#    endif
+
+#endif /* CALC_COULOMB */
+
+#ifdef CALC_LJ
+    /* Lennard-Jones interaction */
+
+#    ifdef VDW_CUTOFF_CHECK
+    SP@wco_vdw_S0 = (SP@rsq_S0 < rcvdw2_S);
+#        ifndef HALF_LJ
+    SP@wco_vdw_S2 = (SP@rsq_S2 < rcvdw2_S);
+#        endif
+#    else
+    /* Same cut-off for Coulomb and VdW, reuse the registers */
+#        define SP@wco_vdw_S0 SP@wco_S0
+#        define SP@wco_vdw_S2 SP@wco_S2
+#    endif
+
+#    ifndef LJ_COMB_LB
+    SP@rinvsix_S0 = SP@rinvsq_S0 * SP@rinvsq_S0;
+#        ifdef EXCL_FORCES
+    rinvsix_S0 = maskzMul(rinvsix_S0, rinvsq_S0, interact_S0);
+#        else
+    SP@rinvsix_S0 = SP@rinvsix_S0 * SP@rinvsq_S0;
+#        endif
+#        ifndef HALF_LJ
+    SP@rinvsix_S2 = SP@rinvsq_S2 * SP@rinvsq_S2;
+#            ifdef EXCL_FORCES
+    rinvsix_S2 = maskzMul(rinvsix_S2, rinvsq_S2, interact_S2);
+#            else
+    SP@rinvsix_S2 = SP@rinvsix_S2 * SP@rinvsq_S2;
+#            endif
+#        endif
+
+#        if defined LJ_CUT || defined LJ_POT_SWITCH
+    /* We have plain LJ or LJ-PME with simple C6/6 C12/12 coefficients */
+    SP@FrLJ6_S0 = SP@c6_S0 * SP@rinvsix_S0;
+#            ifndef HALF_LJ
+    SP@FrLJ6_S2 = SP@c6_S2 * SP@rinvsix_S2;
+#            endif
+    SP@FrLJ12_S0 = SP@c12_S0 * SP@rinvsix_S0 * SP@rinvsix_S0;
+#            ifndef HALF_LJ
+    SP@FrLJ12_S2 = SP@c12_S2 * SP@rinvsix_S2 * SP@rinvsix_S2;
+#            endif
+#        endif
+
+#        if defined LJ_FORCE_SWITCH || defined LJ_POT_SWITCH
+    /* We switch the LJ force */
+    SP@r_S0    = SP@rsq_S0 * SP@rinv_S0;
+    SP@rsw_S0  = max(SP@r_S0 - rswitch_S, zero_S);
+    SP@rsw2_S0 = SP@rsw_S0 * SP@rsw_S0;
+#            ifndef HALF_LJ
+    SP@r_S2    = SP@rsq_S2 * SP@rinv_S2;
+    SP@rsw_S2  = max(SP@r_S2 - rswitch_S, zero_S);
+    SP@rsw2_S2 = SP@rsw_S2 * SP@rsw_S2;
+#            endif
+#        endif
+
+#        ifdef LJ_FORCE_SWITCH
+
+#            define add_fr_switch(fr, rsw, rsw2_r, c2, c3) fma(fma(c3, rsw, c2), rsw2_r, fr)
+    SimdReal SP@rsw2_r_S0 = SP@rsw2_S0 * SP@r_S0;
+    SP@FrLJ6_S0           = SP@c6_S0 * add_fr_switch(SP@rinvsix_S0, SP@rsw_S0, SP@rsw2_r_S0, p6_fc2_S, p6_fc3_S);
+#            ifndef HALF_LJ
+    SimdReal SP@rsw2_r_S2 = SP@rsw2_S2 * SP@r_S2;
+    SP@FrLJ6_S2           = SP@c6_S2 * add_fr_switch(SP@rinvsix_S2, SP@rsw_S2, SP@rsw2_r_S2, p6_fc2_S, p6_fc3_S);
+#            endif
+    SP@FrLJ12_S0 = SP@c12_S0 * add_fr_switch(SP@rinvsix_S0 * SP@rinvsix_S0, SP@rsw_S0, SP@rsw2_r_S0, p12_fc2_S, p12_fc3_S);
+#            ifndef HALF_LJ
+    SP@FrLJ12_S2 = SP@c12_S2 * add_fr_switch(SP@rinvsix_S2 * SP@rinvsix_S2, SP@rsw_S2, SP@rsw2_r_S2, p12_fc2_S, p12_fc3_S);
+#            endif
+#            undef add_fr_switch
+#        endif /* LJ_FORCE_SWITCH */
+
+#    endif /* not LJ_COMB_LB */
+
+#    ifdef LJ_COMB_LB
+    SP@sir_S0 = SP@sig_S0 * SP@rinv_S0;
+#        ifndef HALF_LJ
+    SP@sir_S2 = SP@sig_S2 * SP@rinv_S2;
+#        endif
+    SP@sir2_S0 = SP@sir_S0 * SP@sir_S0;
+#        ifndef HALF_LJ
+    SP@sir2_S2 = SP@sir_S2 * SP@sir_S2;
+#        endif
+#        ifdef VDW_CUTOFF_CHECK
+    SP@sir6_S0 = maskzMul(SP@sir2_S0, SP@sir2_S0, SP@wco_vdw_S0);
+#        else
+    SP@sir6_S0    = SP@sir2_S0 * SP@sir2_S0;
+#        endif
+#        ifdef EXCL_FORCES
+    sir6_S0 = maskzMul(sir6_S0, sir2_S0, interact_S0);
+#        else
+    SP@sir6_S0    = SP@sir6_S0 * SP@sir2_S0;
+#        endif
+#        ifndef HALF_LJ
+#            ifdef VDW_CUTOFF_CHECK
+    SP@sir6_S2 = maskzMul(SP@sir2_S2, SP@sir2_S2, SP@wco_vdw_S2);
+#            else
+    SP@sir6_S2    = SP@sir2_S2 * SP@sir2_S2;
+#            endif
+#            ifdef EXCL_FORCES
+    sir6_S2 = maskzMul(sir6_S2, sir2_S2, interact_S2);
+#            else
+    SP@sir6_S2    = SP@sir6_S2 * SP@sir2_S2;
+#            endif
+#        endif
+    SP@FrLJ6_S0 = SP@eps_S0 * SP@sir6_S0;
+#        ifndef HALF_LJ
+    SP@FrLJ6_S2 = SP@eps_S2 * SP@sir6_S2;
+#        endif
+    SP@FrLJ12_S0 = SP@FrLJ6_S0 * SP@sir6_S0;
+#        ifndef HALF_LJ
+    SP@FrLJ12_S2 = SP@FrLJ6_S2 * SP@sir6_S2;
+#        endif
+#        if defined CALC_ENERGIES
+    /* We need C6 and C12 to calculate the LJ potential shift */
+    SP@sig2_S0 = SP@sig_S0 * SP@sig_S0;
+#            ifndef HALF_LJ
+    SP@sig2_S2 = SP@sig_S2 * SP@sig_S2;
+#            endif
+    SP@sig6_S0 = SP@sig2_S0 * SP@sig2_S0 * SP@sig2_S0;
+#            ifndef HALF_LJ
+    SP@sig6_S2 = SP@sig2_S2 * SP@sig2_S2 * SP@sig2_S2;
+#            endif
+    SimdReal SP@c6_S0 = SP@eps_S0 * SP@sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP@c6_S2 = SP@eps_S2 * SP@sig6_S2;
+#            endif
+    SimdReal SP@c12_S0 = SP@c6_S0 * SP@sig6_S0;
+#            ifndef HALF_LJ
+    SimdReal SP@c12_S2 = SP@c6_S2 * SP@sig6_S2;
+#            endif
+#        endif
+#    endif /* LJ_COMB_LB */
+
+    /* Determine the total scalar LJ force*r */
+    SP@frLJ_S0 = SP@FrLJ12_S0 - SP@FrLJ6_S0;
+#    ifndef HALF_LJ
+    SP@frLJ_S2 = SP@FrLJ12_S2 - SP@FrLJ6_S2;
+#    endif
+
+#    if (defined LJ_CUT || defined LJ_FORCE_SWITCH) && defined CALC_ENERGIES
+
+#        ifdef LJ_CUT
+    /* Calculate the LJ energies, with constant potential shift */
+    SimdReal SP@VLJ6_S0 = sixth_S * fma(SP@c6_S0, p6_cpot_S, SP@FrLJ6_S0);
+#            ifndef HALF_LJ
+    SimdReal SP@VLJ6_S2 = sixth_S * fma(SP@c6_S2, p6_cpot_S, SP@FrLJ6_S2);
+#            endif
+    SimdReal SP@VLJ12_S0 = twelveth_S * fma(SP@c12_S0, p12_cpot_S, SP@FrLJ12_S0);
+#            ifndef HALF_LJ
+    SimdReal SP@VLJ12_S2 = twelveth_S * fma(SP@c12_S2, p12_cpot_S, SP@FrLJ12_S2);
+#            endif
+#        endif /* LJ_CUT */
+
+#        ifdef LJ_FORCE_SWITCH
+#            define v_fswitch_pr(rsw, rsw2, c0, c3, c4) fma(fma(c4, rsw, c3), (rsw2) * (rsw), c0)
+
+    SimdReal SP@VLJ6_S0 = SP@c6_S0 * fma(sixth_S, SP@rinvsix_S0, v_fswitch_pr(SP@rsw_S0, SP@rsw2_S0, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+#            ifndef HALF_LJ
+    SimdReal SP@VLJ6_S2 = SP@c6_S2 * fma(sixth_S, SP@rinvsix_S2, v_fswitch_pr(SP@rsw_S2, SP@rsw2_S2, p6_6cpot_S, p6_vc3_S, p6_vc4_S));
+#            endif
+    SimdReal SP@VLJ12_S0 = SP@c12_S0 * fma(twelveth_S, SP@rinvsix_S0 * SP@rinvsix_S0, v_fswitch_pr(SP@rsw_S0, SP@rsw2_S0, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+#            ifndef HALF_LJ
+    SimdReal SP@VLJ12_S2 = SP@c12_S2 * fma(twelveth_S, SP@rinvsix_S2 * SP@rinvsix_S2, v_fswitch_pr(SP@rsw_S2, SP@rsw2_S2, p12_12cpot_S, p12_vc3_S, p12_vc4_S));
+#            endif
+#            undef v_fswitch_pr
+#        endif /* LJ_FORCE_SWITCH */
+
+    /* Add up the repulsion and dispersion */
+    SimdReal SP@VLJ_S0 = SP@VLJ12_S0 - SP@VLJ6_S0;
+#        ifndef HALF_LJ
+    SimdReal SP@VLJ_S2 = SP@VLJ12_S2 - SP@VLJ6_S2;
+#        endif
+
+#    endif /* (LJ_CUT || LJ_FORCE_SWITCH) && CALC_ENERGIES */
+
+#    ifdef LJ_POT_SWITCH
+    /* We always need the potential, since it is needed for the force */
+    SimdReal SP@VLJ_S0 = fnma(sixth_S, SP@FrLJ6_S0, twelveth_S * SP@FrLJ12_S0);
+#        ifndef HALF_LJ
+    SimdReal SP@VLJ_S2 = fnma(sixth_S, SP@FrLJ6_S2, twelveth_S * SP@FrLJ12_S2);
+#        endif
+
+    {
+        SimdReal SP@sw_S0, SP@dsw_S0;
+#        ifndef HALF_LJ
+        SimdReal SP@sw_S2, SP@dsw_S2;
+#        endif
+
+#        define switch_pr(rsw, rsw2, c3, c4, c5) \
+            fma(fma(fma(c5, rsw, c4), rsw, c3), (rsw2) * (rsw), one_S)
+#        define dswitch_pr(rsw, rsw2, c2, c3, c4) fma(fma(c4, rsw, c3), rsw, c2) * (rsw2)
+
+        SP@sw_S0  = switch_pr(SP@rsw_S0, SP@rsw2_S0, swV3_S, swV4_S, swV5_S);
+        SP@dsw_S0 = dswitch_pr(SP@rsw_S0, SP@rsw2_S0, swF2_S, swF3_S, swF4_S);
+#        ifndef HALF_LJ
+        SP@sw_S2  = switch_pr(SP@rsw_S2, SP@rsw2_S2, swV3_S, swV4_S, swV5_S);
+        SP@dsw_S2 = dswitch_pr(SP@rsw_S2, SP@rsw2_S2, swF2_S, swF3_S, swF4_S);
+#        endif
+        SP@frLJ_S0 = fnma(SP@dsw_S0 * SP@VLJ_S0, SP@r_S0, SP@sw_S0 * SP@frLJ_S0);
+#        ifndef HALF_LJ
+        SP@frLJ_S2 = fnma(SP@dsw_S2 * SP@VLJ_S2, SP@r_S2, SP@sw_S2 * SP@frLJ_S2);
+#        endif
+#        ifdef CALC_ENERGIES
+        SP@VLJ_S0 = SP@sw_S0 * SP@VLJ_S0;
+#            ifndef HALF_LJ
+        SP@VLJ_S2 = SP@sw_S2 * SP@VLJ_S2;
+#            endif
+#        endif
+
+#        undef switch_pr
+#        undef dswitch_pr
+    }
+#    endif /* LJ_POT_SWITCH */
+
+#    if defined CALC_ENERGIES && defined CHECK_EXCLS
+    /* The potential shift should be removed for excluded pairs */
+    VLJ_S0 = selectByMask(VLJ_S0, interact_S0);
+#        ifndef HALF_LJ
+    VLJ_S2 = selectByMask(VLJ_S2, interact_S2);
+#        endif
+#    endif
+
+#    ifdef LJ_EWALD_GEOM
+    {
+        SimdReal SP@c6s_j_S;
+        SimdReal SP@c6grid_S0, SP@rinvsix_nm_S0, SP@cr2_S0, SP@expmcr2_S0, SP@poly_S0;
+#        ifndef HALF_LJ
+        SimdReal SP@c6grid_S2, SP@rinvsix_nm_S2, SP@cr2_S2, SP@expmcr2_S2, SP@poly_S2;
+#        endif
+#        ifdef CALC_ENERGIES
+        SimdReal SP@sh_mask_S0;
+#            ifndef HALF_LJ
+        SimdReal SP@sh_mask_S2;
+#            endif
+#        endif
+
+        /* Determine C6 for the grid using the geometric combination rule */
+        SP@c6s_j_S   = loadDuplicateHsimd(ljc + SP@aj2);
+        SP@c6grid_S0 = c6s_S0 * SP@c6s_j_S;
+#        ifndef HALF_LJ
+        SP@c6grid_S2 = c6s_S2 * SP@c6s_j_S;
+#        endif
+
+#        ifdef CHECK_EXCLS
+        /* Recalculate rinvsix without exclusion mask (compiler might optimize) */
+        rinvsix_nm_S0 = rinvsq_S0 * rinvsq_S0 * rinvsq_S0;
+#            ifndef HALF_LJ
+        rinvsix_nm_S2 = rinvsq_S2 * rinvsq_S2 * rinvsq_S2;
+#            endif
+#        else
+        /* We didn't use a mask, so we can copy */
+        SP@rinvsix_nm_S0 = SP@rinvsix_S0;
+#            ifndef HALF_LJ
+        SP@rinvsix_nm_S2 = SP@rinvsix_S2;
+#            endif
+#        endif
+
+        /* Mask for the cut-off to avoid overflow of cr2^2 */
+        SP@cr2_S0 = maskzMul(lje_c2_S, SP@rsq_S0, SP@wco_vdw_S0);
+#        ifndef HALF_LJ
+        SP@cr2_S2 = maskzMul(lje_c2_S, SP@rsq_S2, SP@wco_vdw_S2);
+#        endif
+        // Unsafe version of our exp() should be fine, since these arguments should never
+        // be smaller than -127 for any reasonable choice of cutoff or ewald coefficients.
+        SP@expmcr2_S0 = exp<MathOptimization::Unsafe>(-SP@cr2_S0);
+#        ifndef HALF_LJ
+        SP@expmcr2_S2 = exp<MathOptimization::Unsafe>(-SP@cr2_S2);
+#        endif
+
+        /* 1 + cr2 + 1/2*cr2^2 */
+        SP@poly_S0 = fma(fma(half_S, SP@cr2_S0, one_S), SP@cr2_S0, one_S);
+#        ifndef HALF_LJ
+        SP@poly_S2 = fma(fma(half_S, SP@cr2_S2, one_S), SP@cr2_S2, one_S);
+#        endif
+
+        /* We calculate LJ F*r = (6*C6)*(r^-6 - F_mesh/6), we use:
+         * r^-6*cexp*(1 + cr2 + cr2^2/2 + cr2^3/6) = cexp*(r^-6*poly + c^6/6)
+         */
+        SP@frLJ_S0 = fma(SP@c6grid_S0, fnma(SP@expmcr2_S0, fma(SP@rinvsix_nm_S0, SP@poly_S0, lje_c6_6_S), SP@rinvsix_nm_S0), SP@frLJ_S0);
+#        ifndef HALF_LJ
+        SP@frLJ_S2 = fma(SP@c6grid_S2, fnma(SP@expmcr2_S2, fma(SP@rinvsix_nm_S2, SP@poly_S2, lje_c6_6_S), SP@rinvsix_nm_S2), SP@frLJ_S2);
+#        endif
+
+#        ifdef CALC_ENERGIES
+#            ifdef CHECK_EXCLS
+        sh_mask_S0 = selectByMask(lje_vc_S, interact_S0);
+#                ifndef HALF_LJ
+        sh_mask_S2 = selectByMask(lje_vc_S, interact_S2);
+#                endif
+#            else
+        SP@sh_mask_S0 = lje_vc_S;
+#                ifndef HALF_LJ
+        SP@sh_mask_S2 = lje_vc_S;
+#                endif
+#            endif
+
+        SP@VLJ_S0 = fma(sixth_S * SP@c6grid_S0, fma(SP@rinvsix_nm_S0, fnma(SP@expmcr2_S0, SP@poly_S0, one_S), SP@sh_mask_S0), SP@VLJ_S0);
+#            ifndef HALF_LJ
+        SP@VLJ_S2 = fma(sixth_S * SP@c6grid_S2, fma(SP@rinvsix_nm_S2, fnma(SP@expmcr2_S2, SP@poly_S2, one_S), SP@sh_mask_S2), SP@VLJ_S2);
+#            endif
+#        endif /* CALC_ENERGIES */
+    }
+#    endif /* LJ_EWALD_GEOM */
+
+#    if defined VDW_CUTOFF_CHECK
+    /* frLJ is multiplied later by rinvsq, which is masked for the Coulomb
+     * cut-off, but if the VdW cut-off is shorter, we need to mask with that.
+     */
+    SP@frLJ_S0 = selectByMask(SP@frLJ_S0, SP@wco_vdw_S0);
+#        ifndef HALF_LJ
+    SP@frLJ_S2 = selectByMask(SP@frLJ_S2, SP@wco_vdw_S2);
+#        endif
+#    endif
+
+#    ifdef CALC_ENERGIES
+    /* The potential shift should be removed for pairs beyond cut-off */
+    SP@VLJ_S0 = selectByMask(SP@VLJ_S0, SP@wco_vdw_S0);
+#        ifndef HALF_LJ
+    SP@VLJ_S2 = selectByMask(SP@VLJ_S2, SP@wco_vdw_S2);
+#        endif
+#    endif
+
+#endif /* CALC_LJ */
+
+#ifdef CALC_ENERGIES
+#    ifdef ENERGY_GROUPS
+    /* Extract the group pair index per j pair.
+     * Energy groups are stored per i-cluster, so things get
+     * complicated when the i- and j-cluster size don't match.
+     */
+    {
+#        if UNROLLJ == 2
+#error
+        const int egps_j    = nbatParams.energrp[cj >> 1];
+        egp_jj[0]       = ((egps_j >> ((cj & 1) * egps_jshift)) & egps_jmask) * egps_jstride;
+#        else
+        /* We assume UNROLLI <= UNROLLJ */
+        for (int jdi = 0; jdi < UNROLLJ / UNROLLI; jdi++)
+        {
+            const int SP@egps_j = nbatParams.energrp[SP@cj * (UNROLLJ / UNROLLI) + jdi];
+            for (int jj = 0; jj < (UNROLLI / 2); jj++)
+            {
+                SP@egp_jj[jdi * (UNROLLI / 2) + jj] = ((SP@egps_j >> (jj * egps_jshift)) & egps_jmask) * egps_jstride;
+            }
+        }
+#        endif
+    }
+#    endif
+
+#    ifdef CALC_COULOMB
+#        ifndef ENERGY_GROUPS
+    vctot_S = vctot_S + SP@vcoul_S0 + SP@vcoul_S2;
+#        else
+    add_ener_grp_halves(SP@vcoul_S0, vctp[0], vctp[1], SP@egp_jj);
+    add_ener_grp_halves(SP@vcoul_S2, vctp[2], vctp[3], SP@egp_jj);
+#        endif
+#    endif
+
+#    ifdef CALC_LJ
+#        ifndef ENERGY_GROUPS
+    Vvdwtot_S = Vvdwtot_S
+                + SP@VLJ_S0
+#            ifndef HALF_LJ
+                + SP@VLJ_S2
+#            endif
+            ;
+#        else
+    add_ener_grp_halves(SP@VLJ_S0, vvdwtp[0], vvdwtp[1], SP@egp_jj);
+#            ifndef HALF_LJ
+    add_ener_grp_halves(SP@VLJ_S2, vvdwtp[2], vvdwtp[3], SP@egp_jj);
+#            endif
+#        endif
+#    endif /* CALC_LJ */
+#endif     /* CALC_ENERGIES */
+
+#ifdef CALC_LJ
+#    ifdef CALC_COULOMB
+    SP@fscal_S0 = SP@rinvsq_S0 * (SP@frcoul_S0 + SP@frLJ_S0);
+#    else
+    SP@fscal_S0 = SP@rinvsq_S0 * SP@frLJ_S0;
+#    endif
+#else
+    fscal_S0 = rinvsq_S0 * frcoul_S0;
+#endif /* CALC_LJ */
+#if defined CALC_LJ && !defined HALF_LJ
+#    ifdef CALC_COULOMB
+    SP@fscal_S2 = SP@rinvsq_S2 * (SP@frcoul_S2 + SP@frLJ_S2);
+#    else
+    SP@fscal_S2 = SP@rinvsq_S2 * SP@frLJ_S2;
+#    endif
+#else
+    /* Atom 2 and 3 don't have LJ, so only add Coulomb forces */
+    SP@fscal_S2 = SP@rinvsq_S2 * SP@frcoul_S2;
+#endif
+
+    /* Calculate temporary vectorial force */
+    SP@tx_S0 = SP@fscal_S0 * SP@dx_S0;
+    SP@tx_S2 = SP@fscal_S2 * SP@dx_S2;
+    SP@ty_S0 = SP@fscal_S0 * SP@dy_S0;
+    SP@ty_S2 = SP@fscal_S2 * SP@dy_S2;
+    SP@tz_S0 = SP@fscal_S0 * SP@dz_S0;
+    SP@tz_S2 = SP@fscal_S2 * SP@dz_S2;
+
+    /* Increment i atom force */
+    fix_S0 = fix_S0 + SP@tx_S0;
+    fix_S2 = fix_S2 + SP@tx_S2;
+    fiy_S0 = fiy_S0 + SP@ty_S0;
+    fiy_S2 = fiy_S2 + SP@ty_S2;
+    fiz_S0 = fiz_S0 + SP@tz_S0;
+    fiz_S2 = fiz_S2 + SP@tz_S2;
+
+    /* Decrement j atom force */
+    decr3Hsimd(f + SP@aj * DIM, SP@tx_S0 + SP@tx_S2, SP@ty_S0 + SP@ty_S2, SP@tz_S0 + SP@tz_S2);
+
+#undef SP@rinv_ex_S0
+#undef SP@rinv_ex_S2
+
+#undef SP@wco_vdw_S0
+#undef SP@wco_vdw_S2
+
+#undef EXCL_FORCES
+#undef SKIP_INVSQRT
diff -ruN orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_outer.h gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_outer.h
--- orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_outer.h	2022-03-08 10:49:11.000000000 +0900
+++ gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/kernel_outer.h	2022-03-08 11:56:47.000000000 +0900
@@ -547,10 +547,37 @@
                 cjind++;
             }
 #undef CHECK_EXCLS
-            for (; (cjind < cjind1); cjind++)
+            for (; (cjind < cjind1-GMX_2XMM_UNROLL_INNER); cjind+=GMX_2XMM_UNROLL_INNER+1)
             {
-#include "kernel_inner.h"
+#if GMX_2XMM_UNROLL_INNER == 3
+#include "kernel_inner_3.h"
+#elif GMX_2XMM_UNROLL_INNER == 2
+#include "kernel_inner_2.h"
+#elif GMX_2XMM_UNROLL_INNER == 1
+#include "kernel_inner_1.h"
+#elif GMX_2XMM_UNROLL_INNER == 0
+#include "kernel_inner_0.h"
+#endif
+            }
+#if GMX_2XMM_UNROLL_INNER > 2
+            if (cjind < cjind1-2)
+            {
+#include "kernel_inner_2.h"
+            } else
+#endif
+#if GMX_2XMM_UNROLL_INNER > 1
+            if (cjind < cjind1-1)
+            {
+#include "kernel_inner_1.h"
+            } else
+#endif
+#if GMX_2XMM_UNROLL_INNER > 0
+            if (cjind < cjind1)
+            {
+#include "kernel_inner_0.h"
             }
+#endif
+            
 #undef HALF_LJ
 #undef CALC_COULOMB
         }
@@ -565,10 +592,36 @@
                 cjind++;
             }
 #undef CHECK_EXCLS
-            for (; (cjind < cjind1); cjind++)
+            for (; (cjind < cjind1-GMX_2XMM_UNROLL_INNER); cjind+=GMX_2XMM_UNROLL_INNER+1)
             {
-#include "kernel_inner.h"
+#if GMX_2XMM_UNROLL_INNER == 3
+#include "kernel_inner_3.h"
+#elif GMX_2XMM_UNROLL_INNER == 2
+#include "kernel_inner_2.h"
+#elif GMX_2XMM_UNROLL_INNER == 1
+#include "kernel_inner_1.h"
+#elif GMX_2XMM_UNROLL_INNER == 0
+#include "kernel_inner_0.h"
+#endif
             }
+#if GMX_2XMM_UNROLL_INNER > 2
+            if (cjind < cjind1-2)
+            {
+#include "kernel_inner_2.h"
+            } else
+#endif
+#if GMX_2XMM_UNROLL_INNER > 1
+            if (cjind < cjind1-1)
+            {
+#include "kernel_inner_1.h"
+            } else
+#endif
+#if GMX_2XMM_UNROLL_INNER > 0
+            if (cjind < cjind1)
+            {
+#include "kernel_inner_0.h"
+            }
+#endif
 #undef CALC_COULOMB
         }
         else
@@ -581,10 +634,36 @@
                 cjind++;
             }
 #undef CHECK_EXCLS
-            for (; (cjind < cjind1); cjind++)
+            for (; (cjind < cjind1-GMX_2XMM_UNROLL_INNER); cjind+=GMX_2XMM_UNROLL_INNER+1)
             {
-#include "kernel_inner.h"
+#if GMX_2XMM_UNROLL_INNER == 3
+#include "kernel_inner_3.h"
+#elif GMX_2XMM_UNROLL_INNER == 2
+#include "kernel_inner_2.h"
+#elif GMX_2XMM_UNROLL_INNER == 1
+#include "kernel_inner_1.h"
+#elif GMX_2XMM_UNROLL_INNER == 0
+#include "kernel_inner_0.h"
+#endif
+            }
+#if GMX_2XMM_UNROLL_INNER > 2
+            if (cjind < cjind1-2)
+            {
+#include "kernel_inner_2.h"
+            } else
+#endif
+#if GMX_2XMM_UNROLL_INNER > 1
+            if (cjind < cjind1-1)
+            {
+#include "kernel_inner_1.h"
+            } else
+#endif
+#if GMX_2XMM_UNROLL_INNER > 0
+            if (cjind < cjind1)
+            {
+#include "kernel_inner_0.h"
             }
+#endif
         }
 #undef CALC_LJ
         /* Add accumulated i-forces to the force array */
diff -ruN orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/make_kernel_inner.py gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/make_kernel_inner.py
--- orig/gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/make_kernel_inner.py	1970-01-01 09:00:00.000000000 +0900
+++ gromacs-2022/src/gromacs/nbnxm/kernels_simd_2xmm/make_kernel_inner.py	2022-03-08 10:54:48.000000000 +0900
@@ -0,0 +1,14 @@
+#!/usr/bin/python3
+
+for unroll in range(4):
+    fin = open ("kernel_inner.h.in", "r")
+    fout = open ("kernel_inner_%d.h" % (unroll), "w")
+    for line in fin.readlines():
+        if "@" in line:
+            for SP in range(unroll+1):
+                fout.write(line.replace("@","%d" %(SP)))
+        else:
+            fout.write(line)
+    fin.close()
+    fout.close()
+
