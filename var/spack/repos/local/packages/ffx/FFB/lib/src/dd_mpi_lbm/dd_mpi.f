C======================================================================C
C                                                                      C
C SOFTWARE NAME : FRONTFLOW_BLUE.1.0                                   C
C                                                                      C
C  SUB ROUTINE    DD_MPI                                               C
C                                                                      C
C                                       WRITTEN BY C.KATO              C
C                                                                      C
C                                                                      C
C Contact address: The University of Tokyo, FSIS project               C  
C                                                                      C
C======================================================================C
C
C      GENERIC TO MPI FORTRAN INTERFACE FOR DOMAIN-DECOMPOSITION
C     PROGRAMMING MODEL
C                        AUTHOR: C. KATO, MERL, HITACHI, LTD.
C                        DATE FIRST WRITTEN : FEBRUARY 20TH, 1998 
C                        DATE       MODIFIED: APRIL    14TH, 2001
C                            (-PRECEXP OPTION IMPLEMENTED)
C                        DATE LAST  MODIFIED: MARCH     7TH, 2003
C                            (ENTRIES DDCOM1 AND DDCOM2 ADDED)
C
C
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
      SUBROUTINE DDINIT(NPART,IPART)
      IMPLICIT REAL*4(A-H,O-Z)
C
      INCLUDE 'mpif.h'
C
C
C      INITIALIZE A PARALLEL TASK FOR DOMAIN-DECOMPOSITION PROGRAMMING
C     MODEL
C                            ( MPI VERSION )
C
C
C NOTE 1; THIS SUBROUTINE QUERIES THE TASK NUMBER (I.E. TASK ID) OF THE
C       CALLING TASK, THE NUMBER OF TASKS WHICH ARE SHARING THE PROGRAM
C       RUN. THE NUMBER OF SUB-DOMAINS AND THE SUB-DOMAIN NUMBER
C       THAT THE CALLING TASK SHOULD TAKE CARE OF, WILL BE RETURNED TO
C       THE CALLING TASK, AFTER BEING SET TO THE NUMBER OF TASKS, AND
C       THE TASK NUMBER PLUS ONE, RESPECTIVELY.
C
C NOTE 2; THE NUMBER OF TASKS TO SHARE A PARTICULAR RUN IS DETERMINED AT
C       RUN TIME BY THE PARALLEL ENVIRONMENT, WHILE THE NUMBER OF
C       SUB-DOMAINS IS THE SAME AS THE NUMBER OF SUB-DOMAIN FILES WHICH
C       HAVE BEEN PREPARED BEFORE THE RUN. THEREFORE, IN SOME CASES, 
C       THE NUMBER OF TASKS MIGHT DIFFER FROM THE ACTUAL NUMBER OF
C       SUB-DOMAINS, ALTHOUGH THEY MUST BE THE SAME TO RUN A MEANINGFUL
C       COMPUTATION. NOTE THAT THIS SUBROUTINE DOES NOT CHECK THE 
C       CONSISTENCY OF THESE VALUES.
C
C NOTE 3; A TASK NUMBER (I.E. TASK ID) IS A UNIQUE NUMBER FROM 0 TO ONE
C       MINUS THE TOTAL NUMBER OF TASKS, ASSIGNED BY THE SYSTEM AT RUN
C       TIME. THUS, THE SUB-DOMAIN NUMBER IS ALSO A UNIQUE NUMBER 
C       FROM 1 TO THE NUMBER OF SUB-DOMAINS.
C
C NOTE 4; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
C       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
C       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
C       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
C
C
C     ARGUMENT LISTINGS
C       (1) INPUT
C          ( NONE )
C
C       (2) OUTPUT
C INT *4   NPART       ; TOTAL NUMBER OF SUB-DOMAINS
C INT *4   IPART       ; SUB-DOMAIN NUMBER THAT THE CALLING TASK SHOULD
C                       TAKE CARE OF
C
C
      CALL MPI_INIT(IERR)
      CALL MPI_COMM_SIZE(MPI_COMM_WORLD,NTASK,IERR)
      CALL MPI_COMM_RANK(MPI_COMM_WORLD,ITASK,IERR)
C
      NPART = NTASK
      IPART = ITASK+1
C
C
      RETURN
      END
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
      SUBROUTINE DDEXIT
      IMPLICIT REAL*4(A-H,O-Z)
C
      INCLUDE 'mpif.h'
C
C
C      EXIT FROM PARALLEL EXECUTIONS
C                            ( MPI VERSION )
C
C
C NOTE ; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
C       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
C       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
C       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
C
C
C     ARGUMENT LISTINGS
C       (1) INPUT
C          ( NONE )
C
C       (2) OUTPUT
C          ( NONE )
C
C
      CALL MPI_FINALIZE(IERR)
C
C
      RETURN
      END
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
      SUBROUTINE DDSYNC
      IMPLICIT REAL*4(A-H,O-Z)
C
      INCLUDE 'mpif.h'
C
C
C      IMPLEMENT BARRIER SYNCHRONIZATION AMONG THE GROUP OF ALL TASKS
C                            ( MPI VERSION )
C
C
C NOTE ; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
C       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
C       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
C       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
C
C
C     ARGUMENT LISTINGS
C       (1) INPUT
C          ( NONE )
C
C       (2) OUTPUT
C          ( NONE )
C
C
      CALL MPI_BARRIER(MPI_COMM_WORLD,IERR)
C
C
      RETURN
      END
C
C
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
      SUBROUTINE DDSTOP(IPART,IUT0)
      IMPLICIT REAL*4(A-H,O-Z)
C
      INCLUDE 'mpif.h'
C
      CHARACTER*60 ERMSGB
     & / ' ## SUBROUTINE DDSTOP: FATAL      ERROR REPORTED ; STOPPING' /
      CHARACTER*60 EREXP1
     & / ' A SUB-DOMAIN COMPUTATION HAS BEEN ABNORMALLY TERMINATED AT' /
C
C
C      STOP ALL THE RUNNING PARALLEL TASKS FOR DOMAIN-DECOMPOSITION
C     PROGRAMMING MODEL
C                            ( MPI VERSION )
C
C
C NOTE 1; IF AN ERROR CONDITION HAS BEEN DETECTED IN SOME TASK RUNNING
C       IN PARALLEL, ALL THE TASKS SHARING THAT PARTICULAR RUN SHOULD BE
C       APPROPRIATELY STOPPED. THIS SUBROUTINE TERMINATES ALL THE 
C       RUNNING TASKS AND CANCELS THE PARALLEL JOB.
C
C NOTE 2; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
C       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
C       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
C       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
C
C
C     ARGUMENT LISTINGS
C       (1) INPUT
C INT *4   IPART       ; SUB-DOMAIN NUMBER THAT THE CALLING TASK IS
C                       TAKING CARE OF
C INT *4   IUT0        ; FILE NUMBER TO WRITE ERROR MESSAGE
C
C       (2) OUTPUT
C          ( NONE )
C
C
      WRITE(IUT0,*) ERMSGB
      WRITE(IUT0,*) EREXP1, IPART
      CALL MPI_ABORT(MPI_COMM_WORLD,IPART,IERR)
C
      RETURN
      END
C
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
      SUBROUTINE DDCOMA(SEND,RECV)
      IMPLICIT REAL*4(A-H,O-Z)
C
      INCLUDE 'mpif.h'
C
#ifdef cputime
      INTEGER*4 NUMALL
      REAL*4 DTALLA,DTALLR
      COMMON /CPUALL/ NUMALL,DTALLA,DTALLR
      REAL*4 DTCPU,TBUF1,TBUF2
#endif
#ifdef USE_TIMER
      include 'timer.h'
#endif      
C
C
C      SUM UP A SINGLE SCALAR AMONG ALL THE PARTICIPATING TASKS FOR
C     DOMAIN-DECOMPOSITION PROGRAMMING MODEL
C
C                            ( MPI VERSION )
C
C
C NOTE 1; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
C       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
C       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
C       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
C
C NOTE 2; SOME COMPILERS, SUCH AS OFORT90 IN HI-UXMPP, SUPPORT AUTOMATIC
C       PRECISION EXPANSION, WHERE ALL THE CONSTANTS, VARIABLES AND
C       ARRAYS OF 4-BYTE PRECISION (REAL*4) ARE AUTOMATICALLY CONVERTED
C       TO THOSE OF 8-BYTE PRECISION (REAL*8) WITH UNFORMATTED I/O DATA 
C       BEING KEPT AS THEY ARE (IF SO SPECIFIED). WHEN USING SUCH 
C       FEATURES (FUNCTIONS) OF A COMPILER, SPECIAL CARE IS NEEDED
C       BECAUSE A COUPLE OF MPI SUBROUTINES CALLED IN THIS SUBPROGRAM
C       ACCEPT THE DATA TYPE (DATA PRECISION) AS THEIR INPUT AND
C       PERFORM THE OPERATIONS ACCORDING TO THIS INPUT VALUE. THIS
C       INTERFACE SUPPORTS THE AUTOMATIC PRECISION EXPANSION MENTIONED
C       ABOVE. IF YOU WISH TO USE SUCH FEATURE, ADD '-DPRECEXP' OPTION
C       WHEN INVOKING 'cpp' FOR PRI-PROCESSING THIS SOURCE PROGRAM FILE.
C
C
C     ARGUMENT LISTINGS
C       (1) INPUT
C REAL*4   SEND             ; SCALAR VARIABLE TO SUM UP
C
C       (2) OUTPUT
C REAL*4   RECV             ; SCALAR VARIABLE SUMMED UP AMONG ALL TASKS
C
C
#ifdef PRECEXP
      CALL MPI_ALLREDUCE(SEND,RECV,1,MPI_REAL8,MPI_SUM,
     &                   MPI_COMM_WORLD,IERR)
#else
      CALL MPI_ALLREDUCE(SEND,RECV,1,MPI_REAL ,MPI_SUM,
     &                   MPI_COMM_WORLD,IERR)
#endif
     
C
      RETURN
      END
C
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
      SUBROUTINE DDCOM0(IPART,MBC,MDOM,NC,NBC,LBC,NDOM,LDOM,IERR)
      IMPLICIT NONE
      INTEGER*4 IPART,MBC,MDOM,NC,IERR,
     *          NBC(NC),LBC(5,MBC,NC),NDOM,LDOM(MDOM)
C
      INTEGER*4 IC,IB,IDOM,JPART
C
      IERR=0
      NDOM=0
      DO 1000 IC=1,NC
          DO 1100 IB=1,NBC(IC)
              JPART=LBC(2,IB,IC)
              IF(IPART.EQ.JPART) GOTO 1100
              DO 1200 IDOM=1,NDOM 
                  IF(LDOM(IDOM).EQ.JPART) GOTO 1100
 1200         CONTINUE
              NDOM=NDOM+1
              IF(NDOM.GT.MDOM) THEN
                  IERR=1
                  RETURN
              ENDIF
              LDOM(NDOM)=JPART
 1100     CONTINUE
 1000 CONTINUE
C
      RETURN
      END
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
      SUBROUTINE DDCOM1(NDOM,LDOM,NBPSND,NBPRCV,IERR)
      IMPLICIT NONE
      INTEGER*4 NDOM,LDOM(NDOM),NBPSND(NDOM),NBPRCV(NDOM),IERR
      INTEGER*4 IDOM,MSGTYP,IRECV,MSGLEN,ISEND
C
      INCLUDE 'mpif.h'
      INTEGER*4 MAXDOM
      PARAMETER ( MAXDOM = 100000 )
      INTEGER*4 MSGIDS(MAXDOM),MSGSTS(MPI_STATUS_SIZE,MAXDOM)
C
C     ====FUNCTION====
C     COMMUNICATE NUMBER OF DATA AMONG NEIBERING DOMAINS
C
C     ====VARIABLES LIST====
C[IN]
C  NDOM   :NUMBER OF THE NEIGHBORING SUB-DOMAINS 
C  LDOM   :NEIGHBORING SUB-DOMAIN NUMBER
C  NBPSND :NUMBER OF DATA TO SEND TO THE NEIGHBORING SUB-DOMAINS
C
C[OUT]
C  NBPRCV :NUMBER OF DATA TO BE RECEIVED FROM THE NEIGHBORING SUB-DOMAINS
C  IERR   :ERROR CODE
C
      IERR=0
C
C [1] POST ALL THE EXPECTED RECEIVES
C
      DO 1000 IDOM = 1 , NDOM
          MSGTYP = 1
          IRECV  = LDOM (IDOM)-1
          MSGLEN = 1
          CALL MPI_IRECV(NBPRCV(IDOM),MSGLEN,MPI_REAL,IRECV,MSGTYP,
     *                   MPI_COMM_WORLD,MSGIDS(IDOM),IERR)
 1000      CONTINUE
C
C [2] SEND THE RESIDUALS
C
      DO 2000 IDOM = 1 , NDOM
          MSGTYP = 1
          ISEND  = LDOM (IDOM)-1
          MSGLEN = 1
          CALL MPI_ISEND(NBPSND(IDOM),MSGLEN,MPI_REAL ,ISEND,MSGTYP,
     *                   MPI_COMM_WORLD,MSGIDS(NDOM+IDOM),IERR)
 2000 CONTINUE
C
C [3] WAIT FOR THE COMPLETION OF ALL THE REQUESTED COMMUNICATIONS
C
      CALL MPI_WAITALL(2*NDOM,MSGIDS,MSGSTS,IERR)
C
      RETURN
      END
C
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
      SUBROUTINE DDCOM2(NDOM,LDOM,MPB,NBPSND,NBPRCV,
     *                  BUFSND,BUFRCV,IERR)
      IMPLICIT NONE
      INTEGER*4 NDOM,LDOM(NDOM),MPB,NBPSND(NDOM),NBPRCV(NDOM)
      REAL*8    BUFSND(MPB,NDOM),BUFRCV(MPB,NDOM)
      INTEGER*4 IDOM,MSGTYP,IRECV,MSGLEN,ISEND,IERR
C
      INCLUDE 'mpif.h'
      INTEGER*4 MAXDOM
      PARAMETER ( MAXDOM = 100000 )
      INTEGER*4 MSGIDS(MAXDOM),MSGSTS(MPI_STATUS_SIZE,MAXDOM)
C
C     ====FUNCTION====
C     COMMUNICATE B.C. DATA AMONG NEIBERING DOMAINS
C
C     ====VARIABLES LIST====
C[IN]
C  NDOM   :NUMBER OF THE NEIGHBORING SUB-DOMAINS 
C  LDOM   :NEIGHBORING SUB-DOMAIN NUMBER
C  MPB    :SIZE OF 1ST DIMENSION OF BUFFERS FOR COMMUNICATION
C  NBPRCV :NUMBER OF DATA TO BE RECEIVED FROM THE NEIGHBORING SUB-DOMAINS
C  NBPSND :NUMBER OF DATA TO SEND TO THE NEIGHBORING SUB-DOMAINS
C  BUFSND :WORK REGION FOR SENDING   
C
C[OUT]
C  BUFRCV :WORK REGION FOR RECEIVING 
C  IERR   :ERROR CODE
C
      IERR=0
C
C [1] POST ALL THE EXPECTED RECEIVES
C
      DO 1000 IDOM = 1 , NDOM
          MSGTYP = 1
          IRECV  = LDOM (IDOM)-1
          MSGLEN = NBPRCV(IDOM)
          CALL MPI_IRECV(BUFRCV(1,IDOM),MSGLEN,MPI_REAL8,IRECV,MSGTYP,
     *                   MPI_COMM_WORLD,MSGIDS(IDOM),IERR)
 1000 CONTINUE
C
C [2] SEND THE RESIDUALS
C
      DO 2000 IDOM = 1 , NDOM
          MSGTYP = 1
          ISEND  = LDOM (IDOM)-1
          MSGLEN = NBPSND(IDOM)
          CALL MPI_ISEND(BUFSND(1,IDOM),MSGLEN,MPI_REAL8,ISEND,MSGTYP,
     *                   MPI_COMM_WORLD,MSGIDS(NDOM+IDOM),IERR)
 2000 CONTINUE
C
C [3] WAIT FOR THE COMPLETION OF ALL THE REQUESTED COMMUNICATIONS
C
      CALL MPI_WAITALL(2*NDOM,MSGIDS,MSGSTS,IERR)
C
      RETURN
      END
